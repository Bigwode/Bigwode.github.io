<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Sora技术报告解读</title>
      <link href="/2024/02/17/computervision/sora/sora/"/>
      <url>/2024/02/17/computervision/sora/sora/</url>
      
        <content type="html"><![CDATA[<ul><li><p>这里看的是OpenAI写的<a href="https://openai.com/research/video-generation-models-as-world-simulators">Sora技术报告</a></p></li><li><p>Sora是一个Video Generation生成器，可以生成不同时长、宽高比和分辨率的视频和图像，最多可达一分钟的高清视频。</p></li><li><p>Sora是使用不同长度的videos和images训练的text-conditional diffusion models。</p></li><li><p>Sora是在video和image的latent code的时空patches上的运行的transformer结构。</p></li><li><p>这份技术报告只包含了两部分内容, 对于模型和实现细节在这份报告中并没有包含。</p><p>  (1) 将所有类型的视觉数据转化为统一的表示形式，从而实现生成模型的大规模训练</p><p>  (2) 对 Sora 的能力和局限性进行定性评估。</p></li></ul><h2 id="Turning-visual-data-into-patches"><a href="#Turning-visual-data-into-patches" class="headerlink" title="Turning visual data into patches"></a>Turning visual data into patches</h2><ul><li>LLM成功的一部分原因来自于token的使用，可以优雅地将不同模态的text进行统一的编码，本文考虑的是在视觉数据中如何从中获益。Sora的patches类似于tokens, patches对于在不同形式的videos和images上训练生成模型是高效并拓展性高的。</li><li>总结下来，首先将视频压缩到较低维的latent space，然后将这个表征分解为时空（spacetime）patches。</li></ul><p><img src="Sora-1-patches.png" alt="Sora Patches"></p><h2 id="Video-compression-network"><a href="#Video-compression-network" class="headerlink" title="Video compression network"></a>Video compression network</h2><ul><li>训练一个网络[Auto-encoding variational bayes.]来降低视觉数据的维度。该网络将原始视频作为输入，并输出压缩了时间和空间的latent representation。Sora在这个压缩的latent space中训练并随后生成视频。我们还训练了一个相应的decoder模型，该模型将生成的latents映射回pixel space。</li></ul><h2 id="Spacetime-latent-patches"><a href="#Spacetime-latent-patches" class="headerlink" title="Spacetime latent patches"></a>Spacetime latent patches</h2><ul><li>给定一个压缩的输入视频，我们提取一系列spacetime patches作为transformer tokens。这个方案也适用于图像，因为图像只是单帧的视频。我们基于patch的表示使Sora能够在不同分辨率、持续时间和宽高比的视频和图像上进行训练。在推理时，我们可以通过在适当大小的网格中安排随机初始化的patches来控制生成视频的大小。</li></ul><h2 id="Scaling-transformers-for-video-generation"><a href="#Scaling-transformers-for-video-generation" class="headerlink" title="Scaling transformers for video generation"></a>Scaling transformers for video generation</h2><ul><li>Sora是一个diffusion model[Scalable diffusion models with transformers]，通过noisy patches预测原始的clean patches。Sora是一个diffusion transformer, 因为transformer的scaling特性。</li><li>Sample quality improves markedly as training compute increases.</li></ul><h2 id="Variable-durations-resolutions-aspect-ratios"><a href="#Variable-durations-resolutions-aspect-ratios" class="headerlink" title="Variable durations, resolutions, aspect ratios"></a>Variable durations, resolutions, aspect ratios</h2><ul><li>采样的灵活性 Sampling flexibility</li><li>提升构图和框架 Improved framing and composition，不会生成主体的部分出现在图像之外。</li></ul><h2 id="Language-understanding"><a href="#Language-understanding" class="headerlink" title="Language understanding"></a>Language understanding</h2><ul><li>使用了DALL·E3中的re-captioning技术。我们首先训练一个高度描述性的captioner model，然后使用它为训练集中的所有视频生成文本描述。作者发现，对高质量描述性的视频字幕进行训练可以提高文本保真度以及视频的整体质量。</li><li>与 DALL·E3 类似，我们还利用GPT将简短的用户提示转换为较长的详细captions，然后送到视频模型中，这使得Sora能够生成准确遵循用户提示的高质量视频。</li></ul><h2 id="Prompting-with-images-and-videos"><a href="#Prompting-with-images-and-videos" class="headerlink" title="Prompting with images and videos"></a>Prompting with images and videos</h2><ul><li>Sora还可以使用图像和视频作为prompts</li><li>Sora可以向前向后拓展视频</li><li>结合SDEdit，可以变换videos的风格和环境</li><li>还可以在两段不同的视频中做插入（这个估计就是对latect space做插值操作了？）</li></ul><h2 id="Image-generation-capabilities"><a href="#Image-generation-capabilities" class="headerlink" title="Image generation capabilities"></a>Image generation capabilities</h2><h2 id="Emerging-simulation-capabilities"><a href="#Emerging-simulation-capabilities" class="headerlink" title="Emerging simulation capabilities"></a>Emerging simulation capabilities</h2><ul><li><p>大规模训练后获得的突发能力，没有任何归纳偏置的情况下，完全来自于scale</p><ul><li>3D consistency. Sora can generate videos with dynamic camera motion.</li><li>Long-range coherence and object permanence.</li><li>Interacting with the world.</li></ul>  <video width="480" height="320" controls="">  <source src="simulation_4.mp4" type="video/mp4">  Sora Interaction.  </video><ul><li>Simulating digital worlds.</li></ul></li></ul><h2 id="Limitations"><a href="#Limitations" class="headerlink" title="Limitations"></a>Limitations</h2><ul><li>For example, it does not accurately model the physics of many basic interactions.</li></ul>]]></content>
      
      
      <categories>
          
          <category> Video Generation </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Diffusion </tag>
            
            <tag> Transformer </tag>
            
            <tag> Video Generation </tag>
            
            <tag> Sora </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>nanoGPT源码解析</title>
      <link href="/2024/02/09/computervision/nanogpt/nanogpt/"/>
      <url>/2024/02/09/computervision/nanogpt/nanogpt/</url>
      
        <content type="html"><![CDATA[<ul><li>这里看的是AK写的<a href="https://github.com/karpathy/nanoGPT">nanoGPT</a></li></ul><h4 id="1-wandb用法"><a href="#1-wandb用法" class="headerlink" title="1. wandb用法"></a>1. wandb用法</h4><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> wandbwandb<span class="token punctuation">.</span>init<span class="token punctuation">(</span>project<span class="token operator">=</span>wandb_project<span class="token punctuation">,</span> name<span class="token operator">=</span>wandb_run_name<span class="token punctuation">,</span> config<span class="token operator">=</span>config<span class="token punctuation">)</span>wandb<span class="token punctuation">.</span>log<span class="token punctuation">(</span><span class="token punctuation">{</span>    <span class="token string">"iter"</span><span class="token punctuation">:</span> iter_num<span class="token punctuation">,</span>    <span class="token string">"train/loss"</span><span class="token punctuation">:</span> losses<span class="token punctuation">[</span><span class="token string">'train'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>    <span class="token string">"val/loss"</span><span class="token punctuation">:</span> losses<span class="token punctuation">[</span><span class="token string">'val'</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">}</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>]]></content>
      
      
      <categories>
          
          <category> GPTs </category>
          
      </categories>
      
      
        <tags>
            
            <tag> GPTs </tag>
            
            <tag> nanoGPT </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>VQGAN源码解析</title>
      <link href="/2024/01/31/computervision/vqgan/vqgan/"/>
      <url>/2024/01/31/computervision/vqgan/vqgan/</url>
      
        <content type="html"><![CDATA[<ul><li>这里看的是一个外国小哥复现的简单版本的<a href="https://github.com/dome272/VQGAN-pytorch">VQGAN</a></li></ul><h4 id="1-tqdm的一个用法"><a href="#1-tqdm的一个用法" class="headerlink" title="1. tqdm的一个用法"></a>1. tqdm的一个用法</h4><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">with</span> tqdm<span class="token punctuation">(</span><span class="token builtin">range</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>train_dataset<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token keyword">as</span> pbar<span class="token punctuation">:</span>    <span class="token keyword">for</span> i<span class="token punctuation">,</span> imgs <span class="token keyword">in</span> <span class="token builtin">zip</span><span class="token punctuation">(</span>pbar<span class="token punctuation">,</span> train_dataset<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token comment"># do something</span>        pbar<span class="token punctuation">.</span>set_postfix<span class="token punctuation">(</span>            VQ_Loss<span class="token operator">=</span>np<span class="token punctuation">.</span><span class="token builtin">round</span><span class="token punctuation">(</span>vq_loss<span class="token punctuation">.</span>cpu<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            GAN_Loss<span class="token operator">=</span>np<span class="token punctuation">.</span><span class="token builtin">round</span><span class="token punctuation">(</span>gan_loss<span class="token punctuation">.</span>cpu<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span>        <span class="token punctuation">)</span>        pbar<span class="token punctuation">.</span>update<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="2-使用torchinfo可视化网络结构"><a href="#2-使用torchinfo可视化网络结构" class="headerlink" title="2. 使用torchinfo可视化网络结构"></a>2. 使用torchinfo可视化网络结构</h4><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> torchinfo <span class="token keyword">import</span> summaryencoder <span class="token operator">=</span> Encoder<span class="token punctuation">(</span><span class="token punctuation">)</span>summary<span class="token punctuation">(</span>encoder<span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">256</span><span class="token punctuation">,</span> <span class="token number">256</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><h4 id="3-VQGAN的Encoder部分"><a href="#3-VQGAN的Encoder部分" class="headerlink" title="3. VQGAN的Encoder部分"></a>3. VQGAN的Encoder部分</h4><ul><li><p>模型结构为：<br><img src="VQGAN-Encoder.png" alt="VQGAN Encoder结构"></p></li><li><p>其中，</p><ul><li>ResidualBlock是由两个GroupNorm+Swish+Conv组成;</li><li>DownSampleBlock是一个stride=2的Conv;</li><li>NonLocalBlock是一个Attention Block;</li></ul></li><li><p>Encode之后会再接一个1x1的conv，称作是quant_conv. 然后过codebook，对应的还有个1x1的post_quant_conv。</p></li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># 简易Attention实现</span>attn <span class="token operator">=</span> torch<span class="token punctuation">.</span>bmm<span class="token punctuation">(</span>q<span class="token punctuation">,</span> k<span class="token punctuation">)</span>attn <span class="token operator">=</span> attn <span class="token operator">*</span> <span class="token punctuation">(</span><span class="token builtin">int</span><span class="token punctuation">(</span>c<span class="token punctuation">)</span> <span class="token operator">**</span> <span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">0.5</span><span class="token punctuation">)</span><span class="token punctuation">)</span>attn <span class="token operator">=</span> F<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>attn<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>attn <span class="token operator">=</span> attn<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>A <span class="token operator">=</span> torch<span class="token punctuation">.</span>bmm<span class="token punctuation">(</span>v<span class="token punctuation">,</span> attn<span class="token punctuation">)</span>A <span class="token operator">=</span> A<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>b<span class="token punctuation">,</span> c<span class="token punctuation">,</span> h<span class="token punctuation">,</span> w<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="4-VQGAN的Codebook部分"><a href="#4-VQGAN的Codebook部分" class="headerlink" title="4. VQGAN的Codebook部分"></a>4. VQGAN的Codebook部分</h4><ul><li>为什么要使用Codebook对Encoder得到的feature做离散化编码？<ul><li>使得模型学习到更抽象和压缩的数据表示</li><li>强制信息瓶颈帮助提升生成模型的泛化能力</li><li>提高计算效率</li></ul></li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn<span class="token keyword">class</span> <span class="token class-name">Codebook</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> args<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token builtin">super</span><span class="token punctuation">(</span>Codebook<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>num_codebook_vectors <span class="token operator">=</span> args<span class="token punctuation">.</span>num_codebook_vectors  <span class="token comment"># 1024</span>        self<span class="token punctuation">.</span>latent_dim <span class="token operator">=</span> args<span class="token punctuation">.</span>latent_dim  <span class="token comment"># 256</span>        self<span class="token punctuation">.</span>beta <span class="token operator">=</span> args<span class="token punctuation">.</span>beta  <span class="token comment"># 0.25</span>        self<span class="token punctuation">.</span>embedding <span class="token operator">=</span> nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>self<span class="token punctuation">.</span>num_codebook_vectors<span class="token punctuation">,</span> self<span class="token punctuation">.</span>latent_dim<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>embedding<span class="token punctuation">.</span>weight<span class="token punctuation">.</span>data<span class="token punctuation">.</span>uniform_<span class="token punctuation">(</span>            <span class="token operator">-</span><span class="token number">1.0</span> <span class="token operator">/</span> self<span class="token punctuation">.</span>num_codebook_vectors<span class="token punctuation">,</span> <span class="token number">1.0</span> <span class="token operator">/</span> self<span class="token punctuation">.</span>num_codebook_vectors        <span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> z<span class="token punctuation">)</span><span class="token punctuation">:</span>        z <span class="token operator">=</span> z<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>contiguous<span class="token punctuation">(</span><span class="token punctuation">)</span>        z_flattened <span class="token operator">=</span> z<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>latent_dim<span class="token punctuation">)</span>  <span class="token comment"># [1*16*16, 256]</span>        d <span class="token operator">=</span> <span class="token punctuation">(</span>            torch<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>z_flattened<span class="token operator">**</span><span class="token number">2</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> keepdim<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>            <span class="token operator">+</span> torch<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>embedding<span class="token punctuation">.</span>weight<span class="token operator">**</span><span class="token number">2</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>            <span class="token operator">-</span> <span class="token number">2</span> <span class="token operator">*</span> <span class="token punctuation">(</span>torch<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>z_flattened<span class="token punctuation">,</span> self<span class="token punctuation">.</span>embedding<span class="token punctuation">.</span>weight<span class="token punctuation">.</span>t<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token punctuation">)</span>  <span class="token comment"># [256, 1024]</span>        min_encoding_indices <span class="token operator">=</span> torch<span class="token punctuation">.</span>argmin<span class="token punctuation">(</span>d<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment"># [256]</span>        z_q <span class="token operator">=</span> self<span class="token punctuation">.</span>embedding<span class="token punctuation">(</span>min_encoding_indices<span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span>z<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>        loss <span class="token operator">=</span> torch<span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token punctuation">(</span>z_q<span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">-</span> z<span class="token punctuation">)</span> <span class="token operator">**</span> <span class="token number">2</span><span class="token punctuation">)</span> <span class="token operator">+</span> self<span class="token punctuation">.</span>beta <span class="token operator">*</span> torch<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>            <span class="token punctuation">(</span>z_q <span class="token operator">-</span> z<span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">**</span> <span class="token number">2</span>        <span class="token punctuation">)</span>        <span class="token comment"># simply copy the gradient from the decoder to the encoder. </span>        <span class="token comment"># 这里实现非常巧妙，也很容易出错。</span>        z_q <span class="token operator">=</span> z <span class="token operator">+</span> <span class="token punctuation">(</span>z_q <span class="token operator">-</span> z<span class="token punctuation">)</span><span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># preserve the gradients for the backward flow.</span>        z_q <span class="token operator">=</span> z_q<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>        <span class="token keyword">return</span> z_q<span class="token punctuation">,</span> min_encoding_indices<span class="token punctuation">,</span> loss<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="5-VQGAN的Decoder部分"><a href="#5-VQGAN的Decoder部分" class="headerlink" title="5. VQGAN的Decoder部分"></a>5. VQGAN的Decoder部分</h4><ul><li>模型结构为：<br><img src="VQGAN-Decoder.png" alt="VQGAN Decoder结构"></li></ul><h4 id="6-VQGAN的Discriminator部分"><a href="#6-VQGAN的Discriminator部分" class="headerlink" title="6. VQGAN的Discriminator部分"></a>6. VQGAN的Discriminator部分</h4><ul><li><p>只有在超过一定的steps(10000)之后才开始加入Disc的训练</p></li><li><p>模型结构为：<br><img src="VQGAN-Discriminator.png" alt="VQGAN Discriminator结构"></p></li></ul><h4 id="7-VQGAN的其他loss部分"><a href="#7-VQGAN的其他loss部分" class="headerlink" title="7. VQGAN的其他loss部分"></a>7. VQGAN的其他loss部分</h4><ul><li>perceptual_loss: 加载了一个pretrained VGG16模型，计算的real_img和gen_img之间的多层feature距离。</li><li>重建loss</li><li>gan_loss:</li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python">perceptual_rec_loss <span class="token operator">=</span> <span class="token punctuation">(</span>    args<span class="token punctuation">.</span>perceptual_loss_factor <span class="token operator">*</span> perceptual_loss    <span class="token operator">+</span> args<span class="token punctuation">.</span>rec_loss_factor <span class="token operator">*</span> rec_loss<span class="token punctuation">)</span>perceptual_rec_loss <span class="token operator">=</span> perceptual_rec_loss<span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token punctuation">)</span>g_loss <span class="token operator">=</span> <span class="token operator">-</span>torch<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>disc_fake<span class="token punctuation">)</span>  <span class="token comment"># 趋于0，或者大于0</span>λ <span class="token operator">=</span> self<span class="token punctuation">.</span>vqgan<span class="token punctuation">.</span>calculate_lambda<span class="token punctuation">(</span>perceptual_rec_loss<span class="token punctuation">,</span> g_loss<span class="token punctuation">)</span>vq_loss <span class="token operator">=</span> perceptual_rec_loss <span class="token operator">+</span> q_loss <span class="token operator">+</span> disc_factor <span class="token operator">*</span> λ <span class="token operator">*</span> g_lossd_loss_real <span class="token operator">=</span> torch<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span><span class="token number">1.0</span> <span class="token operator">-</span> disc_real<span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># 大于1</span>d_loss_fake <span class="token operator">=</span> torch<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span><span class="token number">1.0</span> <span class="token operator">+</span> disc_fake<span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># 小于-1</span>gan_loss <span class="token operator">=</span> disc_factor <span class="token operator">*</span> <span class="token number">0.5</span> <span class="token operator">*</span> <span class="token punctuation">(</span>d_loss_real <span class="token operator">+</span> d_loss_fake<span class="token punctuation">)</span>self<span class="token punctuation">.</span>opt_vq<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>vq_loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span>retain_graph<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>self<span class="token punctuation">.</span>opt_disc<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>gan_loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>self<span class="token punctuation">.</span>opt_vq<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>self<span class="token punctuation">.</span>opt_disc<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="8-VQGAN的loss的calculate-lambda部分"><a href="#8-VQGAN的loss的calculate-lambda部分" class="headerlink" title="8. VQGAN的loss的calculate_lambda部分"></a>8. VQGAN的loss的calculate_lambda部分</h4><ul><li>这个函数的作用: 动态调整模型训练中不同部分的损失贡献相关。<ul><li>根据最后一层的梯度情况来平衡感知损失和GAN损失的影响，有助于控制训练过程中的损失平衡。</li></ul></li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">calculate_lambda</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> perceptual_loss<span class="token punctuation">,</span> gan_loss<span class="token punctuation">)</span><span class="token punctuation">:</span>    last_layer <span class="token operator">=</span> self<span class="token punctuation">.</span>decoder<span class="token punctuation">.</span>model<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span>    last_layer_weight <span class="token operator">=</span> last_layer<span class="token punctuation">.</span>weight    perceptual_loss_grads <span class="token operator">=</span> torch<span class="token punctuation">.</span>autograd<span class="token punctuation">.</span>grad<span class="token punctuation">(</span>        perceptual_loss<span class="token punctuation">,</span> last_layer_weight<span class="token punctuation">,</span> retain_graph<span class="token operator">=</span><span class="token boolean">True</span>    <span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>    gan_loss_grads <span class="token operator">=</span> torch<span class="token punctuation">.</span>autograd<span class="token punctuation">.</span>grad<span class="token punctuation">(</span>        gan_loss<span class="token punctuation">,</span> last_layer_weight<span class="token punctuation">,</span> retain_graph<span class="token operator">=</span><span class="token boolean">True</span>    <span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>    λ <span class="token operator">=</span> torch<span class="token punctuation">.</span>norm<span class="token punctuation">(</span>perceptual_loss_grads<span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token punctuation">(</span>torch<span class="token punctuation">.</span>norm<span class="token punctuation">(</span>gan_loss_grads<span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token number">1e-4</span><span class="token punctuation">)</span>    λ <span class="token operator">=</span> torch<span class="token punctuation">.</span>clamp<span class="token punctuation">(</span>λ<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1e4</span><span class="token punctuation">)</span><span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token keyword">return</span> <span class="token number">0.8</span> <span class="token operator">*</span> λ<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><p>到这里，我们训练VQGAN的第一步已经结束了，我们可以根据一组VQ来重建出一张图，那么接下就需要transformer来负责先预测出一组VQ然后重建出图来啦～</p><hr><h4 id="8-VQGAN的Transformer部分"><a href="#8-VQGAN的Transformer部分" class="headerlink" title="8. VQGAN的Transformer部分"></a>8. VQGAN的Transformer部分</h4><ul><li>AdamW优化器区分了bias, nn.LayerNorm, nn.Embedding以及pos_emb训练时都不加weight decay，而nn.Linear加了weight decay。</li><li>Transformer部分的网络结构主要是一个<a href="https://github.com/karpathy/minGPT/">minGPT</a>。</li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> idx<span class="token punctuation">,</span> embeddings<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token comment"># idx是输入图像通过vqgan的Encoder部分得到的indices经过随机mask掉一部分得到的indices.</span>    token_embeddings <span class="token operator">=</span> self<span class="token punctuation">.</span>tok_emb<span class="token punctuation">(</span>idx<span class="token punctuation">)</span>  <span class="token comment"># each index maps to a (learnable) vector</span>    t <span class="token operator">=</span> token_embeddings<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>    position_embeddings <span class="token operator">=</span> self<span class="token punctuation">.</span>pos_emb<span class="token punctuation">[</span>        <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span>t<span class="token punctuation">,</span> <span class="token punctuation">:</span>    <span class="token punctuation">]</span>  <span class="token comment"># each position maps to a (learnable) vector</span>    x <span class="token operator">=</span> self<span class="token punctuation">.</span>drop<span class="token punctuation">(</span>token_embeddings <span class="token operator">+</span> position_embeddings<span class="token punctuation">)</span>    x <span class="token operator">=</span> self<span class="token punctuation">.</span>blocks<span class="token punctuation">(</span>x<span class="token punctuation">)</span>  <span class="token comment"># A vanilla multi-head masked self-attention layer</span>    x <span class="token operator">=</span> self<span class="token punctuation">.</span>ln_f<span class="token punctuation">(</span>x<span class="token punctuation">)</span>    logits <span class="token operator">=</span> self<span class="token punctuation">.</span>head<span class="token punctuation">(</span>x<span class="token punctuation">)</span>  <span class="token comment"># [1, 1024]</span>    <span class="token keyword">return</span> logits<span class="token punctuation">,</span> <span class="token boolean">None</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li><p>这里训GPTTransformer的时候，</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>    _<span class="token punctuation">,</span> indices <span class="token operator">=</span> self<span class="token punctuation">.</span>encode_to_z<span class="token punctuation">(</span>x<span class="token punctuation">)</span>  <span class="token comment"># 输入图像经过vqgan的Encoder部分得到的indices</span>    sos_tokens <span class="token operator">=</span> torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span>x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">*</span> self<span class="token punctuation">.</span>sos_token    sos_tokens <span class="token operator">=</span> sos_tokens<span class="token punctuation">.</span><span class="token builtin">long</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span><span class="token string">"cuda"</span><span class="token punctuation">)</span>  <span class="token comment"># 起始token的index，默认是0</span>    mask <span class="token operator">=</span> torch<span class="token punctuation">.</span>bernoulli<span class="token punctuation">(</span>        self<span class="token punctuation">.</span>pkeep <span class="token operator">*</span> torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span>indices<span class="token punctuation">.</span>shape<span class="token punctuation">,</span> device<span class="token operator">=</span>indices<span class="token punctuation">.</span>device<span class="token punctuation">)</span>    <span class="token punctuation">)</span>    mask <span class="token operator">=</span> mask<span class="token punctuation">.</span><span class="token builtin">round</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>int64<span class="token punctuation">)</span>    random_indices <span class="token operator">=</span> torch<span class="token punctuation">.</span>randint_like<span class="token punctuation">(</span>indices<span class="token punctuation">,</span> self<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>config<span class="token punctuation">.</span>vocab_size<span class="token punctuation">)</span>    <span class="token comment"># 随机mask掉一半的indices，用随机的indices代替</span>    new_indices <span class="token operator">=</span> mask <span class="token operator">*</span> indices <span class="token operator">+</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">-</span> mask<span class="token punctuation">)</span> <span class="token operator">*</span> random_indices      new_indices <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">(</span>sos_tokens<span class="token punctuation">,</span> new_indices<span class="token punctuation">)</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>    target <span class="token operator">=</span> indices    logits<span class="token punctuation">,</span> _ <span class="token operator">=</span> self<span class="token punctuation">.</span>transformer<span class="token punctuation">(</span>new_indices<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>    <span class="token keyword">return</span> logits<span class="token punctuation">,</span> target  <span class="token comment"># 直接用cross entropy训练</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li><li><p>这样训完之后，GPT Transformer就可以根据前序的token来预测后续的token了</p></li></ul><h4 id="9-训好了怎么sample生成样本呢？"><a href="#9-训好了怎么sample生成样本呢？" class="headerlink" title="9. 训好了怎么sample生成样本呢？"></a>9. 训好了怎么sample生成样本呢？</h4><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token decorator annotation punctuation">@torch<span class="token punctuation">.</span>no_grad</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">def</span> <span class="token function">sample</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">,</span> c<span class="token punctuation">,</span> steps<span class="token punctuation">,</span> temperature<span class="token operator">=</span><span class="token number">1.0</span><span class="token punctuation">,</span> top_k<span class="token operator">=</span><span class="token number">100</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    self<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span><span class="token builtin">eval</span><span class="token punctuation">(</span><span class="token punctuation">)</span>    x <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">(</span>c<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>    <span class="token keyword">for</span> k <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>steps<span class="token punctuation">)</span><span class="token punctuation">:</span>        logits<span class="token punctuation">,</span> _ <span class="token operator">=</span> self<span class="token punctuation">.</span>transformer<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        logits <span class="token operator">=</span> logits<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token operator">/</span> temperature        <span class="token keyword">if</span> top_k <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>            logits <span class="token operator">=</span> self<span class="token punctuation">.</span>top_k_logits<span class="token punctuation">(</span>logits<span class="token punctuation">,</span> top_k<span class="token punctuation">)</span>        probs <span class="token operator">=</span> F<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>logits<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>        ix <span class="token operator">=</span> torch<span class="token punctuation">.</span>multinomial<span class="token punctuation">(</span>probs<span class="token punctuation">,</span> num_samples<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment"># 按照概率随机取一个值</span>        x <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span> ix<span class="token punctuation">)</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>    x <span class="token operator">=</span> x<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> c<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">:</span><span class="token punctuation">]</span>    self<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token keyword">return</span> x<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li>获得一个序列的indices送入到VQGAN的Decoder就可以生成图像了。</li></ul>]]></content>
      
      
      <categories>
          
          <category> Computer Vision </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Computer Vision </tag>
            
            <tag> Image Generation </tag>
            
            <tag> GAN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Python字符串神奇用法</title>
      <link href="/2024/01/23/python/zi-fu-chuan/"/>
      <url>/2024/01/23/python/zi-fu-chuan/</url>
      
        <content type="html"><![CDATA[<h4 id="1-将一个字符串中的前两个’-’替换为’-’"><a href="#1-将一个字符串中的前两个’-’替换为’-’" class="headerlink" title="1. 将一个字符串中的前两个’_’替换为’:’"></a>1. 将一个字符串中的前两个’_’替换为’:’</h4><pre class="line-numbers language-python" data-language="python"><code class="language-python">s <span class="token operator">=</span> <span class="token string">"a_b_c_d_e"</span>s_replaced <span class="token operator">=</span> s<span class="token punctuation">.</span>replace<span class="token punctuation">(</span><span class="token string">"_"</span><span class="token punctuation">,</span> <span class="token string">":"</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>  <span class="token comment"># 指定最大替换次数为2</span><span class="token keyword">print</span><span class="token punctuation">(</span>s_replaced<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><h4 id="2-argparse命令行参数中的-会自动转换为"><a href="#2-argparse命令行参数中的-会自动转换为" class="headerlink" title="2. argparse命令行参数中的-会自动转换为_"></a>2. argparse命令行参数中的-会自动转换为_</h4><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># 在使用Python的argparse库时，如果你定义了一个带有连字符（-）的命令行参数，argparse模块会自动将这些连字符转换为下划线（_）。这样做是因为在Python中，变量名不能包含连字符；它们可以包含下划线。</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h4 id="3-python的-staticmethod经常用在什么情况下？"><a href="#3-python的-staticmethod经常用在什么情况下？" class="headerlink" title="3. python的@staticmethod经常用在什么情况下？"></a>3. python的@staticmethod经常用在什么情况下？</h4><ul><li>staticmethod用于修饰类中的方法,使其可以在不创建类实例的情况下调用方法</li><li>函数逻辑与类有关联但不需要类或实例的任何信息：当你需要定义一些功能，这些功能虽然跟类相关，但执行时它们不需要类的任何信息（即不需要访问任何类变量或实例变量）。</li><li>组织工具函数：如果有一些与类操作相关的工具函数，你可能希望将它们组织在一个类里面，以保持代码的组织和清晰。</li><li>替代命名空间：当你想要使用类作为一个命名空间来避免函数名冲突时，可以定义静态方法。这样，你可以将相关的函数放在一个类下面，但这些函数并不需要访问类或实例的状态。</li><li>继承管理：在子类中，你可能想要重用某个静态方法而不是实例方法。因为静态方法不与特定的实例或类状态关联，它们更容易在继承中被复用。</li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">MathUtils</span><span class="token punctuation">:</span>    <span class="token decorator annotation punctuation">@staticmethod</span>    <span class="token keyword">def</span> <span class="token function">add</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span> y<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> x <span class="token operator">+</span> y    <span class="token comment"># 静态方法可以通过类名直接调用，无需创建类的实例</span>result <span class="token operator">=</span> MathUtils<span class="token punctuation">.</span>add<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>result<span class="token punctuation">)</span>  <span class="token comment"># 输出: 12</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="4-python的-运算符"><a href="#4-python的-运算符" class="headerlink" title="4. python的**运算符"></a>4. python的**运算符</h4><ul><li>可以使用**来将字典中的项作为关键字参数传递给函数。<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">greet</span><span class="token punctuation">(</span>first_name<span class="token punctuation">,</span> last_name<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"Hello </span><span class="token interpolation"><span class="token punctuation">{</span>first_name<span class="token punctuation">}</span></span><span class="token string"> </span><span class="token interpolation"><span class="token punctuation">{</span>last_name<span class="token punctuation">}</span></span><span class="token string">!"</span></span><span class="token punctuation">)</span>person <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token string">'first_name'</span><span class="token punctuation">:</span> <span class="token string">'John'</span><span class="token punctuation">,</span> <span class="token string">'last_name'</span><span class="token punctuation">:</span> <span class="token string">'Doe'</span><span class="token punctuation">}</span>greet<span class="token punctuation">(</span><span class="token operator">**</span>person<span class="token punctuation">)</span>  <span class="token comment"># 等同于 greet(first_name='John', last_name='Doe')</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li></ul><h4 id="5-python自带的globals函数"><a href="#5-python自带的globals函数" class="headerlink" title="5. python自带的globals函数"></a>5. python自带的globals函数</h4><ul><li>全局符号表，其中包含有关程序的所有信息，包含变量名，方法，类名等等。<pre class="line-numbers language-python" data-language="python"><code class="language-python">a <span class="token operator">=</span> <span class="token number">5</span><span class="token keyword">def</span> <span class="token function">func</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    c <span class="token operator">=</span> <span class="token number">10</span>    d <span class="token operator">=</span> c <span class="token operator">+</span> a     <span class="token builtin">globals</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token string">'a'</span><span class="token punctuation">]</span> <span class="token operator">=</span> d     <span class="token keyword">print</span><span class="token punctuation">(</span>d<span class="token punctuation">)</span>  func<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li></ul>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
            <tag> 字符串 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>欢迎</title>
      <link href="/2024/01/21/hello-world/"/>
      <url>/2024/01/21/hello-world/</url>
      
        <content type="html"><![CDATA[<p>欢迎来到一只猪的圈。</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="持续学习"><a href="#持续学习" class="headerlink" title="持续学习!"></a>持续学习!</h3><h3 id="学习是这个世界上最简单的事情"><a href="#学习是这个世界上最简单的事情" class="headerlink" title="学习是这个世界上最简单的事情!"></a>学习是这个世界上最简单的事情!</h3>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
