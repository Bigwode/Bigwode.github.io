<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>DepthAnything论文笔记</title>
      <link href="/2024/06/11/computervision/depthanything/depthanything/"/>
      <url>/2024/06/11/computervision/depthanything/depthanything/</url>
      
        <content type="html"><![CDATA[<blockquote><p>Title: <a href="https://arxiv.org/abs/2401.10891">Depth Anything: Unleashing the Power of Large-Scale Unlabeled Data</a> <br>From HKU &amp; TikTok. <br>CVPR 2024</p></blockquote><h2 id="Highlight"><a href="#Highlight" class="headerlink" title="Highlight"></a>Highlight</h2><ul><li>本文做的是用少量有标签数据和大规模无标签数据做一个有泛化性或者叫有scaling-up能力的深度估计模型。</li><li>作者基于两个发现（1）直接结合有标签和打伪标签的数据进行训练并不能提升只用有标签数据训的基础模型的性能。作者认为这是因为以self-teaching的方式学到的信息是有限的。（2）之前的深度估计工作中引入了语义分割的辅助任务带来了涨点，作者发现当深度估计模型性能非常强的时候，这么做并不会继续带来涨点，这是由于把一张图像编码到一个离散的类别空间会带来很严重的信息损失。</li><li>因此作者主要探索了两个策略，一个和是利用数据增强来构造一个更加具有挑战性的优化目标，迫使模型学习额外的视觉特征更加鲁棒的表征。另一个是辅助监督，用于迫使模型从一个pretrained encoders（DINO-v2）里继承丰富的语义先验。</li></ul><p><img src="DepthAnythingPipeline.png" alt="图1. Depth Anything Pipeline."></p><h2 id="Methods"><a href="#Methods" class="headerlink" title="Methods"></a>Methods</h2><ul><li>为了使用多个数据集联合训练，作者设计了一个affine-invariant loss。</li><li>如图2所示，对于每个样本，减去均值除以方差。论文中的公式写的是减去每个样本的中位数，除以的也不是等价于方差。可能差别不大。</li></ul><!-- ![图2. affine-invariant loss.](variantscale_loss.png) --><img src="variantscale_loss.png" alt="图2. affine-invariant loss." width="50%"><ul><li>另外，为了强化Teacher模型从有标签数据的学习，作者使用了DINOv2的pretrain权重来初始化encoder。对于数据上，用一个预训练好的语义分割模型来将图像中的天空分割出来，然后将深度值设置为0（最远的）。</li><li>然后用这个teacher模型给无标签数据打伪标签。</li><li>另外不是从teacher的权重开始finetune一个student，而是从头训student。</li><li>数据增强主要考虑两个，一个是color distortions，包括color jittering和gaussian blurring, 一个是CutMix。</li><li>另外，作者还加了一个feature alignment loss来约束encoder学习到更具有语义性的特征。</li></ul><!-- ![图3. feature alignment loss.](L_feat.png) --><img src="L_feat.png" alt="图3. feature alignment loss." width="50%"><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p><img src="exps.png" alt="Experiments."></p><h2 id="Thoughts"><a href="#Thoughts" class="headerlink" title="Thoughts"></a>Thoughts</h2><ul><li>利用无标签数据，需要设计比pseudo label更难的任务来迫使模型学习，并且最好是使用一个有充分语义的pretrain来带来额外监督。DINOv2好像本身就具有较强的深度估计能力，换其他任务怎么选这个语义pretrain可能另有说法。</li></ul>]]></content>
      
      
      <categories>
          
          <category> Computer Vision </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Depth Estimation </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>DepthAnything V2论文笔记</title>
      <link href="/2024/06/11/computervision/depthanythingv2/depthanythingv2/"/>
      <url>/2024/06/11/computervision/depthanythingv2/depthanythingv2/</url>
      
        <content type="html"><![CDATA[<blockquote><p>Title: <a href="https://arxiv.org/abs/2406.09414">Depth Anything V2</a> <br>From HKU &amp; TikTok. <br>ArXiv 2024.06.13</p></blockquote><h2 id="Highlight"><a href="#Highlight" class="headerlink" title="Highlight"></a>Highlight</h2><ul><li>本文从分析最近的两个深度估计的工作Depth Aynthing V1和Diffusion-based深度估计工作Marigold的性能对比来总结各自的优缺点，然后提出改进方向。</li><li>如图1所示，作者发现Depth Anything V1对细节估计效果差一些，而Marigold对于一些复杂场景的预测效果会差一点。</li><li>作者提出导致这两个问题的根源不是由于模型本身的差异，而是训练数据。</li></ul><p><img src="Preferable_properties.jpg" alt="图1. Preferable Properties."></p><h2 id="Methods"><a href="#Methods" class="headerlink" title="Methods"></a>Methods</h2><ul><li>方法分为三步</li><li>（1）只使用合成数据来训练largest teacher模型，因为合成数据的深度信息准确度是非常高的，但是缺点就是和真实数据有一定domain gap。</li><li>（2）因此，第二步是在大规模真实数据上使用largest teacher打伪标签。</li><li>（3）第三步是在pseudo-labeled real images 上训student model。</li></ul><p><img src="DepthAnythingV2.jpg" alt="图2. DepthAnything V2."></p><ul><li><p>loss上使用的是scale and shift-invariant loss $L_{ssi}$ and a gradient matching loss $L_{gm}$. 并且忽略了top-n(10%)-largest regions.</p></li><li><p>不过文章并没有说明，第二步打的伪标签是准确的，因为毕竟第一步说了和真实数据有domain gap，那么打的伪标签也有这个问题的话对第三步影响岂不是很大？</p></li></ul><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p><img src="performance.jpg" alt="Experiments."></p><h2 id="Thoughts"><a href="#Thoughts" class="headerlink" title="Thoughts"></a>Thoughts</h2><ul><li>如何缩小合成数据和真实数据之间的domain gap。</li></ul>]]></content>
      
      
      <categories>
          
          <category> Computer Vision </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Depth Estimation </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Diff-Instruct论文笔记</title>
      <link href="/2024/04/18/computervision/diffinstruct/diffinstruct/"/>
      <url>/2024/04/18/computervision/diffinstruct/diffinstruct/</url>
      
        <content type="html"><![CDATA[<blockquote><p>Title: <a href="https://arxiv.org/abs/2305.18455">Diff-Instruct: A Universal Approach for Transferring Knowledge From Pre-trained Diffusion Models</a> <br>From PKU and Noah’s Ark Lab. <br>NeurIPS 2023</p></blockquote><p><img src="IllustrationofourDiff-Instructpipeline.jpg" alt="图1. Illustration of Diff-Instruct pipeline."></p><h2 id="Highlight"><a href="#Highlight" class="headerlink" title="Highlight"></a>Highlight</h2><ul><li>作者提出了一种data-free的diffusion模型蒸馏方法Diff-Instruct，既可以用来蒸馏diffusion模型，又可以用来蒸馏GAN模型，只要生成图像可导的模型都能蒸。</li><li>提出最小化积分KL散度来进行蒸馏。目的是在不使用任何训练数据的情况下，训练一个隐式的模型g_\theta，这个模型一步生成图像的数据分布和可以和pre-trained DM的分布一致。所谓积分KL散度，指的是利用合适的加权策略将不同time levels的知识结合然后进行指导。</li></ul><h2 id="Methods"><a href="#Methods" class="headerlink" title="Methods"></a>Methods</h2><ul><li>我们的目的是让g一步生成的fake分布和pretrain-DM生成的real分布对齐。</li><li>首先我们要训一个online sg模型来建模fake分布，那我们知道diffusion模型本身是可以建模数据分布的，那我们就可以让这个online sg来输入g生成的图像然后去噪，这样sg就逐渐学着建模到了fake分布。</li><li>然后我们就可以算fake部分和pretrain-DM生成的real分布之间的IKL散度了，直接用于更新g的参数\theta。</li></ul><p><img src="IKL_div.jpg" alt="图2. Integral KL divergence."><br><img src="The-gradient-of-the-IKL.jpg" alt="图3. The gradient of the IKL."><br><img src="Diff-Instruct-Algorithm.jpg" alt="图4. Diff-Instruct Algorithm."></p><ul><li>和GAN的discriminator相比，Diff-Instruct是一种交替式的非对抗式的训练策略。</li></ul><h2 id="CodeSnippets"><a href="#CodeSnippets" class="headerlink" title="CodeSnippets"></a>CodeSnippets</h2><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># Accumulate gradients.</span>optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span>set_to_none<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token keyword">for</span> round_idx <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>num_accumulation_rounds<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        G<span class="token punctuation">.</span><span class="token builtin">eval</span><span class="token punctuation">(</span><span class="token punctuation">)</span>        z <span class="token operator">=</span> init_sigma<span class="token operator">*</span>torch<span class="token punctuation">.</span>randn_like<span class="token punctuation">(</span>images<span class="token punctuation">)</span>        gen_images <span class="token operator">=</span> G<span class="token punctuation">(</span>            z<span class="token punctuation">,</span>             init_sigma<span class="token operator">*</span>torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span>z<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>z<span class="token punctuation">.</span>device<span class="token punctuation">)</span><span class="token punctuation">,</span>             labels<span class="token punctuation">,</span>             augment_labels<span class="token operator">=</span>torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>z<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">9</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>z<span class="token punctuation">.</span>device<span class="token punctuation">)</span><span class="token punctuation">)</span>        G<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token comment"># 计算去噪loss更新sg</span>    loss <span class="token operator">=</span> loss_fn<span class="token punctuation">(</span>net<span class="token operator">=</span>Sgddp<span class="token punctuation">,</span> images<span class="token operator">=</span>gen_images<span class="token punctuation">,</span> labels<span class="token operator">=</span>labels<span class="token punctuation">,</span> augment_pipe<span class="token operator">=</span>augment_pipe<span class="token punctuation">)</span>    training_stats<span class="token punctuation">.</span>report<span class="token punctuation">(</span><span class="token string">'SgLoss/loss'</span><span class="token punctuation">,</span> loss<span class="token punctuation">)</span>    loss<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>mul<span class="token punctuation">(</span>sgls <span class="token operator">/</span> batch_gpu_total<span class="token punctuation">)</span><span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>g_optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span>set_to_none<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token keyword">for</span> round_idx <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>num_accumulation_rounds<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">with</span> misc<span class="token punctuation">.</span>ddp_sync<span class="token punctuation">(</span>Gddp<span class="token punctuation">,</span> <span class="token punctuation">(</span>round_idx <span class="token operator">==</span> num_accumulation_rounds <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        gen_images <span class="token operator">=</span> Gddp<span class="token punctuation">(</span>            z<span class="token punctuation">,</span>             init_sigma<span class="token operator">*</span>torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span>z<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>z<span class="token punctuation">.</span>device<span class="token punctuation">)</span><span class="token punctuation">,</span>             labels<span class="token punctuation">,</span>             augment_labels<span class="token operator">=</span>torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>z<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">9</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>z<span class="token punctuation">.</span>device<span class="token punctuation">)</span>        <span class="token punctuation">)</span>                Sg<span class="token punctuation">.</span><span class="token builtin">eval</span><span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token comment"># IKL Loss更新g</span>        loss <span class="token operator">=</span> loss_scaling<span class="token operator">*</span>loss_fn<span class="token punctuation">.</span>gloss<span class="token punctuation">(</span>Sd<span class="token operator">=</span>net<span class="token punctuation">,</span> Sg<span class="token operator">=</span>Sg<span class="token punctuation">,</span> images<span class="token operator">=</span>gen_images<span class="token punctuation">,</span> labels<span class="token operator">=</span>labels<span class="token punctuation">,</span> augment_pipe<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span>        Sg<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token punctuation">)</span>        loss <span class="token operator">=</span> loss<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">)</span>        training_stats<span class="token punctuation">.</span>report<span class="token punctuation">(</span><span class="token string">'GLoss/loss'</span><span class="token punctuation">,</span> loss<span class="token punctuation">)</span>        loss<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>mul<span class="token punctuation">(</span><span class="token number">1.0</span> <span class="token operator">/</span> batch_gpu_total<span class="token punctuation">)</span><span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>g_optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment"># IKL loss</span><span class="token keyword">def</span> <span class="token function">gloss</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> Sd<span class="token punctuation">,</span> Sg<span class="token punctuation">,</span> images<span class="token punctuation">,</span> labels<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> augment_pipe<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    rnd_normal <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token punctuation">[</span>images<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> device<span class="token operator">=</span>images<span class="token punctuation">.</span>device<span class="token punctuation">)</span>    sigma <span class="token operator">=</span> <span class="token punctuation">(</span>rnd_normal <span class="token operator">*</span> self<span class="token punctuation">.</span>P_std <span class="token operator">+</span> self<span class="token punctuation">.</span>P_mean<span class="token punctuation">)</span><span class="token punctuation">.</span>exp<span class="token punctuation">(</span><span class="token punctuation">)</span>    weight <span class="token operator">=</span> <span class="token number">1.0</span>        y<span class="token punctuation">,</span> augment_labels <span class="token operator">=</span> augment_pipe<span class="token punctuation">(</span>images<span class="token punctuation">)</span> <span class="token keyword">if</span> augment_pipe <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span> <span class="token keyword">else</span> <span class="token punctuation">(</span>        images<span class="token punctuation">,</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>images<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">9</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>images<span class="token punctuation">.</span>device<span class="token punctuation">)</span><span class="token punctuation">)</span>    n <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn_like<span class="token punctuation">(</span>y<span class="token punctuation">)</span> <span class="token operator">*</span> sigma        Sg<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> Sd<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        cuda_rng_state <span class="token operator">=</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>get_rng_state<span class="token punctuation">(</span><span class="token punctuation">)</span>        Dd_yn <span class="token operator">=</span> Sd<span class="token punctuation">(</span>y <span class="token operator">+</span> n<span class="token punctuation">,</span> sigma<span class="token punctuation">,</span> labels<span class="token punctuation">,</span> augment_labels<span class="token operator">=</span>augment_labels<span class="token punctuation">)</span>        torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>set_rng_state<span class="token punctuation">(</span>cuda_rng_state<span class="token punctuation">)</span>        Dg_yn <span class="token operator">=</span> Sg<span class="token punctuation">(</span>y <span class="token operator">+</span> n<span class="token punctuation">,</span> sigma<span class="token punctuation">,</span> labels<span class="token punctuation">,</span> augment_labels<span class="token operator">=</span>augment_labels<span class="token punctuation">)</span>    Sd<span class="token punctuation">.</span><span class="token builtin">eval</span><span class="token punctuation">(</span><span class="token punctuation">)</span>        loss <span class="token operator">=</span> weight <span class="token operator">*</span> <span class="token punctuation">(</span><span class="token punctuation">(</span>Dg_yn <span class="token operator">-</span> Dd_yn<span class="token punctuation">)</span> <span class="token operator">*</span> images<span class="token punctuation">)</span>  <span class="token comment"># 对这个损失求导就是公式3</span>        <span class="token keyword">return</span> loss<span class="token operator">*</span> 疑问，没有求积分的操作，是通过多次不同的sigma采样来近似的吗？<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p><img src="exps.jpg" alt="Experiments."></p>]]></content>
      
      
      <categories>
          
          <category> Image Generation </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Diffusion Distillation </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>VAR论文笔记</title>
      <link href="/2024/04/07/computervision/var/var/"/>
      <url>/2024/04/07/computervision/var/var/</url>
      
        <content type="html"><![CDATA[<blockquote><p><strong>Title: <a href="https://arxiv.org/abs/2404.02905">Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale Prediction</a></strong> <br><strong>From Bytedance Inc.</strong> <br><strong>ArXiv 20240403</strong></p></blockquote><h2 id="Highlight"><a href="#Highlight" class="headerlink" title="Highlight"></a>Highlight</h2><ul><li>作者提出了一个新的自回归图像生成范式VAR(Visual AutoRegressive modeling)来预测下一个scale/resolution。</li><li>实验验证了VAR具备更好的Scaling Laws和zero-shot泛化能力。</li></ul><h2 id="Methods"><a href="#Methods" class="headerlink" title="Methods"></a>Methods</h2><h3 id="VAR"><a href="#VAR" class="headerlink" title="VAR"></a>VAR</h3><p><img src="AR.png" alt="图1. Standard autoregressive modeling (AR) vs. our proposed visual autoregressive modeling (VAR)."></p><ul><li>图1展示的是标准的自回归模型和本文提出的新的视觉自回归模型VAR的对比。</li><li>因为自回归任务需要满足unidirectional特性，也就是说下一个tokens的预测只和前序的tokens有关系，和后面的没关系，但是AR经过quantization 和 flattening，就破坏了这个特性。由于flattening，还破坏了空间位置关系。</li><li>因此，作者提出预测下一个分辨率的tokens来自回归生成图像。在rk次生成中，hk × wk tokens的所有分布都是相互有关系的，并且是被并行生成的。使用rk的前序和k-th position embedding map作为condition.</li><li>VAR包含两个训练步骤。首先在图像上使用重建loss训练一个多尺度的VAQVAE，作为步骤2训练的gt，第二步是使用最大似然（最小化交叉熵loss）在tokens上通过next-scale prediction训练VAR Transoformer。<br><img src="VAR.png" alt="图2. VAR involves two separated training stages."></li></ul><h3 id="Tokenization"><a href="#Tokenization" class="headerlink" title="Tokenization"></a>Tokenization</h3><ul><li>图3展示的是第一阶段多尺度VQVAE编码和重建算法步骤。作者使用了残差的设计来获取更好的效果。同时为了降低信息损失在每个插值后接了多个conv。</li></ul><p><img src="Algorithms.png" alt="图3. Algorithm: Multi-scale VQVAE Encoding and Multi-scale VQVAE Reconstruction ."></p><h3 id="VAR-transformer"><a href="#VAR-transformer" class="headerlink" title="VAR transformer"></a>VAR transformer</h3><ul><li>为了简单，作者直接使用了GPT-2和VQGAN的标准的decoder-only的结构。</li><li>使用了AdaLN替代LN。</li><li>对于带条件的生成，直接使用class embeddings作为start token [s]。</li></ul><h2 id="Methods-1"><a href="#Methods-1" class="headerlink" title="Methods"></a>Methods</h2><p><img src="comparisons.png" alt="图4. Generative model family comparison on class-conditional ImageNet 256×256."></p><p><img src="Scaling_behavior.png" alt="图5. Scaling behavior of different models on ImageNet 256×256 conditional generation benchmark."></p><h2 id="Thoughts"><a href="#Thoughts" class="headerlink" title="Thoughts"></a>Thoughts</h2><ul><li>VQVAE这种Tokenization编码方式还是有人一直在研究，看起来这种方式具有一定的意义。</li><li>什么样的重建方式的scaling laws是更好的呢？</li></ul>]]></content>
      
      
      <categories>
          
          <category> Image Generation </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Auto Regressive Model </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SDXS论文笔记</title>
      <link href="/2024/04/02/computervision/sdxs/sdxs/"/>
      <url>/2024/04/02/computervision/sdxs/sdxs/</url>
      
        <content type="html"><![CDATA[<blockquote><p><strong>Title: <a href="https://arxiv.org/abs/2403.16627">SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions</a></strong> <br><strong>From Xiaomi Inc.</strong> <br><strong>ArXiv 20240325</strong></p></blockquote><p><img src="SDXS.png" alt="图1. SDXS生成图展示."></p><h2 id="Highlight"><a href="#Highlight" class="headerlink" title="Highlight"></a>Highlight</h2><ul><li>作者提出了一个同时做diffusion模型轻量化和采样步数压缩的框架SDXS。</li><li>该框架可以一步生成图像并且可以很好的迁移到image-conditioned ControlNet的训练中。</li><li>对于模型压缩部分，主要关注于VAE的decoder和UNet的压缩，因为这两块占据的资源消耗是最多的。</li><li>对于采样步数压缩部分，使用feature matching loss来代替蒸馏loss，<br>并且进一步地，扩展了Diff-Instruct训练策略，在timestep的后半部分，使用了feature matching loss的梯度来代替score distillation的梯度</li></ul><p><img src="Network_architecture_distillation.png" alt="图2. Network architecture distillation."></p><p><img src="LatencyComparison.png" alt="图3. Latency Comparison."></p><h2 id="Methods"><a href="#Methods" class="headerlink" title="Methods"></a>Methods</h2><h3 id="Diff-Instruct"><a href="#Diff-Instruct" class="headerlink" title="Diff-Instruct"></a>Diff-Instruct</h3><ul><li>这一部分的基础知识可以参考对<a href="https://bigwode.github.io/2024/03/21/computervision/swiftbrush/swiftbrush/">DreamFusion</a>中的介绍。</li></ul><h3 id="Architecture-Optimizations"><a href="#Architecture-Optimizations" class="headerlink" title="Architecture Optimizations"></a>Architecture Optimizations</h3><ul><li>对VAE decoder的压缩是结合了output distillation loss和GAN loss来训练一个轻量化的image decoder来模仿原始的VAE decoder的输出。如公式1所示，G()是我要训练的tiny decoder，x是原始的decoder出来的图像，下采样了8倍计算的L1 loss, 说是为了减少冗余。使用的是一些包含残差块和上采样层的纯CNN结构。</li></ul><p><img src="L_VD.png" alt="公式1. L_VD."></p><ul><li><p>对于UNet，使用的是同时监督输出的OKD损失和监督中间层特征的FKD损失。如公式2所示。作者没有对其加原始的去噪损失。<br><img src="L_OKD.png" alt="公式2. L_OKD."></p></li><li><p>借鉴了block removal distillation策略，移除了对latency贡献最多的几个模块。比如，对于SD-2.1 base来说，去掉了middle stage，downsampling stage的最后一个stage，和upsampling的第一个stage，去除了最高分辨率stage的所有Transformer结构。</p></li><li><p>对于ontrolNet的蒸馏，也是蒸馏了UNet中间特征层和最终的输出, 考虑到ControlNet不影响原始的UNet encoder的特征层，所以特征蒸馏只是在UNet的decoder上去做的。</p></li></ul><h3 id="One-Step-Training"><a href="#One-Step-Training" class="headerlink" title="One-Step Training"></a>One-Step Training</h3><ul><li>Feature Matching Warmup.</li><li>这里借鉴了ReFlow的思想来讲故事，不过使用的是一个叫加权版SSIM的FM损失，如公式3所示。<br><img src="L_FM.png" alt="公式3. L_FM."></li><li>约束tiny unet生成的图像和原始的UNet使用ODE采样器生成的图像两个过encoder $f_{\theta}$ 在不同层上的特征的SSIM一致，不过作者使用FM损失只是用于warmup少量的训练steps。</li></ul><h3 id="Segmented-Diff-Instruct"><a href="#Segmented-Diff-Instruct" class="headerlink" title="Segmented Diff-Instruct."></a>Segmented Diff-Instruct.</h3><ul><li><p>虽然feature matching loss可以生成清晰的图像，但是毕竟还不是直接和真实分布的匹配关系，所以只能用来当初始化。</p></li><li><p>所以借鉴Diff-Instruct来匹配模型输出的分布。</p></li><li><p>作者发现在采样轨迹是corse to fine的，t-&gt;T，score function提供了低频信息的梯度，t-&gt;0，提供的是高频信息的梯度。因此作者把采样过程分成了两部分，[0, \alpha T]和(\alpha T, T]，后者使用L_FM来代替，因为它可以提供足够多的低频信息。* 随着训练过程，逐渐降低FM损失的权重。</p></li></ul><p><img src="L_IKL.png" alt="公式4. L_IKL."></p><p><img src="one-step_U-Net_training_strategy.png" alt="图4. one-step U-Net training strategy."></p><ul><li>实际中，online DM和student DM是交替训练的，并且在训练的中间阶段，将teacher DM 从预训练模型切换为经过训练的小型化一步模型本身，进一步减少了训练开销。</li></ul><p><img src="Performace.png" alt="图5. Performace."></p><h2 id="Thoughts"><a href="#Thoughts" class="headerlink" title="Thoughts"></a>Thoughts</h2><ul><li>DM蒸馏框架envolve的模型太多，蒸馏的过程显得很臃肿。不直接对Student DM使用去噪损失L_DM，而是把student模型生成的图像加噪后过Online DM和Offline DM算IKL损失和DM损失，反传梯度给student DM。这样做为什么会比直接对student算DM损失效果好？Online DM是必须的吗？</li></ul>]]></content>
      
      
      <categories>
          
          <category> Image Generation </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Diffusion Distillation </tag>
            
            <tag> Stable Diffusion </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Clockwork Diffusion论文笔记</title>
      <link href="/2024/03/26/computervision/clockwork-diffusion/clockwork-diffusion/"/>
      <url>/2024/03/26/computervision/clockwork-diffusion/clockwork-diffusion/</url>
      
        <content type="html"><![CDATA[<blockquote><p><strong>Title: <a href="https://arxiv.org/abs/2312.08128">Clockwork Diffusion: Efficient Generation With Model-Step Distillation</a></strong> <br><strong>From Qualcomm AI Research.</strong> <br><strong>CVPR 2024.</strong></p></blockquote><p><img src="Perturbing.jpg" alt="图1.Perturbing Stable Diffusion v1.5 UNet representations."></p><h2 id="Highlight"><a href="#Highlight" class="headerlink" title="Highlight"></a>Highlight</h2><ul><li>作者发现并不是denoising model UNet里的所有operation都和最终输出的图像质量有关系。具体地说，UNet在高分辨率feature maps上的操作对于小的扰动来说更敏感一些，低分辨率feature maps影响的是图像的semantic layout，对于小的扰动来说不敏感。</li><li>基于这个发现，作者提出了Clockwork Diffusion。周期性地重复使用先前去噪步骤的计算来近似接下来的一个或多个步骤中的低分辨率特征图。周期性交替使用近似步骤和运行UNet的全部流程来避免误差累计的问题。</li></ul><h2 id="Methods"><a href="#Methods" class="headerlink" title="Methods"></a>Methods</h2><p><img src="Clockwork.jpg" alt="图2.Schematic view of Clockwork."></p><ul><li>作者说提出的方法是model distillation 和 step distillation的结合。</li><li>既然低分辨率对扰动不敏感，那我们就可以用一个更小的adaptor来代替原始的UNet的低分辨率部分。同时接收了起那一个采样步的features。</li></ul><p><img src="results.png" alt="图3. Text guided image generation results."></p><h2 id="Thoughts"><a href="#Thoughts" class="headerlink" title="Thoughts"></a>Thoughts</h2><ul><li>通过分析UNet在不同步骤不同层的特征表现来节省时间是有道理的。</li></ul>]]></content>
      
      
      <categories>
          
          <category> Image Generation </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Diffusion Distillation </tag>
            
            <tag> UNet </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SwiftBrush论文笔记</title>
      <link href="/2024/03/21/computervision/swiftbrush/swiftbrush/"/>
      <url>/2024/03/21/computervision/swiftbrush/swiftbrush/</url>
      
        <content type="html"><![CDATA[<blockquote><p><strong>Title: <a href="https://arxiv.org/abs/2312.05239">SwiftBrush: One-Step Text-to-Image Diffusion Model with Variational Score Distillation</a></strong> <br><strong>From VinAI Research.</strong> <br><strong>CVPR 2024</strong></p></blockquote><p><img src="SwiftBrush_overview.jpg" alt="图1.SwiftBrush overview."></p><h2 id="Highlight"><a href="#Highlight" class="headerlink" title="Highlight"></a>Highlight</h2><ul><li>作者提出了一个image-free的蒸馏方法SwiftBrush.</li><li>已有方法Score Distillation Sampling (SDS)有过饱和，过平滑和多样性差的问题，本文基于SDS来提出了Variational Score Distillation (VSD) 的方法。</li></ul><h2 id="Methods"><a href="#Methods" class="headerlink" title="Methods"></a>Methods</h2><h3 id="DREAMFUSION-TEXT-TO-3D-USING-2D-DIFFUSION"><a href="#DREAMFUSION-TEXT-TO-3D-USING-2D-DIFFUSION" class="headerlink" title="DREAMFUSION: TEXT-TO-3D USING 2D DIFFUSION"></a>DREAMFUSION: TEXT-TO-3D USING 2D DIFFUSION</h3><ul><li>因为文章提到的score distillation的思想来自于论文DREAMFUSION，所以我们这里先简单介绍一下score distilltion是怎么来的。</li><li>DreamFusion的提出来自于，3D生成需要大规模的有标签的3D 数据，并且需要对3D数据的去噪设计高效的结构。</li><li>那我们能不能把pretrain好的2D的text-to-iamge的diffusion模型用来做text-to-3D的生成呢？</li><li>DreamFusion作者提出了一个基于概率密度蒸馏的损失函数，可以使用2D扩散模型作为先验，优化一个参数化图像生成器。</li><li>使用这个loss，我们可以通过梯度下降来优化一个随机初始化的3D模型（NeRF），这个3D模型从任意角度的的2D渲染图像可以获得一个更低的loss。</li><li>这里的3D模型可以理解为一个深度模型，他建模了一个3D模型，我输入一个角度信息，他就可以给我一个渲染后的2D图像。</li></ul><h3 id="HOW-CAN-WE-SAMPLE-IN-PARAMETER-SPACE-NOT-PIXEL-SPACE"><a href="#HOW-CAN-WE-SAMPLE-IN-PARAMETER-SPACE-NOT-PIXEL-SPACE" class="headerlink" title="HOW CAN WE SAMPLE IN PARAMETER SPACE, NOT PIXEL SPACE?"></a>HOW CAN WE SAMPLE IN PARAMETER SPACE, NOT PIXEL SPACE?</h3><ul><li>可微图像参数化（differentiable image parameterization (DIP) ）, 可以把参数{\theta}转换为一个image x=g(\theta)</li><li>然后需要一个可微的损失函数来使得渲染出来的好的图像有更低的loss。</li><li>但是直接约束diffusion training loss不能生成一个真实的图像，为了理解为啥，作者计算了L_{diff}的梯度。</li><li>作者发现在实际中，U-Net的雅可比项是expensive to compute的（需要通过diffusion模型UNet反传）, 并且不适合小噪声水平，因为它是被训练来近似边缘密度的缩放 Hessian 矩阵的。作者发现忽略U-Net的雅可比项可以有更好的梯度来优化DIPs。</li><li>可以跟随diffusion模型的score function来移动到一个更高密度的区域。</li></ul><p><img src="L_diff.jpg" alt="图2.L_{Diff}."></p><p><img src="gradient_L_diff.jpg" alt="图3.The gradient of L_{Diff}."></p><p><img src="eff_gradient_L_diff.jpg" alt="图4.The effective gradient of L_{Diff}."></p><h3 id="Variational-Score-Distillation-VSD"><a href="#Variational-Score-Distillation-VSD" class="headerlink" title="Variational Score Distillation (VSD)"></a>Variational Score Distillation (VSD)</h3><p><img src="L_VSD.jpg" alt="图5.L_{VSD}."></p><ul><li>a pre-trained text-to-image teacher εψ and one additional LoRA teacher εφ.</li><li>train both the student model and the LoRA teacher alternately using Eq. (5) and Eq. (6)。</li></ul><p><img src="SwiftBrush_distillation.jpg" alt="图6.SwiftBrush distillation."></p><h2 id="Thoughts"><a href="#Thoughts" class="headerlink" title="Thoughts"></a>Thoughts</h2><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li><a href="https://arxiv.org/abs/2209.14988">DREAMFUSION: TEXT-TO-3D USING 2D DIFFUSION</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> Image Generation </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Diffusion Distillation </tag>
            
            <tag> Stable Diffusion </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CoDi论文笔记</title>
      <link href="/2024/03/17/computervision/codi/codi/"/>
      <url>/2024/03/17/computervision/codi/codi/</url>
      
        <content type="html"><![CDATA[<blockquote><p><strong>Title: <a href="https://arxiv.org/abs/2310.01407">CoDi: Conditional Diffusion Distillation for Higher-Fidelity and Faster Image Generation</a></strong> <br><strong>From Google Research.</strong> <br><strong>CVPR 2024.</strong></p></blockquote><p><img src="CoDi_Network.jpg" alt="图1.Network architecture illustration of CoDi."></p><h2 id="Highlight"><a href="#Highlight" class="headerlink" title="Highlight"></a>Highlight</h2><ul><li>本文在Consistency Models一致性模型的基础上，拓展到了带条件的生成模型的蒸馏上。</li><li>其他的还有几个点是作者声称可以original training-data free的和parameters efficient的蒸馏。</li></ul><h2 id="Methods"><a href="#Methods" class="headerlink" title="Methods"></a>Methods</h2><h3 id="Adapt-the-unconditional-diffusion-model-into-a-conditional-version"><a href="#Adapt-the-unconditional-diffusion-model-into-a-conditional-version" class="headerlink" title="Adapt the unconditional diffusion model into a conditional version"></a>Adapt the unconditional diffusion model into a conditional version</h3><ul><li>通过直接复制pretrained network的encoder层来引入conditional-module，其中 $h_{\theta}(.)$ 是pretrain模型的encoder feature，$h_{\eta}(.)$ 是新增的条件encoder，$\mu$ 是一个可学习的参数，初始化为0.<br><img src="conditional_encoder_params.jpg" alt="图2.new encoder features of the adapted model."></li></ul><h3 id="A-New-Conditional-Diffusion-Consistency"><a href="#A-New-Conditional-Diffusion-Consistency" class="headerlink" title="A New Conditional Diffusion Consistency"></a>A New Conditional Diffusion Consistency</h3><ul><li>Remark 1. 作者这里说在noise prediction上满足self-coinsistency性质的diffusion model，在signal prediction上也同样满足。</li><li>这是因为原始的Consistency model是约束的PF ODE轨迹上两个点的去噪图像的一致性，而这里作者用了预测噪声一致性来代替。这里的边界条件也是通过喝consistency model一样的skip connection来约束的。另外还有一个是学习在去噪图像基础上，学习新的条件生成。作者说这样两者的优化方向就不会冲突。。</li></ul><p><img src="conditional_distillation.jpg" alt="图3.training loss for conditional distillation."></p><ul><li>其中，$ Z^{hat}_{s}s $ 是通过adapted条件生成模型预测得到的。和consistency model一样也使用了EMA target模型来稳定训练。</li></ul><p><img src="Z_hat_s.jpg" alt="图4.training loss for conditional distillation."></p><h3 id="Parameter-Efficient-Conditional-Distillation"><a href="#Parameter-Efficient-Conditional-Distillation" class="headerlink" title="Parameter-Efficient Conditional Distillation"></a>Parameter-Efficient Conditional Distillation</h3><ul><li>作者生成自己是Parameter-Efficient Conditional Distillation，因为只更新了复制出来的encoder。</li><li>这里应该只用了new conditional data来训的蒸馏，所以作者论文里声称不需要原始的text-to-image data。</li><li>其实这里的条件控制是图像，有点类似于超分的任务。</li></ul><p><img src="CDD.jpg" alt="图5.Algorithm 1 Conditional Diffusion Distillation (CDD)."></p><h2 id="Thoughts"><a href="#Thoughts" class="headerlink" title="Thoughts"></a>Thoughts</h2><ul><li>通过蒸馏来缩减步数看上去只蒸结构的一部分也行，蒸哪一部分最重要呢？</li></ul>]]></content>
      
      
      <categories>
          
          <category> Image Generation </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Diffusion Distillation </tag>
            
            <tag> Consistency Models </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SD3_Turbo论文笔记</title>
      <link href="/2024/03/17/computervision/sd3-turbo/sd3-turbo/"/>
      <url>/2024/03/17/computervision/sd3-turbo/sd3-turbo/</url>
      
        <content type="html"><![CDATA[<blockquote><p><strong>Title: <a href="https://arxiv.org/abs/2403.12015">Fast High-Resolution Image Synthesis with Latent Adversarial Diffusion Distillation</a></strong> <br><strong>From Stability AI.</strong> <br><strong>ArXiv 2024.03.18</strong></p></blockquote><p><img src="Comparing_ADD_and_LADD.jpg" alt="图1.Comparing ADD and LADD."></p><h2 id="Highlight"><a href="#Highlight" class="headerlink" title="Highlight"></a>Highlight</h2><ul><li>本文对标的是Stability AI 同一个团队在之前提出的方法 Adversarial Diffusion Distillation (ADD)。本文首先指出ADD的缺点是需要依赖DINOv2来作为判别器，计算消耗过大且输入必须在pixel space上，导致可拓展性差。</li><li>本文提出的LADD直接在latent上做判别，避免了decoding到pixel space才能算loss的复杂计算。并且直接利用teacher模型当判别器，避免了ADD的缺点。</li><li>值得一提的是，LADD直接用teacher模型生成的数据进行student模型的学习，并且去掉了蒸馏loss，只使用discriminator loss就取得了很好的效果。</li><li>本文还研究了方法是否遵循scaling laws，得出结论是student模型的大小比teacher模型的大小以及训练数据的质量要更加重要。后两者很快会遇到性能的瓶颈。</li></ul><h2 id="Methods"><a href="#Methods" class="headerlink" title="Methods"></a>Methods</h2><h3 id="Unifying-teacher-and-discriminator"><a href="#Unifying-teacher-and-discriminator" class="headerlink" title="Unifying teacher and discriminator."></a>Unifying teacher and discriminator.</h3><ul><li>利用pretrained的diffusion模型的生成特征做判别器的好处是，可以对噪声程度进行针对性采样，当添加噪声程度高的时候，会更加关注global特征，当添加特征程度低的时候，会更加关注local特征，以此获取更好的balance。</li><li>teacher生成的图像加噪送到teacher里和student生成的图像进行加噪后不可区分，另外这样我们就不需要真实图像去训练了！</li><li>具体地，在得到的每一个token序列里，应用单独的判别器heads。另外，每一个判别器conditioned by noise level和池化后的CLIP Embedding。</li><li>对于判别器的结构，作者没有利用1D卷积，而是把token序列reshape成原始的spatial layout，然后使用2D卷积。这样做的原因是对于不同长宽比的图像生成，1D卷积会用不同的strides来处理token序列。</li></ul><h3 id="Leveraging-synthetic-data"><a href="#Leveraging-synthetic-data" class="headerlink" title="Leveraging synthetic data."></a>Leveraging synthetic data.</h3><ul><li>使用生成图像训练student是为了有更加均衡的image-text对齐的数据，可以被视作另一种蒸馏teacher知识的方法。</li><li>作者通过对比实验发现，真实数据对蒸馏loss有用，但是生成数据没有用。因此，作者只用了生成数据和advcersarial loss来训练student。</li></ul><h2 id="Thoughts"><a href="#Thoughts" class="headerlink" title="Thoughts"></a>Thoughts</h2><ul><li>使用teacher模型生辰数据训练student和用teacher模型当判别器两个点都是很make sense的点。</li></ul>]]></content>
      
      
      <categories>
          
          <category> Image Generation </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Diffusion Distillation </tag>
            
            <tag> Stable Diffusion </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Consistency Models论文笔记</title>
      <link href="/2024/03/12/computervision/consistency-models/consistency-models/"/>
      <url>/2024/03/12/computervision/consistency-models/consistency-models/</url>
      
        <content type="html"><![CDATA[<blockquote><p><strong>Title: <a href="https://arxiv.org/abs/2303.01469">Consistency Models</a></strong> <br><strong>From OpenAI, Yang Song.</strong> <br><strong>ICML 2023.</strong></p></blockquote><p><img src="Consistency_Models.jpg" alt="图1.Consistency Models."></p><h2 id="Highlight"><a href="#Highlight" class="headerlink" title="Highlight"></a>Highlight</h2><ul><li>所谓的Consistency Models一致性模型，是约束在同一条PF ODE轨迹上的任意点都对应相同的输出。满足这样的性质的生成模型则可获得不错的一步生成效果。</li></ul><h2 id="Background-for-Score-based-Models"><a href="#Background-for-Score-based-Models" class="headerlink" title="Background for Score-based Models"></a>Background for Score-based Models</h2><ul><li>先来介绍一下关于score based model的一些基础知识。</li></ul><p><img src="SDE.jpg" alt="图2.SDE."></p><ul><li>如图2所示，Diffusion model对原始的数据的扩散过程（加噪过程）是一个随机过程，我们可以用一个随机微分方程SDE来建模它。</li><li>这个SDE有个非常好的性质是存在一个常微分方程ODE，作者叫Probability Flow(PF) ODE，这个ODE的解轨迹在t采样时刻的分布和p_t(x)的score function有关。这个也叫做逆随机过程 Reverse SDE，也就是由噪声到生成图像的过程。如图3所示，</li></ul><p><img src="Reverse_SDE.jpg" alt="图3.Reverse SDE."></p><ul><li><p>把一个SDE转换为ODE的表示的好处是什么呢？首先ODE的轨迹定义了data和noise之间的一对一的映射关系。然后我们就可以用一些ODE的求解器，比如DDIM，DPM-Solver等等去求解这个随机过程。这个发现非常重要，因为它意味着我们可以使用ODE求解器来连续地模拟逆向过程，而不是使用基于MCMC的迭代方法。这种方法可以显著提高采样效率，并且通常能够生成更高质量的样本。【我们熟知的DDIM对应了diffusion ODE的1阶ODE solver，它的加速效果好是因为它考虑了ODE的半线性结构，而DPM-Solver给出了对应的更高阶的solver，可以让10步左右的采样达到与DDPM的1000步的采样相当。】</p></li><li><p>和现有的Denoisers不一样的是，现有的Denoisers往往预测的是后验概率，所以对加噪图像进行去噪的话，更倾向于得到的是一个对不同噪声去噪结果的平均，所以图像会更加blur，而一致性模型预测的是ODE轨迹上的唯一的对应的image，所以其生成效果会sharp一些。</p></li></ul><h2 id="Methods"><a href="#Methods" class="headerlink" title="Methods"></a>Methods</h2><p><img src="Condition.jpg" alt="图4.Consistency model properties."></p><ul><li>如图4所示，一个生成模型f_{\theta}要想具有上面所说的一步生成的能力，那它得满足两条基本的性质。<ul><li>边界条件。当t非常小的时候，输入原图x_0得到的是x_0。这里用一个非常小的t而不是0是为了避免0带来的数值不稳定。这个条件可以通过网络参数化来约束，如图5所示，用skip connection改变一下UNet可以满足。</li><li>Self-Consistency条件。而一致性要求两个不同时刻的采样点的预测结果一致。至于我们怎么找到在同一条ODE解轨迹上的两个点下面再讨论。</li></ul></li></ul><p><img src="boundary_conditions.jpg" alt="图5.Enforce the boundary conditions."></p><ul><li>我们通过一致性模型一步或者多步采样如图6和6-2所示，</li></ul><p><img src="sampling.jpg" alt="图6.Sampling."></p><p><img src="Sampling_Algorithm.jpg" alt="图6-2.Sampling Algorithm."></p><ul><li>到这里，我们要说我们该怎么找到一条ODE轨迹上的两个点来训练一致性模型了。有两种训练方式，一种是通过蒸馏的方式，即我已经有一个训好的diffusion模型了，那我必然知道它每一条ODE解轨迹。另一种是模型通过近似的方式找到两个点自己学。</li></ul><p><strong>1. 通过Distillation训练</strong></p><ul><li>如图7所示，先随机采样一个timestep T_{n+}，然后运行一步ODE到t_{n}，得到的两个点送入模型来使用一致性loss约束。这里的EMA作者在Improved CM论文中说不带EMA效果更好～</li></ul><p><img src="distillation.jpg" alt="图7.Training CM via distillation."></p><p><strong>2. 通过单独训练</strong></p><ul><li>如图8所示，作者在论文中也证明了当t_{n+1}和t_{n}间隔足够小的话，目标函数等价于distillation。</li></ul><p><img src="isolation.jpg" alt="图8.Training CM via isolation."></p><ul><li>训练一致性模型的算法流程图如图9所示。<br><img src="Training_Algorithm.jpg" alt="图9.Training Algorithm."></li></ul><h2 id="Thoughts"><a href="#Thoughts" class="headerlink" title="Thoughts"></a>Thoughts</h2><ul><li>这种一致性约束的思想加在常规的Progressize Diffusion Distillation里不知道效果咋样。</li></ul><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li><a href="https://neurips.cc/virtual/2023/75013">Yang Song‘s presentation about Consistency Models at NeuraIPS 2023.</a></li><li><a href="https://github.com/openai/consistency_models">https://github.com/openai/consistency_models</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> Image Generation </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Consistency Models </tag>
            
            <tag> Diffusion </tag>
            
            <tag> Score-based model </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>DeepCache论文笔记</title>
      <link href="/2024/02/29/computervision/deepcache/deepcache/"/>
      <url>/2024/02/29/computervision/deepcache/deepcache/</url>
      
        <content type="html"><![CDATA[<blockquote><p>Title: <a href="https://arxiv.org/abs/2312.00858">DeepCache: Accelerating Diffusion Models for Free</a> <br>From NUS <br>ArXiv 2023.12.07</p></blockquote><p><img src="Feature_similarity.jpg" alt="Examples of Feature Maps and Similarity."></p><h2 id="Highlight"><a href="#Highlight" class="headerlink" title="Highlight"></a>Highlight</h2><ul><li>作者发现在diffusion生成过程中，相邻步的UNet的high-level特征存在非常高的相似性。</li><li>因此可以把上一步的UNet深层特征存下来，当前步只需要计算浅层特征并和上一步缓存的深层特征Concat。</li></ul><h2 id="Methods"><a href="#Methods" class="headerlink" title="Methods"></a>Methods</h2><p><img src="DeepCache.jpg" alt="Illustration of DeepCache."></p><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p><img src="Results.jpg" alt="Class-conditional generation quality on ImageNet."></p><h2 id="Thoughts"><a href="#Thoughts" class="headerlink" title="Thoughts"></a>Thoughts</h2><ul><li>对于Progressive Diffusion Distillation来说的话，两步蒸馏一步，那说明实际起作用的是在蒸馏浅层feature？但是随着误差累积，深层feature的重要性才逐渐显现出来。</li></ul>]]></content>
      
      
      <categories>
          
          <category> Image Generation </category>
          
      </categories>
      
      
        <tags>
            
            <tag> UNet </tag>
            
            <tag> Diffusion </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Preserving Fairness Generalization in Deepfake Detection 论文笔记</title>
      <link href="/2024/02/28/computervision/preserving-fairness-generalization-in-deepfake-detection/preserving-fairness-generalization-in-deepfake-detection/"/>
      <url>/2024/02/28/computervision/preserving-fairness-generalization-in-deepfake-detection/preserving-fairness-generalization-in-deepfake-detection/</url>
      
        <content type="html"><![CDATA[<blockquote><p><strong>Title:</strong> <a href="https://arxiv.org/abs/2402.17229">《Preserving Fairness Generalization in Deepfake Detection》</a> <br>From Purdue University <br>CVPR 2024 paper</p></blockquote><h2 id="Highlight"><a href="#Highlight" class="headerlink" title="Highlight"></a>Highlight</h2><ul><li>文中提出同时考虑features，loss，和optimization三方面来解决deepfake detection的问题。</li><li>使用解偶学习来提取人种和domain无关的防伪特征，并在一个平坦的loss平面上通过fuse他们来促进fair learning。（平坦的loss平面是使用了SAM优化器。文中提到的fair learning其实是cross domain的generalization的问题，换了种说法而已。）</li></ul><p><img src="Comparison.jpg" alt="Comparison with existing deepfake detection baselines."></p><h2 id="Methods"><a href="#Methods" class="headerlink" title="Methods"></a>Methods</h2><p><img src="overview.jpg" alt="An overview of our proposed method."></p><ul><li>A代表的是Domain Label，例如real, DeepFake，Face2Face等不同伪造的数据集。</li><li>Y代表的是real / fake</li><li>D代表的是人口统计变量，比如根据性别可以分为{male, female}两个subgroups。</li></ul><h3 id="人口统计特征-防伪特征"><a href="#人口统计特征-防伪特征" class="headerlink" title="人口统计特征 &amp; 防伪特征"></a>人口统计特征 &amp; 防伪特征</h3><ul><li>输入的是fake + real的一对儿images；</li><li>包含三个独立的encoders用来提取content特征 c(和图像背景相关？)， 防伪特征 f，人口统计特征 d。</li><li>防伪特征f里同时包含和domain相关f_a的以及和domain无关的f_g，</li></ul><h3 id="Classification-Loss"><a href="#Classification-Loss" class="headerlink" title="Classification Loss"></a>Classification Loss</h3><ul><li>deepfake数据集有人口统计subgroup分布的不均衡问题，所以使用均衡的ditribution-aware margin loss来分类D。</li><li>同时，L_cls还包含区分Y和A的两个CE loss。</li></ul><h3 id="Contrastive-Loss"><a href="#Contrastive-Loss" class="headerlink" title="Contrastive Loss"></a>Contrastive Loss</h3><ul><li>用的是一个triplet loss L_con</li><li>L_con用在f_a和f_g上。</li></ul><h3 id="Reconstruction-Loss"><a href="#Reconstruction-Loss" class="headerlink" title="Reconstruction Loss"></a>Reconstruction Loss</h3><ul><li>L_rec = ||X_i - D(c_i, f_i, d_i)|| + ||X_i - D(c_i, f_i’, d_i)||</li><li>根据输入图像的latent feature来重建，后者是根据partner的forgery特征来进行重建。</li></ul><p><img src="resc.jpg" alt="The architecture details of the decoder."></p><h3 id="Fair-Learning-under-Generalization"><a href="#Fair-Learning-under-Generalization" class="headerlink" title="Fair Learning under Generalization"></a>Fair Learning under Generalization</h3><ul><li>获得了domain无关的防伪特征和人口统计特征，将他们使用AdaIN结合起来, 融合后的特征I_i为：</li></ul><p>I_i = \delta(d_i) * ((f_g - \mu(f_g)) / \delta(f_g)) + \mu(d_i)</p><ul><li>\mu和\delta为在feature的spatial维度上计算的均值和标准差，即将f_g的style转换为d的。</li></ul><h3 id="Fairness-Learning"><a href="#Fairness-Learning" class="headerlink" title="Fairness Learning"></a>Fairness Learning</h3><ul><li>作者提出了bi-level fairness loss在不同的subgroups之间提高公平性。</li><li>文章说的有点复杂不好理解，简单理解就是把domain无关的防伪特征便成对应不同人口统计特征对应的domain下的特征，然后去分类。</li></ul><p><img src="fair_loss.jpg" alt="bi-level fairness loss."><br><img src="algo.jpg" alt="End-to-end Training Algorithm."></p><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p><img src="results.jpg" alt="Ablation study of the loss constraints."></p>]]></content>
      
      
      <categories>
          
          <category> Deepfake Detection </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deepfake Detection </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>TimeSformer论文笔记</title>
      <link href="/2024/02/24/computervision/timesformer/timesformer/"/>
      <url>/2024/02/24/computervision/timesformer/timesformer/</url>
      
        <content type="html"><![CDATA[<blockquote><p><strong>Title:</strong> <a href="https://arxiv.org/abs/2102.05095">《Is Space-Time Attention All You Need for Video Understanding?》</a> <br>From Facebook <br>ICML 2021 paper</p></blockquote><h2 id="Highlight"><a href="#Highlight" class="headerlink" title="Highlight"></a>Highlight</h2><ul><li>背景：第一篇用纯self-attn结构来做视频分类任务的工作。</li><li>文中设计了几种在spatial和temperal上做attn的方法，实验发现将temporal attention和 spatial attention 分开一个接一个交替用可以减少计算量并且获得更好的效果。</li></ul><h2 id="Methods"><a href="#Methods" class="headerlink" title="Methods"></a>Methods</h2><p><img src="video_self-attention_blocks.jpg" alt="Different Video Self-Attn Blocks"></p><ul><li>将每一帧（HxWx3）划分成PxP个不重叠的patches，然后把每个patch flatten成3P^2的vector.</li><li>过一个linear embedding变成D维的，然后加上一个learnable positional embedding。在序列的最开始加上一个token作为[CLS] token。</li><li>为了减少计算量，将temporal attention和 spatial attention 分开一个接一个交替用可以减少计算量并且获得更好的效果。</li></ul><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p><img src="Video-level-accuracy.jpg" alt="Video Level Accuracy"></p><h2 id="Pytorch-Code"><a href="#Pytorch-Code" class="headerlink" title="Pytorch Code"></a>Pytorch Code</h2><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">TimeSFormer</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token comment"># ...</span>    <span class="token keyword">def</span> <span class="token function">forward_features</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>        B <span class="token operator">=</span> x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>  <span class="token comment"># [2, 3, 8, 224, 224]  # BS, image channel, frame（这里以输入8帧图为例）, H, W</span>        <span class="token comment"># 先过一个kernel_size=16，stride=16的conv得到embedding</span>        x<span class="token punctuation">,</span> T<span class="token punctuation">,</span> W <span class="token operator">=</span> self<span class="token punctuation">.</span>patch_embed<span class="token punctuation">(</span>x<span class="token punctuation">)</span>  <span class="token comment"># [16, 196, 768], 8, 14</span>        cls_tokens <span class="token operator">=</span> self<span class="token punctuation">.</span>cls_token<span class="token punctuation">.</span>expand<span class="token punctuation">(</span>x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>        x <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">(</span>cls_tokens<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment"># [16, 197, 768]</span>        x <span class="token operator">=</span> x <span class="token operator">+</span> self<span class="token punctuation">.</span>pos_embed        x <span class="token operator">=</span> self<span class="token punctuation">.</span>pos_drop<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        <span class="token comment">##  Time Embeddings</span>        <span class="token keyword">if</span> self<span class="token punctuation">.</span>attention_type <span class="token operator">!=</span> <span class="token string">"space_only"</span><span class="token punctuation">:</span>            cls_tokens <span class="token operator">=</span> x<span class="token punctuation">[</span><span class="token punctuation">:</span>B<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment"># [2, 1, 768]</span>            x <span class="token operator">=</span> x<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">]</span>            x <span class="token operator">=</span> rearrange<span class="token punctuation">(</span>x<span class="token punctuation">,</span> <span class="token string">"(b t) n m -&gt; (b n) t m"</span><span class="token punctuation">,</span> b<span class="token operator">=</span>B<span class="token punctuation">,</span> t<span class="token operator">=</span>T<span class="token punctuation">)</span>  <span class="token comment"># 2,8,196,768, # [392, 8, 768]</span>            x <span class="token operator">=</span> x <span class="token operator">+</span> self<span class="token punctuation">.</span>time_embed  <span class="token comment"># 在每一个patch上都加上同样的time embedding.</span>            x <span class="token operator">=</span> self<span class="token punctuation">.</span>time_drop<span class="token punctuation">(</span>x<span class="token punctuation">)</span>            x <span class="token operator">=</span> rearrange<span class="token punctuation">(</span>x<span class="token punctuation">,</span> <span class="token string">"(b n) t m -&gt; b (n t) m"</span><span class="token punctuation">,</span> b<span class="token operator">=</span>B<span class="token punctuation">,</span> t<span class="token operator">=</span>T<span class="token punctuation">)</span>            x <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">(</span>cls_tokens<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment"># [2, 1+8*196, 768]</span>        <span class="token comment">## Attention blocks</span>        <span class="token keyword">for</span> blk <span class="token keyword">in</span> self<span class="token punctuation">.</span>blocks<span class="token punctuation">:</span>            x <span class="token operator">=</span> blk<span class="token punctuation">(</span>x<span class="token punctuation">,</span> B<span class="token punctuation">,</span> T<span class="token punctuation">,</span> W<span class="token punctuation">)</span>  <span class="token comment"># 12层attention</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>norm<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        <span class="token keyword">return</span> x<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>forward_features<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>head<span class="token punctuation">(</span>x<span class="token punctuation">)</span>  <span class="token comment"># 映射成分类分数</span>        <span class="token keyword">return</span> x<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li>每一层blk的结构如下所示</li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Block</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token comment"># ...</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">,</span> B<span class="token punctuation">,</span> T<span class="token punctuation">,</span> W<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token comment"># x: [2, 1 + 8 * 196, 768]</span>        num_spatial_tokens <span class="token operator">=</span> <span class="token punctuation">(</span>x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">//</span> T        H <span class="token operator">=</span> num_spatial_tokens <span class="token operator">//</span> W        <span class="token keyword">if</span> self<span class="token punctuation">.</span>attention_type <span class="token keyword">in</span> <span class="token punctuation">[</span><span class="token string">"space_only"</span><span class="token punctuation">,</span> <span class="token string">"joint_space_time"</span><span class="token punctuation">]</span><span class="token punctuation">:</span>            x <span class="token operator">=</span> x <span class="token operator">+</span> self<span class="token punctuation">.</span>drop_path<span class="token punctuation">(</span>self<span class="token punctuation">.</span>attn<span class="token punctuation">(</span>self<span class="token punctuation">.</span>norm1<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>            x <span class="token operator">=</span> x <span class="token operator">+</span> self<span class="token punctuation">.</span>drop_path<span class="token punctuation">(</span>self<span class="token punctuation">.</span>mlp<span class="token punctuation">(</span>self<span class="token punctuation">.</span>norm2<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>            <span class="token keyword">return</span> x        <span class="token keyword">elif</span> self<span class="token punctuation">.</span>attention_type <span class="token operator">==</span> <span class="token string">"divided_space_time"</span><span class="token punctuation">:</span>            <span class="token comment">## Temporal</span>            <span class="token comment"># Note: temporal没有和cls token做attn，而spatial做了。</span>            xt <span class="token operator">=</span> x<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span>            xt <span class="token operator">=</span> rearrange<span class="token punctuation">(</span>xt<span class="token punctuation">,</span> <span class="token string">"b (h w t) m -&gt; (b h w) t m"</span><span class="token punctuation">,</span> b<span class="token operator">=</span>B<span class="token punctuation">,</span> h<span class="token operator">=</span>H<span class="token punctuation">,</span> w<span class="token operator">=</span>W<span class="token punctuation">,</span> t<span class="token operator">=</span>T<span class="token punctuation">)</span>  <span class="token comment"># [392, 8, 768]</span>            <span class="token comment"># 不同帧之间做attn, attn map的尺寸为: [392, 12, 8, 8]</span>            res_temporal <span class="token operator">=</span> self<span class="token punctuation">.</span>drop_path<span class="token punctuation">(</span>self<span class="token punctuation">.</span>temporal_attn<span class="token punctuation">(</span>self<span class="token punctuation">.</span>temporal_norm1<span class="token punctuation">(</span>xt<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>            res_temporal <span class="token operator">=</span> rearrange<span class="token punctuation">(</span>                res_temporal<span class="token punctuation">,</span> <span class="token string">"(b h w) t m -&gt; b (h w t) m"</span><span class="token punctuation">,</span> b<span class="token operator">=</span>B<span class="token punctuation">,</span> h<span class="token operator">=</span>H<span class="token punctuation">,</span> w<span class="token operator">=</span>W<span class="token punctuation">,</span> t<span class="token operator">=</span>T            <span class="token punctuation">)</span>            res_temporal <span class="token operator">=</span> self<span class="token punctuation">.</span>temporal_fc<span class="token punctuation">(</span>res_temporal<span class="token punctuation">)</span>            xt <span class="token operator">=</span> x<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token operator">+</span> res_temporal  <span class="token comment"># [2, 8 * 196, 768]</span>            <span class="token comment">## Spatial</span>            init_cls_token <span class="token operator">=</span> x<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment"># [2, 1, 768]</span>            cls_token <span class="token operator">=</span> init_cls_token<span class="token punctuation">.</span>repeat<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> T<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment"># [2, 8, 768]</span>            cls_token <span class="token operator">=</span> rearrange<span class="token punctuation">(</span>cls_token<span class="token punctuation">,</span> <span class="token string">"b t m -&gt; (b t) m"</span><span class="token punctuation">,</span> b<span class="token operator">=</span>B<span class="token punctuation">,</span> t<span class="token operator">=</span>T<span class="token punctuation">)</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment"># [16, 1, 768]</span>            xs <span class="token operator">=</span> xt            xs <span class="token operator">=</span> rearrange<span class="token punctuation">(</span>xs<span class="token punctuation">,</span> <span class="token string">"b (h w t) m -&gt; (b t) (h w) m"</span><span class="token punctuation">,</span> b<span class="token operator">=</span>B<span class="token punctuation">,</span> h<span class="token operator">=</span>H<span class="token punctuation">,</span> w<span class="token operator">=</span>W<span class="token punctuation">,</span> t<span class="token operator">=</span>T<span class="token punctuation">)</span>  <span class="token comment"># [16, 196, 768]</span>            xs <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">(</span>cls_token<span class="token punctuation">,</span> xs<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment"># [16, 197, 768]</span>            <span class="token comment"># 不同spatial token 之间做attn</span>            res_spatial <span class="token operator">=</span> self<span class="token punctuation">.</span>drop_path<span class="token punctuation">(</span>self<span class="token punctuation">.</span>attn<span class="token punctuation">(</span>self<span class="token punctuation">.</span>norm1<span class="token punctuation">(</span>xs<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># [16, 197, 768]</span>            <span class="token comment">### Taking care of CLS token</span>            cls_token <span class="token operator">=</span> res_spatial<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span>            cls_token <span class="token operator">=</span> rearrange<span class="token punctuation">(</span>cls_token<span class="token punctuation">,</span> <span class="token string">"(b t) m -&gt; b t m"</span><span class="token punctuation">,</span> b<span class="token operator">=</span>B<span class="token punctuation">,</span> t<span class="token operator">=</span>T<span class="token punctuation">)</span>  <span class="token comment"># [2, 8, 768]</span>            cls_token <span class="token operator">=</span> torch<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>cls_token<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token boolean">True</span><span class="token punctuation">)</span>  <span class="token comment">## averaging for every frame  # [2, 1, 768]</span>            res_spatial <span class="token operator">=</span> res_spatial<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span>  <span class="token comment"># [16, 196, 768]</span>            res_spatial <span class="token operator">=</span> rearrange<span class="token punctuation">(</span>                res_spatial<span class="token punctuation">,</span> <span class="token string">"(b t) (h w) m -&gt; b (h w t) m"</span><span class="token punctuation">,</span> b<span class="token operator">=</span>B<span class="token punctuation">,</span> h<span class="token operator">=</span>H<span class="token punctuation">,</span> w<span class="token operator">=</span>W<span class="token punctuation">,</span> t<span class="token operator">=</span>T            <span class="token punctuation">)</span>  <span class="token comment"># [2, 8*196, 768]</span>            res <span class="token operator">=</span> res_spatial            x <span class="token operator">=</span> xt  <span class="token comment"># [2, 1568, 768]</span>            <span class="token comment">## Mlp</span>            <span class="token comment"># 相当于temp和spatial的结果ensemble</span>            x <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">(</span>init_cls_token<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">+</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">(</span>cls_token<span class="token punctuation">,</span> res<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>            x <span class="token operator">=</span> x <span class="token operator">+</span> self<span class="token punctuation">.</span>drop_path<span class="token punctuation">(</span>self<span class="token punctuation">.</span>mlp<span class="token punctuation">(</span>self<span class="token punctuation">.</span>norm2<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>            <span class="token keyword">return</span> x<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="Thoughts"><a href="#Thoughts" class="headerlink" title="Thoughts"></a>Thoughts</h2><ul><li>有个spacetime attention for diffusion distillation的想法，即让student学习teacher重建的过程，不过有个问题是teacher要一次denoising多步速度可能比较慢。</li></ul>]]></content>
      
      
      <categories>
          
          <category> Video Recognition </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Transformer </tag>
            
            <tag> Video Recognition </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>DiTs论文笔记</title>
      <link href="/2024/02/20/computervision/dits/dits/"/>
      <url>/2024/02/20/computervision/dits/dits/</url>
      
        <content type="html"><![CDATA[<blockquote><p>Title: <a href="https://arxiv.org/abs/2212.09748">Scalable Diffusion Models with Transformers</a> <br>From UC Berkeley and New York University <br>ICCV 2023 (Oral Presentation)</p></blockquote><p><img src="DiT.jpg" alt="DiTs"></p><h2 id="Highlight"><a href="#Highlight" class="headerlink" title="Highlight"></a>Highlight</h2><ul><li>作者说UNet的inductive bias对diffusion模型的性能并不重要，因此可以使用transformer结构来代替。</li><li>DiTs是在结合VAE的LDM框架下设计的。另外作者发现模型的复杂度和生成样本的质量有很大关系，简单的scaling up DiT并且用一个高容量的backbone训一个LDM可以取得SOTA的生成效果。</li></ul><h2 id="Methods"><a href="#Methods" class="headerlink" title="Methods"></a>Methods</h2><p><img src="DiT_Block.jpg" alt="The Diffusion Transformer (DiT) architecture."></p><ul><li>上图展示的是一个conditional latent DiT模型。</li><li>作者在这里实验了几种不同的结合conditioning的方法并发现使用adaLN-Zero效果最好，同时增加的资源消耗也少。</li></ul><p><img src="Different_conditioning_strategies.jpg" alt="Different conditioning strategies."></p><ul><li>In-context conditioning.<ul><li>将t和c的vector embedding作为input sequence之外的两个额外的tokens，和其他的image tokens同样处理。</li></ul></li><li>Cross-attentionblock<ul><li>将t和c的embeddings concat成一个长度为2的序列，和image token sequence分开。将transformer block 修改为在multi-head self-attn之后加一个multi-head cross-attn</li></ul></li><li>Adaptive layer norm (adaLN) block.<ul><li>使用adaptive LN替换LN。不再像LN那样直接学习一个dimension-wise的scale和shift权重gamma, beta，而是根据t和c的embedding的sum直接回归出这两个参数。</li></ul></li><li>adaLN-Zero block.<ul><li>受之前工作（在有监督学习任务中，零初始化每一个block的最后一个BN的scale权重可以加速large scale training）的启发，除了回归gamma和beta参数之外，也回归一个dimension-wise scaling 参数aplha作用在每个残差连接之前，然后初始化MLP输出一个全零的alpha，相当于将整个DiT block初始化为identity function。</li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> Image Generation </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Diffusion </tag>
            
            <tag> Transformers </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>动态规划专题</title>
      <link href="/2024/02/20/python/leetcode/dong-tai-gui-hua/"/>
      <url>/2024/02/20/python/leetcode/dong-tai-gui-hua/</url>
      
        <content type="html"><![CDATA[<blockquote><p>02/20/2024 - 01</p></blockquote><h1 id="300-最长递增子序列"><a href="#300-最长递增子序列" class="headerlink" title="300. 最长递增子序列"></a>300. 最长递增子序列</h1><ul><li>给定一个列表，求出最长递增子序列的长度。</li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Solution</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">lengthOfLIS</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> nums<span class="token punctuation">:</span> List<span class="token punctuation">[</span><span class="token builtin">int</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">&gt;</span> <span class="token builtin">int</span><span class="token punctuation">:</span>        lens <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>nums<span class="token punctuation">)</span>        <span class="token comment"># 定义dp[i]为以index=i结尾的最长子序列的长度</span>        dp <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">*</span> lens        <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>lens<span class="token punctuation">)</span><span class="token punctuation">:</span>            <span class="token keyword">for</span> j <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>i<span class="token punctuation">)</span><span class="token punctuation">:</span>                <span class="token keyword">if</span> nums<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">&gt;</span> nums<span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token punctuation">:</span>                    <span class="token comment"># 状态转移方程</span>                    dp<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token builtin">max</span><span class="token punctuation">(</span>dp<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">,</span> dp<span class="token punctuation">[</span>j<span class="token punctuation">]</span> <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span>                <span class="token keyword">return</span> <span class="token builtin">max</span><span class="token punctuation">(</span>dp<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><blockquote><p>02/20/2024 - 02</p></blockquote><h1 id="152-乘积最大子数组"><a href="#152-乘积最大子数组" class="headerlink" title="152. 乘积最大子数组"></a>152. 乘积最大子数组</h1><ul><li>给定一个列表，求乘积最大的连续子数组的乘积。</li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Solution</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">maxProduct</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> nums<span class="token punctuation">:</span> List<span class="token punctuation">[</span><span class="token builtin">int</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">&gt;</span> <span class="token builtin">int</span><span class="token punctuation">:</span>        maxF<span class="token punctuation">,</span> minF<span class="token punctuation">,</span> res <span class="token operator">=</span> nums<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> nums<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> nums<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>        lens <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>nums<span class="token punctuation">)</span>        <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> lens<span class="token punctuation">)</span><span class="token punctuation">:</span>            mx <span class="token operator">=</span> maxF            mn <span class="token operator">=</span> minF            <span class="token comment"># 因为涉及到数值正负号的问题，所以需要同时记录i-1位置为结尾的子数组的乘积的最大值和最小值</span>            maxF <span class="token operator">=</span> <span class="token builtin">max</span><span class="token punctuation">(</span>mx <span class="token operator">*</span> nums<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token builtin">max</span><span class="token punctuation">(</span>nums<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">,</span> mn <span class="token operator">*</span> nums<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>            minF <span class="token operator">=</span> <span class="token builtin">min</span><span class="token punctuation">(</span>mx <span class="token operator">*</span> nums<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token builtin">min</span><span class="token punctuation">(</span>nums<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">,</span> mn <span class="token operator">*</span> nums<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>            res <span class="token operator">=</span> <span class="token builtin">max</span><span class="token punctuation">(</span>maxF<span class="token punctuation">,</span> res<span class="token punctuation">)</span>                <span class="token keyword">return</span> res<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>]]></content>
      
      
      <categories>
          
          <category> Leetcode </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Leetcode </tag>
            
            <tag> 动态规划 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Lumiere论文笔记</title>
      <link href="/2024/02/19/computervision/lumiere/lumiere/"/>
      <url>/2024/02/19/computervision/lumiere/lumiere/</url>
      
        <content type="html"><![CDATA[<blockquote><p>Title: <a href="https://arxiv.org/abs/2401.12945">Lumiere: A Space-Time Diffusion Model for Video Generation</a> <br>From Google <br>ArXiv 2024.02.05</p></blockquote><p><img src="Lumiere.jpg" alt="Lumiere 电影时光"></p><h2 id="Highlight"><a href="#Highlight" class="headerlink" title="Highlight"></a>Highlight</h2><ul><li>背景：训练一个large-scale text-to-video(T2V) Foundation model是非常challenge的，因为引入了motion这个复杂度，同时时间维度增加也带来了更大的内存和计算量的消耗，需要的训练数据量也非常大。如下图(a)所示，已有的T2V的方法需要先使用一个base model生成一些关键帧，然后使用级联时间超分模型(cascade of temporal super-resolution (TSR) models)来扩充中间帧，然后在一些没有重叠的window上使用空间超分模型(spatial super-resolution (SSR) model)来获得高分辨率的结果。</li><li>如图b所示，本文提出的Lumiere模型提出STUNet来直接一步到位生成所有的帧，然后在一些重叠的windows上使用SSR来获得分辨率更高的视频(MultiDiffusion)。<br><img src="Lumiere_Pipeline.png" alt="Lumiere Pipeline"></li><li>Lumiere可以比较好地迁移到各种视频生成任务上，比如，视频风格生成，有条件生成，Image2Video，Inpainting，Cinemagraphs（在一副静态图像上画一个框，只生成框内的视频）</li></ul><h2 id="Methods"><a href="#Methods" class="headerlink" title="Methods"></a>Methods</h2><p><img src="STUNet.png" alt="STUNet architecture"></p><ul><li>StuNet(SpaceTime UNet)是在一个预训练好的T2I U-Net结构上在video的space和time上都进行下采样和上采样。</li><li>Convolution-based blocks是一个pre-trained T2I layers紧跟着一个space-time convolution。</li><li>Attention-based blocks是在原始的UNet层的pre-trained T2I layers跟上多个时间attention层。</li><li>Multidiffusion for Spatial-Super Resolution. 在时间维度重叠的windows上应用了MultiDiffusion进行线性融合获得最终的结果。</li><li>只有新加的这些时间层是要训练的，原始的T2I权重是固定住的。</li></ul><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p><strong>1. Stylized Generation</strong></p><ul><li>通过将fine-tuned T2I weights和原始的T2I weights进行线性插值可以获得较好的风格迁移视频。</li></ul><p><img src="Style_Generation.jpg" alt="Style Generation"></p><p><strong>2. Conditional Generation</strong></p><ul><li>拓展了输入的形式来兼容多个任务，将Noisy video(TxHxWx3)，masked conditioning(TxHxWx3)，binary mask（TxHxWx1）concat成7通道的输入，然后根据任务，比如Image-to-Video，Inpainting等来调整mask condition和binary mask的输入。</li><li>有微调阶段。</li></ul><h2 id="Thoughts"><a href="#Thoughts" class="headerlink" title="Thoughts"></a>Thoughts</h2><ul><li>Video生成直接一步到位生成多帧是合理的，但是最大的问题是资源消耗大的问题，因此SpaceTime卷积和SpaceTime Attention要做得足够好才能将feature降维到一个计算量能接受的程度。</li></ul>]]></content>
      
      
      <categories>
          
          <category> Video Generation </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Diffusion </tag>
            
            <tag> Video Generation </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Sora技术报告解读</title>
      <link href="/2024/02/17/computervision/sora/sora/"/>
      <url>/2024/02/17/computervision/sora/sora/</url>
      
        <content type="html"><![CDATA[<ul><li><p>这里看的是OpenAI写的<a href="https://openai.com/research/video-generation-models-as-world-simulators">Sora技术报告</a></p></li><li><p>Sora是一个Video Generation生成器，可以生成不同时长、宽高比和分辨率的视频和图像，最多可达一分钟的高清视频。</p></li><li><p>Sora是使用不同长度的videos和images训练的text-conditional diffusion models。</p></li><li><p>Sora是在video和image的latent code的时空patches上的运行的transformer结构。</p></li><li><p>这份技术报告只包含了两部分内容, 对于模型和实现细节在这份报告中并没有包含。</p><p>  (1) 将所有类型的视觉数据转化为统一的表示形式，从而实现生成模型的大规模训练</p><p>  (2) 对 Sora 的能力和局限性进行定性评估。</p></li></ul><h2 id="Turning-visual-data-into-patches"><a href="#Turning-visual-data-into-patches" class="headerlink" title="Turning visual data into patches"></a>Turning visual data into patches</h2><ul><li>LLM成功的一部分原因来自于token的使用，可以优雅地将不同模态的text进行统一的编码，本文考虑的是在视觉数据中如何从中获益。Sora的patches类似于tokens, patches对于在不同形式的videos和images上训练生成模型是高效并拓展性高的。</li><li>总结下来，首先将视频压缩到较低维的latent space，然后将这个表征分解为时空（spacetime）patches。</li></ul><p><img src="Sora-1-patches.png" alt="Sora Patches"></p><h2 id="Video-compression-network"><a href="#Video-compression-network" class="headerlink" title="Video compression network"></a>Video compression network</h2><ul><li>训练一个网络[Auto-encoding variational bayes.]来降低视觉数据的维度。该网络将原始视频作为输入，并输出压缩了时间（服务于长视频生成）和空间的latent representation。Sora在这个压缩的latent space中训练并随后生成视频。我们还训练了一个相应的decoder模型，该模型将生成的latents映射回pixel space。</li></ul><h2 id="Spacetime-latent-patches"><a href="#Spacetime-latent-patches" class="headerlink" title="Spacetime latent patches"></a>Spacetime latent patches</h2><ul><li>给定一个压缩的输入视频，我们提取一系列spacetime patches作为transformer tokens。这个方案也适用于图像，因为图像只是单帧的视频。我们基于patch的表示使Sora能够在不同分辨率、持续时间和宽高比的视频和图像上进行训练。在推理时，我们可以通过在适当大小的网格中安排随机初始化的patches来控制生成视频的大小。</li></ul><h2 id="Scaling-transformers-for-video-generation"><a href="#Scaling-transformers-for-video-generation" class="headerlink" title="Scaling transformers for video generation"></a>Scaling transformers for video generation</h2><ul><li>Sora是一个diffusion model[Scalable diffusion models with transformers]，通过noisy patches预测原始的clean patches。Sora是一个diffusion transformer, 因为transformer的scaling特性。</li><li>Sample quality improves markedly as training compute increases.</li></ul><h2 id="Variable-durations-resolutions-aspect-ratios"><a href="#Variable-durations-resolutions-aspect-ratios" class="headerlink" title="Variable durations, resolutions, aspect ratios"></a>Variable durations, resolutions, aspect ratios</h2><ul><li>采样的灵活性 Sampling flexibility</li><li>提升构图和框架 Improved framing and composition，不会生成主体的部分出现在图像之外。</li></ul><h2 id="Language-understanding"><a href="#Language-understanding" class="headerlink" title="Language understanding"></a>Language understanding</h2><ul><li>使用了DALL·E3中的re-captioning技术。我们首先训练一个高度描述性的captioner model，然后使用它为训练集中的所有视频生成文本描述。作者发现，对高质量描述性的视频字幕进行训练可以提高文本保真度以及视频的整体质量。</li><li>与 DALL·E3 类似，我们还利用GPT将简短的用户提示转换为较长的详细captions，然后送到视频模型中，这使得Sora能够生成准确遵循用户提示的高质量视频。</li></ul><h2 id="Prompting-with-images-and-videos"><a href="#Prompting-with-images-and-videos" class="headerlink" title="Prompting with images and videos"></a>Prompting with images and videos</h2><ul><li>Sora还可以使用图像和视频作为prompts</li><li>Sora可以向前向后拓展视频</li><li>结合SDEdit，可以变换videos的风格和环境</li><li>还可以在两段不同的视频中做插入（这个估计就是对latect space做插值操作了？）</li></ul><h2 id="Image-generation-capabilities"><a href="#Image-generation-capabilities" class="headerlink" title="Image generation capabilities"></a>Image generation capabilities</h2><h2 id="Emerging-simulation-capabilities"><a href="#Emerging-simulation-capabilities" class="headerlink" title="Emerging simulation capabilities"></a>Emerging simulation capabilities</h2><ul><li><p>大规模训练后获得的突发能力，没有任何归纳偏置的情况下，完全来自于scale</p><ul><li>3D consistency. Sora can generate videos with dynamic camera motion.</li><li>Long-range coherence and object permanence.</li><li>Interacting with the world.</li></ul>  <video width="480" height="320" controls="">  <source src="simulation_4.mp4" type="video/mp4">  Sora Interaction.  </video><ul><li>Simulating digital worlds.</li></ul></li></ul><h2 id="Limitations"><a href="#Limitations" class="headerlink" title="Limitations"></a>Limitations</h2><ul><li>For example, it does not accurately model the physics of many basic interactions.</li></ul>]]></content>
      
      
      <categories>
          
          <category> Video Generation </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Diffusion </tag>
            
            <tag> Video Generation </tag>
            
            <tag> Transformer </tag>
            
            <tag> Sora </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>nanoGPT源码解析</title>
      <link href="/2024/02/09/computervision/nanogpt/nanogpt/"/>
      <url>/2024/02/09/computervision/nanogpt/nanogpt/</url>
      
        <content type="html"><![CDATA[<ul><li>这里看的是AK写的<a href="https://github.com/karpathy/nanoGPT">nanoGPT</a></li></ul><h4 id="1-wandb用法"><a href="#1-wandb用法" class="headerlink" title="1. wandb用法"></a>1. wandb用法</h4><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> wandbwandb<span class="token punctuation">.</span>init<span class="token punctuation">(</span>project<span class="token operator">=</span>wandb_project<span class="token punctuation">,</span> name<span class="token operator">=</span>wandb_run_name<span class="token punctuation">,</span> config<span class="token operator">=</span>config<span class="token punctuation">)</span>wandb<span class="token punctuation">.</span>log<span class="token punctuation">(</span><span class="token punctuation">{</span>    <span class="token string">"iter"</span><span class="token punctuation">:</span> iter_num<span class="token punctuation">,</span>    <span class="token string">"train/loss"</span><span class="token punctuation">:</span> losses<span class="token punctuation">[</span><span class="token string">'train'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>    <span class="token string">"val/loss"</span><span class="token punctuation">:</span> losses<span class="token punctuation">[</span><span class="token string">'val'</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">}</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>]]></content>
      
      
      <categories>
          
          <category> GPTs </category>
          
      </categories>
      
      
        <tags>
            
            <tag> GPTs </tag>
            
            <tag> nanoGPT </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>VQGAN源码解析</title>
      <link href="/2024/01/31/computervision/vqgan/vqgan/"/>
      <url>/2024/01/31/computervision/vqgan/vqgan/</url>
      
        <content type="html"><![CDATA[<ul><li>这里看的是一个外国小哥复现的简单版本的<a href="https://github.com/dome272/VQGAN-pytorch">VQGAN</a></li></ul><h4 id="1-tqdm的一个用法"><a href="#1-tqdm的一个用法" class="headerlink" title="1. tqdm的一个用法"></a>1. tqdm的一个用法</h4><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">with</span> tqdm<span class="token punctuation">(</span><span class="token builtin">range</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>train_dataset<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token keyword">as</span> pbar<span class="token punctuation">:</span>    <span class="token keyword">for</span> i<span class="token punctuation">,</span> imgs <span class="token keyword">in</span> <span class="token builtin">zip</span><span class="token punctuation">(</span>pbar<span class="token punctuation">,</span> train_dataset<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token comment"># do something</span>        pbar<span class="token punctuation">.</span>set_postfix<span class="token punctuation">(</span>            VQ_Loss<span class="token operator">=</span>np<span class="token punctuation">.</span><span class="token builtin">round</span><span class="token punctuation">(</span>vq_loss<span class="token punctuation">.</span>cpu<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            GAN_Loss<span class="token operator">=</span>np<span class="token punctuation">.</span><span class="token builtin">round</span><span class="token punctuation">(</span>gan_loss<span class="token punctuation">.</span>cpu<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span>        <span class="token punctuation">)</span>        pbar<span class="token punctuation">.</span>update<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="2-使用torchinfo可视化网络结构"><a href="#2-使用torchinfo可视化网络结构" class="headerlink" title="2. 使用torchinfo可视化网络结构"></a>2. 使用torchinfo可视化网络结构</h4><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> torchinfo <span class="token keyword">import</span> summaryencoder <span class="token operator">=</span> Encoder<span class="token punctuation">(</span><span class="token punctuation">)</span>summary<span class="token punctuation">(</span>encoder<span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">256</span><span class="token punctuation">,</span> <span class="token number">256</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><h4 id="3-VQGAN的Encoder部分"><a href="#3-VQGAN的Encoder部分" class="headerlink" title="3. VQGAN的Encoder部分"></a>3. VQGAN的Encoder部分</h4><ul><li><p>模型结构为：<br><img src="VQGAN-Encoder.png" alt="VQGAN Encoder结构"></p></li><li><p>其中，</p><ul><li>ResidualBlock是由两个GroupNorm+Swish+Conv组成;</li><li>DownSampleBlock是一个stride=2的Conv;</li><li>NonLocalBlock是一个Attention Block;</li></ul></li><li><p>Encode之后会再接一个1x1的conv，称作是quant_conv. 然后过codebook，对应的还有个1x1的post_quant_conv。</p></li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># 简易Attention实现</span>attn <span class="token operator">=</span> torch<span class="token punctuation">.</span>bmm<span class="token punctuation">(</span>q<span class="token punctuation">,</span> k<span class="token punctuation">)</span>attn <span class="token operator">=</span> attn <span class="token operator">*</span> <span class="token punctuation">(</span><span class="token builtin">int</span><span class="token punctuation">(</span>c<span class="token punctuation">)</span> <span class="token operator">**</span> <span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">0.5</span><span class="token punctuation">)</span><span class="token punctuation">)</span>attn <span class="token operator">=</span> F<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>attn<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>attn <span class="token operator">=</span> attn<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>A <span class="token operator">=</span> torch<span class="token punctuation">.</span>bmm<span class="token punctuation">(</span>v<span class="token punctuation">,</span> attn<span class="token punctuation">)</span>A <span class="token operator">=</span> A<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>b<span class="token punctuation">,</span> c<span class="token punctuation">,</span> h<span class="token punctuation">,</span> w<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="4-VQGAN的Codebook部分"><a href="#4-VQGAN的Codebook部分" class="headerlink" title="4. VQGAN的Codebook部分"></a>4. VQGAN的Codebook部分</h4><ul><li>为什么要使用Codebook对Encoder得到的feature做离散化编码？<ul><li>使得模型学习到更抽象和压缩的数据表示</li><li>强制信息瓶颈帮助提升生成模型的泛化能力</li><li>提高计算效率</li></ul></li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn<span class="token keyword">class</span> <span class="token class-name">Codebook</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> args<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token builtin">super</span><span class="token punctuation">(</span>Codebook<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>num_codebook_vectors <span class="token operator">=</span> args<span class="token punctuation">.</span>num_codebook_vectors  <span class="token comment"># 1024</span>        self<span class="token punctuation">.</span>latent_dim <span class="token operator">=</span> args<span class="token punctuation">.</span>latent_dim  <span class="token comment"># 256</span>        self<span class="token punctuation">.</span>beta <span class="token operator">=</span> args<span class="token punctuation">.</span>beta  <span class="token comment"># 0.25</span>        self<span class="token punctuation">.</span>embedding <span class="token operator">=</span> nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>self<span class="token punctuation">.</span>num_codebook_vectors<span class="token punctuation">,</span> self<span class="token punctuation">.</span>latent_dim<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>embedding<span class="token punctuation">.</span>weight<span class="token punctuation">.</span>data<span class="token punctuation">.</span>uniform_<span class="token punctuation">(</span>            <span class="token operator">-</span><span class="token number">1.0</span> <span class="token operator">/</span> self<span class="token punctuation">.</span>num_codebook_vectors<span class="token punctuation">,</span> <span class="token number">1.0</span> <span class="token operator">/</span> self<span class="token punctuation">.</span>num_codebook_vectors        <span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> z<span class="token punctuation">)</span><span class="token punctuation">:</span>        z <span class="token operator">=</span> z<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>contiguous<span class="token punctuation">(</span><span class="token punctuation">)</span>        z_flattened <span class="token operator">=</span> z<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>latent_dim<span class="token punctuation">)</span>  <span class="token comment"># [1*16*16, 256]</span>        d <span class="token operator">=</span> <span class="token punctuation">(</span>            torch<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>z_flattened<span class="token operator">**</span><span class="token number">2</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> keepdim<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>            <span class="token operator">+</span> torch<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>embedding<span class="token punctuation">.</span>weight<span class="token operator">**</span><span class="token number">2</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>            <span class="token operator">-</span> <span class="token number">2</span> <span class="token operator">*</span> <span class="token punctuation">(</span>torch<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>z_flattened<span class="token punctuation">,</span> self<span class="token punctuation">.</span>embedding<span class="token punctuation">.</span>weight<span class="token punctuation">.</span>t<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token punctuation">)</span>  <span class="token comment"># [256, 1024]</span>        min_encoding_indices <span class="token operator">=</span> torch<span class="token punctuation">.</span>argmin<span class="token punctuation">(</span>d<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment"># [256]</span>        z_q <span class="token operator">=</span> self<span class="token punctuation">.</span>embedding<span class="token punctuation">(</span>min_encoding_indices<span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span>z<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>        loss <span class="token operator">=</span> torch<span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token punctuation">(</span>z_q<span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">-</span> z<span class="token punctuation">)</span> <span class="token operator">**</span> <span class="token number">2</span><span class="token punctuation">)</span> <span class="token operator">+</span> self<span class="token punctuation">.</span>beta <span class="token operator">*</span> torch<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>            <span class="token punctuation">(</span>z_q <span class="token operator">-</span> z<span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">**</span> <span class="token number">2</span>        <span class="token punctuation">)</span>        <span class="token comment"># simply copy the gradient from the decoder to the encoder. </span>        <span class="token comment"># 这里实现非常巧妙，也很容易出错。</span>        z_q <span class="token operator">=</span> z <span class="token operator">+</span> <span class="token punctuation">(</span>z_q <span class="token operator">-</span> z<span class="token punctuation">)</span><span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># preserve the gradients for the backward flow.</span>        z_q <span class="token operator">=</span> z_q<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>        <span class="token keyword">return</span> z_q<span class="token punctuation">,</span> min_encoding_indices<span class="token punctuation">,</span> loss<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="5-VQGAN的Decoder部分"><a href="#5-VQGAN的Decoder部分" class="headerlink" title="5. VQGAN的Decoder部分"></a>5. VQGAN的Decoder部分</h4><ul><li>模型结构为：<br><img src="VQGAN-Decoder.png" alt="VQGAN Decoder结构"></li></ul><h4 id="6-VQGAN的Discriminator部分"><a href="#6-VQGAN的Discriminator部分" class="headerlink" title="6. VQGAN的Discriminator部分"></a>6. VQGAN的Discriminator部分</h4><ul><li><p>只有在超过一定的steps(10000)之后才开始加入Disc的训练</p></li><li><p>模型结构为：<br><img src="VQGAN-Discriminator.png" alt="VQGAN Discriminator结构"></p></li></ul><h4 id="7-VQGAN的其他loss部分"><a href="#7-VQGAN的其他loss部分" class="headerlink" title="7. VQGAN的其他loss部分"></a>7. VQGAN的其他loss部分</h4><ul><li>perceptual_loss: 加载了一个pretrained VGG16模型，计算的real_img和gen_img之间的多层feature距离。</li><li>重建loss</li><li>gan_loss:</li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python">perceptual_rec_loss <span class="token operator">=</span> <span class="token punctuation">(</span>    args<span class="token punctuation">.</span>perceptual_loss_factor <span class="token operator">*</span> perceptual_loss    <span class="token operator">+</span> args<span class="token punctuation">.</span>rec_loss_factor <span class="token operator">*</span> rec_loss<span class="token punctuation">)</span>perceptual_rec_loss <span class="token operator">=</span> perceptual_rec_loss<span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token punctuation">)</span>g_loss <span class="token operator">=</span> <span class="token operator">-</span>torch<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>disc_fake<span class="token punctuation">)</span>  <span class="token comment"># 趋于0，或者大于0</span>λ <span class="token operator">=</span> self<span class="token punctuation">.</span>vqgan<span class="token punctuation">.</span>calculate_lambda<span class="token punctuation">(</span>perceptual_rec_loss<span class="token punctuation">,</span> g_loss<span class="token punctuation">)</span>vq_loss <span class="token operator">=</span> perceptual_rec_loss <span class="token operator">+</span> q_loss <span class="token operator">+</span> disc_factor <span class="token operator">*</span> λ <span class="token operator">*</span> g_lossd_loss_real <span class="token operator">=</span> torch<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span><span class="token number">1.0</span> <span class="token operator">-</span> disc_real<span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># 大于1</span>d_loss_fake <span class="token operator">=</span> torch<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span><span class="token number">1.0</span> <span class="token operator">+</span> disc_fake<span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># 小于-1</span>gan_loss <span class="token operator">=</span> disc_factor <span class="token operator">*</span> <span class="token number">0.5</span> <span class="token operator">*</span> <span class="token punctuation">(</span>d_loss_real <span class="token operator">+</span> d_loss_fake<span class="token punctuation">)</span>self<span class="token punctuation">.</span>opt_vq<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>vq_loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span>retain_graph<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>self<span class="token punctuation">.</span>opt_disc<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>gan_loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>self<span class="token punctuation">.</span>opt_vq<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>self<span class="token punctuation">.</span>opt_disc<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="8-VQGAN的loss的calculate-lambda部分"><a href="#8-VQGAN的loss的calculate-lambda部分" class="headerlink" title="8. VQGAN的loss的calculate_lambda部分"></a>8. VQGAN的loss的calculate_lambda部分</h4><ul><li>这个函数的作用: 动态调整模型训练中不同部分的损失贡献相关。<ul><li>根据最后一层的梯度情况来平衡感知损失和GAN损失的影响，有助于控制训练过程中的损失平衡。</li></ul></li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">calculate_lambda</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> perceptual_loss<span class="token punctuation">,</span> gan_loss<span class="token punctuation">)</span><span class="token punctuation">:</span>    last_layer <span class="token operator">=</span> self<span class="token punctuation">.</span>decoder<span class="token punctuation">.</span>model<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span>    last_layer_weight <span class="token operator">=</span> last_layer<span class="token punctuation">.</span>weight    perceptual_loss_grads <span class="token operator">=</span> torch<span class="token punctuation">.</span>autograd<span class="token punctuation">.</span>grad<span class="token punctuation">(</span>        perceptual_loss<span class="token punctuation">,</span> last_layer_weight<span class="token punctuation">,</span> retain_graph<span class="token operator">=</span><span class="token boolean">True</span>    <span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>    gan_loss_grads <span class="token operator">=</span> torch<span class="token punctuation">.</span>autograd<span class="token punctuation">.</span>grad<span class="token punctuation">(</span>        gan_loss<span class="token punctuation">,</span> last_layer_weight<span class="token punctuation">,</span> retain_graph<span class="token operator">=</span><span class="token boolean">True</span>    <span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>    λ <span class="token operator">=</span> torch<span class="token punctuation">.</span>norm<span class="token punctuation">(</span>perceptual_loss_grads<span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token punctuation">(</span>torch<span class="token punctuation">.</span>norm<span class="token punctuation">(</span>gan_loss_grads<span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token number">1e-4</span><span class="token punctuation">)</span>    λ <span class="token operator">=</span> torch<span class="token punctuation">.</span>clamp<span class="token punctuation">(</span>λ<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1e4</span><span class="token punctuation">)</span><span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token keyword">return</span> <span class="token number">0.8</span> <span class="token operator">*</span> λ<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><p>到这里，我们训练VQGAN的第一步已经结束了，我们可以根据一组VQ来重建出一张图，那么接下就需要transformer来负责先预测出一组VQ然后重建出图来啦～</p><hr><h4 id="8-VQGAN的Transformer部分"><a href="#8-VQGAN的Transformer部分" class="headerlink" title="8. VQGAN的Transformer部分"></a>8. VQGAN的Transformer部分</h4><ul><li>AdamW优化器区分了bias, nn.LayerNorm, nn.Embedding以及pos_emb训练时都不加weight decay，而nn.Linear加了weight decay。</li><li>Transformer部分的网络结构主要是一个<a href="https://github.com/karpathy/minGPT/">minGPT</a>。</li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> idx<span class="token punctuation">,</span> embeddings<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token comment"># idx是输入图像通过vqgan的Encoder部分得到的indices经过随机mask掉一部分得到的indices.</span>    token_embeddings <span class="token operator">=</span> self<span class="token punctuation">.</span>tok_emb<span class="token punctuation">(</span>idx<span class="token punctuation">)</span>  <span class="token comment"># each index maps to a (learnable) vector</span>    t <span class="token operator">=</span> token_embeddings<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>    position_embeddings <span class="token operator">=</span> self<span class="token punctuation">.</span>pos_emb<span class="token punctuation">[</span>        <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span>t<span class="token punctuation">,</span> <span class="token punctuation">:</span>    <span class="token punctuation">]</span>  <span class="token comment"># each position maps to a (learnable) vector</span>    x <span class="token operator">=</span> self<span class="token punctuation">.</span>drop<span class="token punctuation">(</span>token_embeddings <span class="token operator">+</span> position_embeddings<span class="token punctuation">)</span>    x <span class="token operator">=</span> self<span class="token punctuation">.</span>blocks<span class="token punctuation">(</span>x<span class="token punctuation">)</span>  <span class="token comment"># A vanilla multi-head masked self-attention layer</span>    x <span class="token operator">=</span> self<span class="token punctuation">.</span>ln_f<span class="token punctuation">(</span>x<span class="token punctuation">)</span>    logits <span class="token operator">=</span> self<span class="token punctuation">.</span>head<span class="token punctuation">(</span>x<span class="token punctuation">)</span>  <span class="token comment"># [1, 1024]</span>    <span class="token keyword">return</span> logits<span class="token punctuation">,</span> <span class="token boolean">None</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li><p>这里训GPTTransformer的时候，</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>    _<span class="token punctuation">,</span> indices <span class="token operator">=</span> self<span class="token punctuation">.</span>encode_to_z<span class="token punctuation">(</span>x<span class="token punctuation">)</span>  <span class="token comment"># 输入图像经过vqgan的Encoder部分得到的indices</span>    sos_tokens <span class="token operator">=</span> torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span>x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">*</span> self<span class="token punctuation">.</span>sos_token    sos_tokens <span class="token operator">=</span> sos_tokens<span class="token punctuation">.</span><span class="token builtin">long</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span><span class="token string">"cuda"</span><span class="token punctuation">)</span>  <span class="token comment"># 起始token的index，默认是0</span>    mask <span class="token operator">=</span> torch<span class="token punctuation">.</span>bernoulli<span class="token punctuation">(</span>        self<span class="token punctuation">.</span>pkeep <span class="token operator">*</span> torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span>indices<span class="token punctuation">.</span>shape<span class="token punctuation">,</span> device<span class="token operator">=</span>indices<span class="token punctuation">.</span>device<span class="token punctuation">)</span>    <span class="token punctuation">)</span>    mask <span class="token operator">=</span> mask<span class="token punctuation">.</span><span class="token builtin">round</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>int64<span class="token punctuation">)</span>    random_indices <span class="token operator">=</span> torch<span class="token punctuation">.</span>randint_like<span class="token punctuation">(</span>indices<span class="token punctuation">,</span> self<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>config<span class="token punctuation">.</span>vocab_size<span class="token punctuation">)</span>    <span class="token comment"># 随机mask掉一半的indices，用随机的indices代替</span>    new_indices <span class="token operator">=</span> mask <span class="token operator">*</span> indices <span class="token operator">+</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">-</span> mask<span class="token punctuation">)</span> <span class="token operator">*</span> random_indices      new_indices <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">(</span>sos_tokens<span class="token punctuation">,</span> new_indices<span class="token punctuation">)</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>    target <span class="token operator">=</span> indices    logits<span class="token punctuation">,</span> _ <span class="token operator">=</span> self<span class="token punctuation">.</span>transformer<span class="token punctuation">(</span>new_indices<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>    <span class="token keyword">return</span> logits<span class="token punctuation">,</span> target  <span class="token comment"># 直接用cross entropy训练</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li><li><p>这样训完之后，GPT Transformer就可以根据前序的token来预测后续的token了</p></li></ul><h4 id="9-训好了怎么sample生成样本呢？"><a href="#9-训好了怎么sample生成样本呢？" class="headerlink" title="9. 训好了怎么sample生成样本呢？"></a>9. 训好了怎么sample生成样本呢？</h4><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token decorator annotation punctuation">@torch<span class="token punctuation">.</span>no_grad</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">def</span> <span class="token function">sample</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">,</span> c<span class="token punctuation">,</span> steps<span class="token punctuation">,</span> temperature<span class="token operator">=</span><span class="token number">1.0</span><span class="token punctuation">,</span> top_k<span class="token operator">=</span><span class="token number">100</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    self<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span><span class="token builtin">eval</span><span class="token punctuation">(</span><span class="token punctuation">)</span>    x <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">(</span>c<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>    <span class="token keyword">for</span> k <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>steps<span class="token punctuation">)</span><span class="token punctuation">:</span>        logits<span class="token punctuation">,</span> _ <span class="token operator">=</span> self<span class="token punctuation">.</span>transformer<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        logits <span class="token operator">=</span> logits<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token operator">/</span> temperature        <span class="token keyword">if</span> top_k <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>            logits <span class="token operator">=</span> self<span class="token punctuation">.</span>top_k_logits<span class="token punctuation">(</span>logits<span class="token punctuation">,</span> top_k<span class="token punctuation">)</span>        probs <span class="token operator">=</span> F<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>logits<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>        ix <span class="token operator">=</span> torch<span class="token punctuation">.</span>multinomial<span class="token punctuation">(</span>probs<span class="token punctuation">,</span> num_samples<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment"># 按照概率随机取一个值</span>        x <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span> ix<span class="token punctuation">)</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>    x <span class="token operator">=</span> x<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> c<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">:</span><span class="token punctuation">]</span>    self<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token keyword">return</span> x<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li>获得一个序列的indices送入到VQGAN的Decoder就可以生成图像了。</li></ul>]]></content>
      
      
      <categories>
          
          <category> Computer Vision </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Computer Vision </tag>
            
            <tag> Image Generation </tag>
            
            <tag> GAN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Python字符串神奇用法</title>
      <link href="/2024/01/23/python/zi-fu-chuan/"/>
      <url>/2024/01/23/python/zi-fu-chuan/</url>
      
        <content type="html"><![CDATA[<h4 id="1-将一个字符串中的前两个’-’替换为’-’"><a href="#1-将一个字符串中的前两个’-’替换为’-’" class="headerlink" title="1. 将一个字符串中的前两个’_’替换为’:’"></a>1. 将一个字符串中的前两个’_’替换为’:’</h4><pre class="line-numbers language-python" data-language="python"><code class="language-python">s <span class="token operator">=</span> <span class="token string">"a_b_c_d_e"</span>s_replaced <span class="token operator">=</span> s<span class="token punctuation">.</span>replace<span class="token punctuation">(</span><span class="token string">"_"</span><span class="token punctuation">,</span> <span class="token string">":"</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>  <span class="token comment"># 指定最大替换次数为2</span><span class="token keyword">print</span><span class="token punctuation">(</span>s_replaced<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><h4 id="2-argparse命令行参数中的-会自动转换为"><a href="#2-argparse命令行参数中的-会自动转换为" class="headerlink" title="2. argparse命令行参数中的-会自动转换为_"></a>2. argparse命令行参数中的-会自动转换为_</h4><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># 在使用Python的argparse库时，如果你定义了一个带有连字符（-）的命令行参数，argparse模块会自动将这些连字符转换为下划线（_）。这样做是因为在Python中，变量名不能包含连字符；它们可以包含下划线。</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h4 id="3-python的-staticmethod经常用在什么情况下？"><a href="#3-python的-staticmethod经常用在什么情况下？" class="headerlink" title="3. python的@staticmethod经常用在什么情况下？"></a>3. python的@staticmethod经常用在什么情况下？</h4><ul><li>staticmethod用于修饰类中的方法,使其可以在不创建类实例的情况下调用方法</li><li>函数逻辑与类有关联但不需要类或实例的任何信息：当你需要定义一些功能，这些功能虽然跟类相关，但执行时它们不需要类的任何信息（即不需要访问任何类变量或实例变量）。</li><li>组织工具函数：如果有一些与类操作相关的工具函数，你可能希望将它们组织在一个类里面，以保持代码的组织和清晰。</li><li>替代命名空间：当你想要使用类作为一个命名空间来避免函数名冲突时，可以定义静态方法。这样，你可以将相关的函数放在一个类下面，但这些函数并不需要访问类或实例的状态。</li><li>继承管理：在子类中，你可能想要重用某个静态方法而不是实例方法。因为静态方法不与特定的实例或类状态关联，它们更容易在继承中被复用。</li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">MathUtils</span><span class="token punctuation">:</span>    <span class="token decorator annotation punctuation">@staticmethod</span>    <span class="token keyword">def</span> <span class="token function">add</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span> y<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> x <span class="token operator">+</span> y    <span class="token comment"># 静态方法可以通过类名直接调用，无需创建类的实例</span>result <span class="token operator">=</span> MathUtils<span class="token punctuation">.</span>add<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>result<span class="token punctuation">)</span>  <span class="token comment"># 输出: 12</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="4-python的-运算符"><a href="#4-python的-运算符" class="headerlink" title="4. python的**运算符"></a>4. python的**运算符</h4><ul><li>可以使用**来将字典中的项作为关键字参数传递给函数。<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">greet</span><span class="token punctuation">(</span>first_name<span class="token punctuation">,</span> last_name<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"Hello </span><span class="token interpolation"><span class="token punctuation">{</span>first_name<span class="token punctuation">}</span></span><span class="token string"> </span><span class="token interpolation"><span class="token punctuation">{</span>last_name<span class="token punctuation">}</span></span><span class="token string">!"</span></span><span class="token punctuation">)</span>person <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token string">'first_name'</span><span class="token punctuation">:</span> <span class="token string">'John'</span><span class="token punctuation">,</span> <span class="token string">'last_name'</span><span class="token punctuation">:</span> <span class="token string">'Doe'</span><span class="token punctuation">}</span>greet<span class="token punctuation">(</span><span class="token operator">**</span>person<span class="token punctuation">)</span>  <span class="token comment"># 等同于 greet(first_name='John', last_name='Doe')</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li></ul><h4 id="5-python自带的globals函数"><a href="#5-python自带的globals函数" class="headerlink" title="5. python自带的globals函数"></a>5. python自带的globals函数</h4><ul><li>全局符号表，其中包含有关程序的所有信息，包含变量名，方法，类名等等。<pre class="line-numbers language-python" data-language="python"><code class="language-python">a <span class="token operator">=</span> <span class="token number">5</span><span class="token keyword">def</span> <span class="token function">func</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    c <span class="token operator">=</span> <span class="token number">10</span>    d <span class="token operator">=</span> c <span class="token operator">+</span> a     <span class="token builtin">globals</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token string">'a'</span><span class="token punctuation">]</span> <span class="token operator">=</span> d     <span class="token keyword">print</span><span class="token punctuation">(</span>d<span class="token punctuation">)</span>  func<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li></ul>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
            <tag> 字符串 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>欢迎</title>
      <link href="/2024/01/21/hello-world/"/>
      <url>/2024/01/21/hello-world/</url>
      
        <content type="html"><![CDATA[<p>欢迎来到一只猪的圈。</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="持续学习"><a href="#持续学习" class="headerlink" title="持续学习!"></a>持续学习!</h3><h3 id="学习是这个世界上最简单的事情"><a href="#学习是这个世界上最简单的事情" class="headerlink" title="学习是这个世界上最简单的事情!"></a>学习是这个世界上最简单的事情!</h3>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
