<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Consistency Models论文笔记</title>
      <link href="/2024/03/12/computervision/consistency-models/consistency-models/"/>
      <url>/2024/03/12/computervision/consistency-models/consistency-models/</url>
      
        <content type="html"><![CDATA[<blockquote><p>Title: <a href="https://arxiv.org/abs/2303.01469">Consistency Models</a> <br>From OpenAI, Yang Song. <br>ICML 2023.</p></blockquote><p><img src="Consistency_Models.jpg" alt="图1.Consistency Models."></p><h2 id="Highlight"><a href="#Highlight" class="headerlink" title="Highlight"></a>Highlight</h2><ul><li>所谓的Consistency Models一致性模型，是约束在同一条PF ODE轨迹上的任意点都对应相同的输出。满足这样的性质的生成模型则可获得不错的一步生成效果。</li></ul><h2 id="Background-for-Score-based-Models"><a href="#Background-for-Score-based-Models" class="headerlink" title="Background for Score-based Models"></a>Background for Score-based Models</h2><ul><li>先来介绍一下关于score based model的一些基础知识。</li></ul><p><img src="SDE.jpg" alt="图2.SDE."></p><ul><li>如图2所示，Diffusion model对原始的数据的扩散过程（加噪过程）是一个随机过程，我们可以用一个随机微分方程SDE来建模它。</li><li>这个SDE有个非常好的性质是存在一个常微分方程ODE，作者叫Probability Flow(PF) ODE，这个ODE的解轨迹在t采样时刻的分布和p_t(x)的score function有关。这个也叫做逆随机过程 Reverse SDE，也就是由噪声到生成图像的过程。如图3所示，</li></ul><p><img src="Reverse_SDE.jpg" alt="图3.Reverse SDE."></p><ul><li><p>把一个SDE转换为ODE的表示的好处是什么呢？首先ODE的轨迹定义了data和noise之间的一对一的映射关系。然后我们就可以用一些ODE的求解器，比如DDIM，DPM-Solver等等去求解这个随机过程。这个发现非常重要，因为它意味着我们可以使用ODE求解器来连续地模拟逆向过程，而不是使用基于MCMC的迭代方法。这种方法可以显著提高采样效率，并且通常能够生成更高质量的样本。【我们熟知的DDIM对应了diffusion ODE的1阶ODE solver，它的加速效果好是因为它考虑了ODE的半线性结构，而DPM-Solver给出了对应的更高阶的solver，可以让10步左右的采样达到与DDPM的1000步的采样相当。】</p></li><li><p>和现有的Denoisers不一样的是，现有的Denoisers往往预测的是后验概率，所以对加噪图像进行去噪的话，更倾向于得到的是一个对不同噪声去噪结果的平均，所以图像会更加blur，而一致性模型预测的是ODE轨迹上的唯一的对应的image，所以其生成效果会sharp一些。</p></li></ul><h2 id="Methods"><a href="#Methods" class="headerlink" title="Methods"></a>Methods</h2><p><img src="Condition.jpg" alt="图4.Consistency model properties."></p><ul><li>如图4所示，一个生成模型f_{\theta}要想具有上面所说的一步生成的能力，那它得满足两条基本的性质。<ul><li>边界条件。当t非常小的时候，输入原图x_0得到的是x_0。这里用一个非常小的t而不是0是为了避免0带来的数值不稳定。这个条件可以通过网络参数化来约束，如图5所示，用skip connection改变一下UNet可以满足。</li><li>Self-Consistency条件。而一致性要求两个不同时刻的采样点的预测结果一致。至于我们怎么找到在同一条ODE解轨迹上的两个点下面再讨论。</li></ul></li></ul><p><img src="boundary_conditions.jpg" alt="图5.Enforce the boundary conditions."></p><ul><li>我们通过一致性模型一步或者多步采样如图6和6-2所示，</li></ul><p><img src="sampling.jpg" alt="图6.Sampling."></p><p><img src="Sampling_Algorithm.jpg" alt="图6-2.Sampling Algorithm."></p><ul><li>到这里，我们要说我们该怎么找到一条ODE轨迹上的两个点来训练一致性模型了。有两种训练方式，一种是通过蒸馏的方式，即我已经有一个训好的diffusion模型了，那我必然知道它每一条ODE解轨迹。另一种是模型通过近似的方式找到两个点自己学。</li></ul><ol><li>通过Distillation训练</li></ol><ul><li>如图7所示，先随机采样一个timestep T_{n+}，然后运行一步ODE到t_{n}，得到的两个点送入模型来使用一致性loss约束。这里的EMA作者在Improved CM论文中说不带EMA效果更好～</li></ul><p><img src="distillation.jpg" alt="图7.Training CM via distillation."></p><ol start="2"><li>通过单独训练</li></ol><ul><li>如图8所示，作者在论文中也证明了当t_{n+1}和t_{n}间隔足够小的话，目标函数等价于distillation。</li></ul><p><img src="isolation.jpg" alt="图8.Training CM via isolation."></p><ul><li>训练一致性模型的算法流程图如图9所示。<br><img src="Training_Algorithm.jpg" alt="图9.Training Algorithm."></li></ul><h2 id="Thoughts"><a href="#Thoughts" class="headerlink" title="Thoughts"></a>Thoughts</h2><ul><li>这种一致性约束的思想加在常规的Progressize Diffusion Distillation里不知道效果咋样。</li></ul><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li><a href="https://neurips.cc/virtual/2023/75013">Yang Song‘s presentation about Consistency Models at NeuraIPS 2023.</a></li><li><a href="https://github.com/openai/consistency_models">https://github.com/openai/consistency_models</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> Image Generation </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Diffusion </tag>
            
            <tag> Score-based model </tag>
            
            <tag> Consistency Models </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>DeepCache论文笔记</title>
      <link href="/2024/02/29/computervision/deepcache/deepcache/"/>
      <url>/2024/02/29/computervision/deepcache/deepcache/</url>
      
        <content type="html"><![CDATA[<blockquote><p>Title: <a href="https://arxiv.org/abs/2312.00858">DeepCache: Accelerating Diffusion Models for Free</a> <br>From NUS <br>ArXiv 2023.12.07</p></blockquote><p><img src="Feature_similarity.jpg" alt="Examples of Feature Maps and Similarity."></p><h2 id="Highlight"><a href="#Highlight" class="headerlink" title="Highlight"></a>Highlight</h2><ul><li>作者发现在diffusion生成过程中，相邻步的UNet的high-level特征存在非常高的相似性。</li><li>因此可以把上一步的UNet深层特征存下来，当前步只需要计算浅层特征并和上一步缓存的深层特征Concat。</li></ul><h2 id="Methods"><a href="#Methods" class="headerlink" title="Methods"></a>Methods</h2><p><img src="DeepCache.jpg" alt="Illustration of DeepCache."></p><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p><img src="Results.jpg" alt="Class-conditional generation quality on ImageNet."></p><h2 id="Thoughts"><a href="#Thoughts" class="headerlink" title="Thoughts"></a>Thoughts</h2><ul><li>对于Progressive Diffusion Distillation来说的话，两步蒸馏一步，那说明实际起作用的是在蒸馏浅层feature？但是随着误差累积，深层feature的重要性才逐渐显现出来。</li></ul>]]></content>
      
      
      <categories>
          
          <category> Image Generation </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Diffusion </tag>
            
            <tag> UNet </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Preserving Fairness Generalization in Deepfake Detection 论文笔记</title>
      <link href="/2024/02/28/computervision/preserving-fairness-generalization-in-deepfake-detection/preserving-fairness-generalization-in-deepfake-detection/"/>
      <url>/2024/02/28/computervision/preserving-fairness-generalization-in-deepfake-detection/preserving-fairness-generalization-in-deepfake-detection/</url>
      
        <content type="html"><![CDATA[<blockquote><p><strong>Title:</strong> <a href="https://arxiv.org/abs/2402.17229">《Preserving Fairness Generalization in Deepfake Detection》</a> <br>From Purdue University <br>CVPR 2024 paper</p></blockquote><h2 id="Highlight"><a href="#Highlight" class="headerlink" title="Highlight"></a>Highlight</h2><ul><li>文中提出同时考虑features，loss，和optimization三方面来解决deepfake detection的问题。</li><li>使用解偶学习来提取人种和domain无关的防伪特征，并在一个平坦的loss平面上通过fuse他们来促进fair learning。（平坦的loss平面是使用了SAM优化器。文中提到的fair learning其实是cross domain的generalization的问题，换了种说法而已。）</li></ul><p><img src="Comparison.jpg" alt="Comparison with existing deepfake detection baselines."></p><h2 id="Methods"><a href="#Methods" class="headerlink" title="Methods"></a>Methods</h2><p><img src="overview.jpg" alt="An overview of our proposed method."></p><ul><li>A代表的是Domain Label，例如real, DeepFake，Face2Face等不同伪造的数据集。</li><li>Y代表的是real / fake</li><li>D代表的是人口统计变量，比如根据性别可以分为{male, female}两个subgroups。</li></ul><h3 id="人口统计特征-防伪特征"><a href="#人口统计特征-防伪特征" class="headerlink" title="人口统计特征 &amp; 防伪特征"></a>人口统计特征 &amp; 防伪特征</h3><ul><li>输入的是fake + real的一对儿images；</li><li>包含三个独立的encoders用来提取content特征 c(和图像背景相关？)， 防伪特征 f，人口统计特征 d。</li><li>防伪特征f里同时包含和domain相关f_a的以及和domain无关的f_g，</li></ul><h3 id="Classification-Loss"><a href="#Classification-Loss" class="headerlink" title="Classification Loss"></a>Classification Loss</h3><ul><li>deepfake数据集有人口统计subgroup分布的不均衡问题，所以使用均衡的ditribution-aware margin loss来分类D。</li><li>同时，L_cls还包含区分Y和A的两个CE loss。</li></ul><h3 id="Contrastive-Loss"><a href="#Contrastive-Loss" class="headerlink" title="Contrastive Loss"></a>Contrastive Loss</h3><ul><li>用的是一个triplet loss L_con</li><li>L_con用在f_a和f_g上。</li></ul><h3 id="Reconstruction-Loss"><a href="#Reconstruction-Loss" class="headerlink" title="Reconstruction Loss"></a>Reconstruction Loss</h3><ul><li>L_rec = ||X_i - D(c_i, f_i, d_i)|| + ||X_i - D(c_i, f_i’, d_i)||</li><li>根据输入图像的latent feature来重建，后者是根据partner的forgery特征来进行重建。</li></ul><p><img src="resc.jpg" alt="The architecture details of the decoder."></p><h3 id="Fair-Learning-under-Generalization"><a href="#Fair-Learning-under-Generalization" class="headerlink" title="Fair Learning under Generalization"></a>Fair Learning under Generalization</h3><ul><li>获得了domain无关的防伪特征和人口统计特征，将他们使用AdaIN结合起来, 融合后的特征I_i为：</li></ul><p>I_i = \delta(d_i) * ((f_g - \mu(f_g)) / \delta(f_g)) + \mu(d_i)</p><ul><li>\mu和\delta为在feature的spatial维度上计算的均值和标准差，即将f_g的style转换为d的。</li></ul><h3 id="Fairness-Learning"><a href="#Fairness-Learning" class="headerlink" title="Fairness Learning"></a>Fairness Learning</h3><ul><li>作者提出了bi-level fairness loss在不同的subgroups之间提高公平性。</li><li>文章说的有点复杂不好理解，简单理解就是把domain无关的防伪特征便成对应不同人口统计特征对应的domain下的特征，然后去分类。</li></ul><p><img src="fair_loss.jpg" alt="bi-level fairness loss."><br><img src="algo.jpg" alt="End-to-end Training Algorithm."></p><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p><img src="results.jpg" alt="Ablation study of the loss constraints."></p>]]></content>
      
      
      <categories>
          
          <category> Deepfake Detection </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deepfake Detection </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>TimeSformer论文笔记</title>
      <link href="/2024/02/24/computervision/timesformer/timesformer/"/>
      <url>/2024/02/24/computervision/timesformer/timesformer/</url>
      
        <content type="html"><![CDATA[<blockquote><p><strong>Title:</strong> <a href="https://arxiv.org/abs/2102.05095">《Is Space-Time Attention All You Need for Video Understanding?》</a> <br>From Facebook <br>ICML 2021 paper</p></blockquote><h2 id="Highlight"><a href="#Highlight" class="headerlink" title="Highlight"></a>Highlight</h2><ul><li>背景：第一篇用纯self-attn结构来做视频分类任务的工作。</li><li>文中设计了几种在spatial和temperal上做attn的方法，实验发现将temporal attention和 spatial attention 分开一个接一个交替用可以减少计算量并且获得更好的效果。</li></ul><h2 id="Methods"><a href="#Methods" class="headerlink" title="Methods"></a>Methods</h2><p><img src="video_self-attention_blocks.jpg" alt="Different Video Self-Attn Blocks"></p><ul><li>将每一帧（HxWx3）划分成PxP个不重叠的patches，然后把每个patch flatten成3P^2的vector.</li><li>过一个linear embedding变成D维的，然后加上一个learnable positional embedding。在序列的最开始加上一个token作为[CLS] token。</li><li>为了减少计算量，将temporal attention和 spatial attention 分开一个接一个交替用可以减少计算量并且获得更好的效果。</li></ul><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p><img src="Video-level-accuracy.jpg" alt="Video Level Accuracy"></p><h2 id="Pytorch-Code"><a href="#Pytorch-Code" class="headerlink" title="Pytorch Code"></a>Pytorch Code</h2><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">TimeSFormer</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token comment"># ...</span>    <span class="token keyword">def</span> <span class="token function">forward_features</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>        B <span class="token operator">=</span> x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>  <span class="token comment"># [2, 3, 8, 224, 224]  # BS, image channel, frame（这里以输入8帧图为例）, H, W</span>        <span class="token comment"># 先过一个kernel_size=16，stride=16的conv得到embedding</span>        x<span class="token punctuation">,</span> T<span class="token punctuation">,</span> W <span class="token operator">=</span> self<span class="token punctuation">.</span>patch_embed<span class="token punctuation">(</span>x<span class="token punctuation">)</span>  <span class="token comment"># [16, 196, 768], 8, 14</span>        cls_tokens <span class="token operator">=</span> self<span class="token punctuation">.</span>cls_token<span class="token punctuation">.</span>expand<span class="token punctuation">(</span>x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>        x <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">(</span>cls_tokens<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment"># [16, 197, 768]</span>        x <span class="token operator">=</span> x <span class="token operator">+</span> self<span class="token punctuation">.</span>pos_embed        x <span class="token operator">=</span> self<span class="token punctuation">.</span>pos_drop<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        <span class="token comment">##  Time Embeddings</span>        <span class="token keyword">if</span> self<span class="token punctuation">.</span>attention_type <span class="token operator">!=</span> <span class="token string">"space_only"</span><span class="token punctuation">:</span>            cls_tokens <span class="token operator">=</span> x<span class="token punctuation">[</span><span class="token punctuation">:</span>B<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment"># [2, 1, 768]</span>            x <span class="token operator">=</span> x<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">]</span>            x <span class="token operator">=</span> rearrange<span class="token punctuation">(</span>x<span class="token punctuation">,</span> <span class="token string">"(b t) n m -&gt; (b n) t m"</span><span class="token punctuation">,</span> b<span class="token operator">=</span>B<span class="token punctuation">,</span> t<span class="token operator">=</span>T<span class="token punctuation">)</span>  <span class="token comment"># 2,8,196,768, # [392, 8, 768]</span>            x <span class="token operator">=</span> x <span class="token operator">+</span> self<span class="token punctuation">.</span>time_embed  <span class="token comment"># 在每一个patch上都加上同样的time embedding.</span>            x <span class="token operator">=</span> self<span class="token punctuation">.</span>time_drop<span class="token punctuation">(</span>x<span class="token punctuation">)</span>            x <span class="token operator">=</span> rearrange<span class="token punctuation">(</span>x<span class="token punctuation">,</span> <span class="token string">"(b n) t m -&gt; b (n t) m"</span><span class="token punctuation">,</span> b<span class="token operator">=</span>B<span class="token punctuation">,</span> t<span class="token operator">=</span>T<span class="token punctuation">)</span>            x <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">(</span>cls_tokens<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment"># [2, 1+8*196, 768]</span>        <span class="token comment">## Attention blocks</span>        <span class="token keyword">for</span> blk <span class="token keyword">in</span> self<span class="token punctuation">.</span>blocks<span class="token punctuation">:</span>            x <span class="token operator">=</span> blk<span class="token punctuation">(</span>x<span class="token punctuation">,</span> B<span class="token punctuation">,</span> T<span class="token punctuation">,</span> W<span class="token punctuation">)</span>  <span class="token comment"># 12层attention</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>norm<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        <span class="token keyword">return</span> x<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>forward_features<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>head<span class="token punctuation">(</span>x<span class="token punctuation">)</span>  <span class="token comment"># 映射成分类分数</span>        <span class="token keyword">return</span> x<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li>每一层blk的结构如下所示</li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Block</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token comment"># ...</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">,</span> B<span class="token punctuation">,</span> T<span class="token punctuation">,</span> W<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token comment"># x: [2, 1 + 8 * 196, 768]</span>        num_spatial_tokens <span class="token operator">=</span> <span class="token punctuation">(</span>x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">//</span> T        H <span class="token operator">=</span> num_spatial_tokens <span class="token operator">//</span> W        <span class="token keyword">if</span> self<span class="token punctuation">.</span>attention_type <span class="token keyword">in</span> <span class="token punctuation">[</span><span class="token string">"space_only"</span><span class="token punctuation">,</span> <span class="token string">"joint_space_time"</span><span class="token punctuation">]</span><span class="token punctuation">:</span>            x <span class="token operator">=</span> x <span class="token operator">+</span> self<span class="token punctuation">.</span>drop_path<span class="token punctuation">(</span>self<span class="token punctuation">.</span>attn<span class="token punctuation">(</span>self<span class="token punctuation">.</span>norm1<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>            x <span class="token operator">=</span> x <span class="token operator">+</span> self<span class="token punctuation">.</span>drop_path<span class="token punctuation">(</span>self<span class="token punctuation">.</span>mlp<span class="token punctuation">(</span>self<span class="token punctuation">.</span>norm2<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>            <span class="token keyword">return</span> x        <span class="token keyword">elif</span> self<span class="token punctuation">.</span>attention_type <span class="token operator">==</span> <span class="token string">"divided_space_time"</span><span class="token punctuation">:</span>            <span class="token comment">## Temporal</span>            <span class="token comment"># Note: temporal没有和cls token做attn，而spatial做了。</span>            xt <span class="token operator">=</span> x<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span>            xt <span class="token operator">=</span> rearrange<span class="token punctuation">(</span>xt<span class="token punctuation">,</span> <span class="token string">"b (h w t) m -&gt; (b h w) t m"</span><span class="token punctuation">,</span> b<span class="token operator">=</span>B<span class="token punctuation">,</span> h<span class="token operator">=</span>H<span class="token punctuation">,</span> w<span class="token operator">=</span>W<span class="token punctuation">,</span> t<span class="token operator">=</span>T<span class="token punctuation">)</span>  <span class="token comment"># [392, 8, 768]</span>            <span class="token comment"># 不同帧之间做attn, attn map的尺寸为: [392, 12, 8, 8]</span>            res_temporal <span class="token operator">=</span> self<span class="token punctuation">.</span>drop_path<span class="token punctuation">(</span>self<span class="token punctuation">.</span>temporal_attn<span class="token punctuation">(</span>self<span class="token punctuation">.</span>temporal_norm1<span class="token punctuation">(</span>xt<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>            res_temporal <span class="token operator">=</span> rearrange<span class="token punctuation">(</span>                res_temporal<span class="token punctuation">,</span> <span class="token string">"(b h w) t m -&gt; b (h w t) m"</span><span class="token punctuation">,</span> b<span class="token operator">=</span>B<span class="token punctuation">,</span> h<span class="token operator">=</span>H<span class="token punctuation">,</span> w<span class="token operator">=</span>W<span class="token punctuation">,</span> t<span class="token operator">=</span>T            <span class="token punctuation">)</span>            res_temporal <span class="token operator">=</span> self<span class="token punctuation">.</span>temporal_fc<span class="token punctuation">(</span>res_temporal<span class="token punctuation">)</span>            xt <span class="token operator">=</span> x<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token operator">+</span> res_temporal  <span class="token comment"># [2, 8 * 196, 768]</span>            <span class="token comment">## Spatial</span>            init_cls_token <span class="token operator">=</span> x<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment"># [2, 1, 768]</span>            cls_token <span class="token operator">=</span> init_cls_token<span class="token punctuation">.</span>repeat<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> T<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment"># [2, 8, 768]</span>            cls_token <span class="token operator">=</span> rearrange<span class="token punctuation">(</span>cls_token<span class="token punctuation">,</span> <span class="token string">"b t m -&gt; (b t) m"</span><span class="token punctuation">,</span> b<span class="token operator">=</span>B<span class="token punctuation">,</span> t<span class="token operator">=</span>T<span class="token punctuation">)</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment"># [16, 1, 768]</span>            xs <span class="token operator">=</span> xt            xs <span class="token operator">=</span> rearrange<span class="token punctuation">(</span>xs<span class="token punctuation">,</span> <span class="token string">"b (h w t) m -&gt; (b t) (h w) m"</span><span class="token punctuation">,</span> b<span class="token operator">=</span>B<span class="token punctuation">,</span> h<span class="token operator">=</span>H<span class="token punctuation">,</span> w<span class="token operator">=</span>W<span class="token punctuation">,</span> t<span class="token operator">=</span>T<span class="token punctuation">)</span>  <span class="token comment"># [16, 196, 768]</span>            xs <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">(</span>cls_token<span class="token punctuation">,</span> xs<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment"># [16, 197, 768]</span>            <span class="token comment"># 不同spatial token 之间做attn</span>            res_spatial <span class="token operator">=</span> self<span class="token punctuation">.</span>drop_path<span class="token punctuation">(</span>self<span class="token punctuation">.</span>attn<span class="token punctuation">(</span>self<span class="token punctuation">.</span>norm1<span class="token punctuation">(</span>xs<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># [16, 197, 768]</span>            <span class="token comment">### Taking care of CLS token</span>            cls_token <span class="token operator">=</span> res_spatial<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span>            cls_token <span class="token operator">=</span> rearrange<span class="token punctuation">(</span>cls_token<span class="token punctuation">,</span> <span class="token string">"(b t) m -&gt; b t m"</span><span class="token punctuation">,</span> b<span class="token operator">=</span>B<span class="token punctuation">,</span> t<span class="token operator">=</span>T<span class="token punctuation">)</span>  <span class="token comment"># [2, 8, 768]</span>            cls_token <span class="token operator">=</span> torch<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>cls_token<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token boolean">True</span><span class="token punctuation">)</span>  <span class="token comment">## averaging for every frame  # [2, 1, 768]</span>            res_spatial <span class="token operator">=</span> res_spatial<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span>  <span class="token comment"># [16, 196, 768]</span>            res_spatial <span class="token operator">=</span> rearrange<span class="token punctuation">(</span>                res_spatial<span class="token punctuation">,</span> <span class="token string">"(b t) (h w) m -&gt; b (h w t) m"</span><span class="token punctuation">,</span> b<span class="token operator">=</span>B<span class="token punctuation">,</span> h<span class="token operator">=</span>H<span class="token punctuation">,</span> w<span class="token operator">=</span>W<span class="token punctuation">,</span> t<span class="token operator">=</span>T            <span class="token punctuation">)</span>  <span class="token comment"># [2, 8*196, 768]</span>            res <span class="token operator">=</span> res_spatial            x <span class="token operator">=</span> xt  <span class="token comment"># [2, 1568, 768]</span>            <span class="token comment">## Mlp</span>            <span class="token comment"># 相当于temp和spatial的结果ensemble</span>            x <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">(</span>init_cls_token<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">+</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">(</span>cls_token<span class="token punctuation">,</span> res<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>            x <span class="token operator">=</span> x <span class="token operator">+</span> self<span class="token punctuation">.</span>drop_path<span class="token punctuation">(</span>self<span class="token punctuation">.</span>mlp<span class="token punctuation">(</span>self<span class="token punctuation">.</span>norm2<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>            <span class="token keyword">return</span> x<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="Thoughts"><a href="#Thoughts" class="headerlink" title="Thoughts"></a>Thoughts</h2><ul><li>有个spacetime attention for diffusion distillation的想法，即让student学习teacher重建的过程，不过有个问题是teacher要一次denoising多步速度可能比较慢。</li></ul>]]></content>
      
      
      <categories>
          
          <category> Video Recognition </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Video Recognition </tag>
            
            <tag> Transformer </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>DiTs论文笔记</title>
      <link href="/2024/02/20/computervision/dits/dits/"/>
      <url>/2024/02/20/computervision/dits/dits/</url>
      
        <content type="html"><![CDATA[<blockquote><p>Title: <a href="https://arxiv.org/abs/2212.09748">Scalable Diffusion Models with Transformers</a> <br>From UC Berkeley and New York University <br>ICCV 2023 (Oral Presentation)</p></blockquote><p><img src="DiT.jpg" alt="DiTs"></p><h2 id="Highlight"><a href="#Highlight" class="headerlink" title="Highlight"></a>Highlight</h2><ul><li>作者说UNet的inductive bias对diffusion模型的性能并不重要，因此可以使用transformer结构来代替。</li><li>DiTs是在结合VAE的LDM框架下设计的。另外作者发现模型的复杂度和生成样本的质量有很大关系，简单的scaling up DiT并且用一个高容量的backbone训一个LDM可以取得SOTA的生成效果。</li></ul><h2 id="Methods"><a href="#Methods" class="headerlink" title="Methods"></a>Methods</h2><p><img src="DiT_Block.jpg" alt="The Diffusion Transformer (DiT) architecture."></p><ul><li>上图展示的是一个conditional latent DiT模型。</li><li>作者在这里实验了几种不同的结合conditioning的方法并发现使用adaLN-Zero效果最好，同时增加的资源消耗也少。</li></ul><p><img src="Different_conditioning_strategies.jpg" alt="Different conditioning strategies."></p><ul><li>In-context conditioning.<ul><li>将t和c的vector embedding作为input sequence之外的两个额外的tokens，和其他的image tokens同样处理。</li></ul></li><li>Cross-attentionblock<ul><li>将t和c的embeddings concat成一个长度为2的序列，和image token sequence分开。将transformer block 修改为在multi-head self-attn之后加一个multi-head cross-attn</li></ul></li><li>Adaptive layer norm (adaLN) block.<ul><li>使用adaptive LN替换LN。不再像LN那样直接学习一个dimension-wise的scale和shift权重gamma, beta，而是根据t和c的embedding的sum直接回归出这两个参数。</li></ul></li><li>adaLN-Zero block.<ul><li>受之前工作（在有监督学习任务中，零初始化每一个block的最后一个BN的scale权重可以加速large scale training）的启发，除了回归gamma和beta参数之外，也回归一个dimension-wise scaling 参数aplha作用在每个残差连接之前，然后初始化MLP输出一个全零的alpha，相当于将整个DiT block初始化为identity function。</li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> Image Generation </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Diffusion </tag>
            
            <tag> Transformers </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>动态规划专题</title>
      <link href="/2024/02/20/python/leetcode/dong-tai-gui-hua/"/>
      <url>/2024/02/20/python/leetcode/dong-tai-gui-hua/</url>
      
        <content type="html"><![CDATA[<blockquote><p>02/20/2024 - 01</p></blockquote><h1 id="300-最长递增子序列"><a href="#300-最长递增子序列" class="headerlink" title="300. 最长递增子序列"></a>300. 最长递增子序列</h1><ul><li>给定一个列表，求出最长递增子序列的长度。</li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Solution</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">lengthOfLIS</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> nums<span class="token punctuation">:</span> List<span class="token punctuation">[</span><span class="token builtin">int</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">&gt;</span> <span class="token builtin">int</span><span class="token punctuation">:</span>        lens <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>nums<span class="token punctuation">)</span>        <span class="token comment"># 定义dp[i]为以index=i结尾的最长子序列的长度</span>        dp <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">*</span> lens        <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>lens<span class="token punctuation">)</span><span class="token punctuation">:</span>            <span class="token keyword">for</span> j <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>i<span class="token punctuation">)</span><span class="token punctuation">:</span>                <span class="token keyword">if</span> nums<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">&gt;</span> nums<span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token punctuation">:</span>                    <span class="token comment"># 状态转移方程</span>                    dp<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token builtin">max</span><span class="token punctuation">(</span>dp<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">,</span> dp<span class="token punctuation">[</span>j<span class="token punctuation">]</span> <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span>                <span class="token keyword">return</span> <span class="token builtin">max</span><span class="token punctuation">(</span>dp<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><blockquote><p>02/20/2024 - 02</p></blockquote><h1 id="152-乘积最大子数组"><a href="#152-乘积最大子数组" class="headerlink" title="152. 乘积最大子数组"></a>152. 乘积最大子数组</h1><ul><li>给定一个列表，求乘积最大的连续子数组的乘积。</li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Solution</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">maxProduct</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> nums<span class="token punctuation">:</span> List<span class="token punctuation">[</span><span class="token builtin">int</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">&gt;</span> <span class="token builtin">int</span><span class="token punctuation">:</span>        maxF<span class="token punctuation">,</span> minF<span class="token punctuation">,</span> res <span class="token operator">=</span> nums<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> nums<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> nums<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>        lens <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>nums<span class="token punctuation">)</span>        <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> lens<span class="token punctuation">)</span><span class="token punctuation">:</span>            mx <span class="token operator">=</span> maxF            mn <span class="token operator">=</span> minF            <span class="token comment"># 因为涉及到数值正负号的问题，所以需要同时记录i-1位置为结尾的子数组的乘积的最大值和最小值</span>            maxF <span class="token operator">=</span> <span class="token builtin">max</span><span class="token punctuation">(</span>mx <span class="token operator">*</span> nums<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token builtin">max</span><span class="token punctuation">(</span>nums<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">,</span> mn <span class="token operator">*</span> nums<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>            minF <span class="token operator">=</span> <span class="token builtin">min</span><span class="token punctuation">(</span>mx <span class="token operator">*</span> nums<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token builtin">min</span><span class="token punctuation">(</span>nums<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">,</span> mn <span class="token operator">*</span> nums<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>            res <span class="token operator">=</span> <span class="token builtin">max</span><span class="token punctuation">(</span>maxF<span class="token punctuation">,</span> res<span class="token punctuation">)</span>                <span class="token keyword">return</span> res<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>]]></content>
      
      
      <categories>
          
          <category> Leetcode </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Leetcode </tag>
            
            <tag> 动态规划 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Lumiere论文笔记</title>
      <link href="/2024/02/19/computervision/lumiere/lumiere/"/>
      <url>/2024/02/19/computervision/lumiere/lumiere/</url>
      
        <content type="html"><![CDATA[<blockquote><p>Title: <a href="https://arxiv.org/abs/2401.12945">Lumiere: A Space-Time Diffusion Model for Video Generation</a> <br>From Google <br>ArXiv 2024.02.05</p></blockquote><p><img src="Lumiere.jpg" alt="Lumiere 电影时光"></p><h2 id="Highlight"><a href="#Highlight" class="headerlink" title="Highlight"></a>Highlight</h2><ul><li>背景：训练一个large-scale text-to-video(T2V) Foundation model是非常challenge的，因为引入了motion这个复杂度，同时时间维度增加也带来了更大的内存和计算量的消耗，需要的训练数据量也非常大。如下图(a)所示，已有的T2V的方法需要先使用一个base model生成一些关键帧，然后使用级联时间超分模型(cascade of temporal super-resolution (TSR) models)来扩充中间帧，然后在一些没有重叠的window上使用空间超分模型(spatial super-resolution (SSR) model)来获得高分辨率的结果。</li><li>如图b所示，本文提出的Lumiere模型提出STUNet来直接一步到位生成所有的帧，然后在一些重叠的windows上使用SSR来获得分辨率更高的视频(MultiDiffusion)。<br><img src="Lumiere_Pipeline.png" alt="Lumiere Pipeline"></li><li>Lumiere可以比较好地迁移到各种视频生成任务上，比如，视频风格生成，有条件生成，Image2Video，Inpainting，Cinemagraphs（在一副静态图像上画一个框，只生成框内的视频）</li></ul><h2 id="Methods"><a href="#Methods" class="headerlink" title="Methods"></a>Methods</h2><p><img src="STUNet.png" alt="STUNet architecture"></p><ul><li>StuNet(SpaceTime UNet)是在一个预训练好的T2I U-Net结构上在video的space和time上都进行下采样和上采样。</li><li>Convolution-based blocks是一个pre-trained T2I layers紧跟着一个space-time convolution。</li><li>Attention-based blocks是在原始的UNet层的pre-trained T2I layers跟上多个时间attention层。</li><li>Multidiffusion for Spatial-Super Resolution. 在时间维度重叠的windows上应用了MultiDiffusion进行线性融合获得最终的结果。</li><li>只有新加的这些时间层是要训练的，原始的T2I权重是固定住的。</li></ul><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p><strong>1. Stylized Generation</strong></p><ul><li>通过将fine-tuned T2I weights和原始的T2I weights进行线性插值可以获得较好的风格迁移视频。</li></ul><p><img src="Style_Generation.jpg" alt="Style Generation"></p><p><strong>2. Conditional Generation</strong></p><ul><li>拓展了输入的形式来兼容多个任务，将Noisy video(TxHxWx3)，masked conditioning(TxHxWx3)，binary mask（TxHxWx1）concat成7通道的输入，然后根据任务，比如Image-to-Video，Inpainting等来调整mask condition和binary mask的输入。</li><li>有微调阶段。</li></ul><h2 id="Thoughts"><a href="#Thoughts" class="headerlink" title="Thoughts"></a>Thoughts</h2><ul><li>Video生成直接一步到位生成多帧是合理的，但是最大的问题是资源消耗大的问题，因此SpaceTime卷积和SpaceTime Attention要做得足够好才能将feature降维到一个计算量能接受的程度。</li></ul>]]></content>
      
      
      <categories>
          
          <category> Video Generation </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Diffusion </tag>
            
            <tag> Video Generation </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Sora技术报告解读</title>
      <link href="/2024/02/17/computervision/sora/sora/"/>
      <url>/2024/02/17/computervision/sora/sora/</url>
      
        <content type="html"><![CDATA[<ul><li><p>这里看的是OpenAI写的<a href="https://openai.com/research/video-generation-models-as-world-simulators">Sora技术报告</a></p></li><li><p>Sora是一个Video Generation生成器，可以生成不同时长、宽高比和分辨率的视频和图像，最多可达一分钟的高清视频。</p></li><li><p>Sora是使用不同长度的videos和images训练的text-conditional diffusion models。</p></li><li><p>Sora是在video和image的latent code的时空patches上的运行的transformer结构。</p></li><li><p>这份技术报告只包含了两部分内容, 对于模型和实现细节在这份报告中并没有包含。</p><p>  (1) 将所有类型的视觉数据转化为统一的表示形式，从而实现生成模型的大规模训练</p><p>  (2) 对 Sora 的能力和局限性进行定性评估。</p></li></ul><h2 id="Turning-visual-data-into-patches"><a href="#Turning-visual-data-into-patches" class="headerlink" title="Turning visual data into patches"></a>Turning visual data into patches</h2><ul><li>LLM成功的一部分原因来自于token的使用，可以优雅地将不同模态的text进行统一的编码，本文考虑的是在视觉数据中如何从中获益。Sora的patches类似于tokens, patches对于在不同形式的videos和images上训练生成模型是高效并拓展性高的。</li><li>总结下来，首先将视频压缩到较低维的latent space，然后将这个表征分解为时空（spacetime）patches。</li></ul><p><img src="Sora-1-patches.png" alt="Sora Patches"></p><h2 id="Video-compression-network"><a href="#Video-compression-network" class="headerlink" title="Video compression network"></a>Video compression network</h2><ul><li>训练一个网络[Auto-encoding variational bayes.]来降低视觉数据的维度。该网络将原始视频作为输入，并输出压缩了时间（服务于长视频生成）和空间的latent representation。Sora在这个压缩的latent space中训练并随后生成视频。我们还训练了一个相应的decoder模型，该模型将生成的latents映射回pixel space。</li></ul><h2 id="Spacetime-latent-patches"><a href="#Spacetime-latent-patches" class="headerlink" title="Spacetime latent patches"></a>Spacetime latent patches</h2><ul><li>给定一个压缩的输入视频，我们提取一系列spacetime patches作为transformer tokens。这个方案也适用于图像，因为图像只是单帧的视频。我们基于patch的表示使Sora能够在不同分辨率、持续时间和宽高比的视频和图像上进行训练。在推理时，我们可以通过在适当大小的网格中安排随机初始化的patches来控制生成视频的大小。</li></ul><h2 id="Scaling-transformers-for-video-generation"><a href="#Scaling-transformers-for-video-generation" class="headerlink" title="Scaling transformers for video generation"></a>Scaling transformers for video generation</h2><ul><li>Sora是一个diffusion model[Scalable diffusion models with transformers]，通过noisy patches预测原始的clean patches。Sora是一个diffusion transformer, 因为transformer的scaling特性。</li><li>Sample quality improves markedly as training compute increases.</li></ul><h2 id="Variable-durations-resolutions-aspect-ratios"><a href="#Variable-durations-resolutions-aspect-ratios" class="headerlink" title="Variable durations, resolutions, aspect ratios"></a>Variable durations, resolutions, aspect ratios</h2><ul><li>采样的灵活性 Sampling flexibility</li><li>提升构图和框架 Improved framing and composition，不会生成主体的部分出现在图像之外。</li></ul><h2 id="Language-understanding"><a href="#Language-understanding" class="headerlink" title="Language understanding"></a>Language understanding</h2><ul><li>使用了DALL·E3中的re-captioning技术。我们首先训练一个高度描述性的captioner model，然后使用它为训练集中的所有视频生成文本描述。作者发现，对高质量描述性的视频字幕进行训练可以提高文本保真度以及视频的整体质量。</li><li>与 DALL·E3 类似，我们还利用GPT将简短的用户提示转换为较长的详细captions，然后送到视频模型中，这使得Sora能够生成准确遵循用户提示的高质量视频。</li></ul><h2 id="Prompting-with-images-and-videos"><a href="#Prompting-with-images-and-videos" class="headerlink" title="Prompting with images and videos"></a>Prompting with images and videos</h2><ul><li>Sora还可以使用图像和视频作为prompts</li><li>Sora可以向前向后拓展视频</li><li>结合SDEdit，可以变换videos的风格和环境</li><li>还可以在两段不同的视频中做插入（这个估计就是对latect space做插值操作了？）</li></ul><h2 id="Image-generation-capabilities"><a href="#Image-generation-capabilities" class="headerlink" title="Image generation capabilities"></a>Image generation capabilities</h2><h2 id="Emerging-simulation-capabilities"><a href="#Emerging-simulation-capabilities" class="headerlink" title="Emerging simulation capabilities"></a>Emerging simulation capabilities</h2><ul><li><p>大规模训练后获得的突发能力，没有任何归纳偏置的情况下，完全来自于scale</p><ul><li>3D consistency. Sora can generate videos with dynamic camera motion.</li><li>Long-range coherence and object permanence.</li><li>Interacting with the world.</li></ul>  <video width="480" height="320" controls="">  <source src="simulation_4.mp4" type="video/mp4">  Sora Interaction.  </video><ul><li>Simulating digital worlds.</li></ul></li></ul><h2 id="Limitations"><a href="#Limitations" class="headerlink" title="Limitations"></a>Limitations</h2><ul><li>For example, it does not accurately model the physics of many basic interactions.</li></ul>]]></content>
      
      
      <categories>
          
          <category> Video Generation </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Diffusion </tag>
            
            <tag> Video Generation </tag>
            
            <tag> Transformer </tag>
            
            <tag> Sora </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>nanoGPT源码解析</title>
      <link href="/2024/02/09/computervision/nanogpt/nanogpt/"/>
      <url>/2024/02/09/computervision/nanogpt/nanogpt/</url>
      
        <content type="html"><![CDATA[<ul><li>这里看的是AK写的<a href="https://github.com/karpathy/nanoGPT">nanoGPT</a></li></ul><h4 id="1-wandb用法"><a href="#1-wandb用法" class="headerlink" title="1. wandb用法"></a>1. wandb用法</h4><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> wandbwandb<span class="token punctuation">.</span>init<span class="token punctuation">(</span>project<span class="token operator">=</span>wandb_project<span class="token punctuation">,</span> name<span class="token operator">=</span>wandb_run_name<span class="token punctuation">,</span> config<span class="token operator">=</span>config<span class="token punctuation">)</span>wandb<span class="token punctuation">.</span>log<span class="token punctuation">(</span><span class="token punctuation">{</span>    <span class="token string">"iter"</span><span class="token punctuation">:</span> iter_num<span class="token punctuation">,</span>    <span class="token string">"train/loss"</span><span class="token punctuation">:</span> losses<span class="token punctuation">[</span><span class="token string">'train'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>    <span class="token string">"val/loss"</span><span class="token punctuation">:</span> losses<span class="token punctuation">[</span><span class="token string">'val'</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">}</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>]]></content>
      
      
      <categories>
          
          <category> GPTs </category>
          
      </categories>
      
      
        <tags>
            
            <tag> GPTs </tag>
            
            <tag> nanoGPT </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>VQGAN源码解析</title>
      <link href="/2024/01/31/computervision/vqgan/vqgan/"/>
      <url>/2024/01/31/computervision/vqgan/vqgan/</url>
      
        <content type="html"><![CDATA[<ul><li>这里看的是一个外国小哥复现的简单版本的<a href="https://github.com/dome272/VQGAN-pytorch">VQGAN</a></li></ul><h4 id="1-tqdm的一个用法"><a href="#1-tqdm的一个用法" class="headerlink" title="1. tqdm的一个用法"></a>1. tqdm的一个用法</h4><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">with</span> tqdm<span class="token punctuation">(</span><span class="token builtin">range</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>train_dataset<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token keyword">as</span> pbar<span class="token punctuation">:</span>    <span class="token keyword">for</span> i<span class="token punctuation">,</span> imgs <span class="token keyword">in</span> <span class="token builtin">zip</span><span class="token punctuation">(</span>pbar<span class="token punctuation">,</span> train_dataset<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token comment"># do something</span>        pbar<span class="token punctuation">.</span>set_postfix<span class="token punctuation">(</span>            VQ_Loss<span class="token operator">=</span>np<span class="token punctuation">.</span><span class="token builtin">round</span><span class="token punctuation">(</span>vq_loss<span class="token punctuation">.</span>cpu<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            GAN_Loss<span class="token operator">=</span>np<span class="token punctuation">.</span><span class="token builtin">round</span><span class="token punctuation">(</span>gan_loss<span class="token punctuation">.</span>cpu<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span>        <span class="token punctuation">)</span>        pbar<span class="token punctuation">.</span>update<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="2-使用torchinfo可视化网络结构"><a href="#2-使用torchinfo可视化网络结构" class="headerlink" title="2. 使用torchinfo可视化网络结构"></a>2. 使用torchinfo可视化网络结构</h4><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> torchinfo <span class="token keyword">import</span> summaryencoder <span class="token operator">=</span> Encoder<span class="token punctuation">(</span><span class="token punctuation">)</span>summary<span class="token punctuation">(</span>encoder<span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">256</span><span class="token punctuation">,</span> <span class="token number">256</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><h4 id="3-VQGAN的Encoder部分"><a href="#3-VQGAN的Encoder部分" class="headerlink" title="3. VQGAN的Encoder部分"></a>3. VQGAN的Encoder部分</h4><ul><li><p>模型结构为：<br><img src="VQGAN-Encoder.png" alt="VQGAN Encoder结构"></p></li><li><p>其中，</p><ul><li>ResidualBlock是由两个GroupNorm+Swish+Conv组成;</li><li>DownSampleBlock是一个stride=2的Conv;</li><li>NonLocalBlock是一个Attention Block;</li></ul></li><li><p>Encode之后会再接一个1x1的conv，称作是quant_conv. 然后过codebook，对应的还有个1x1的post_quant_conv。</p></li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># 简易Attention实现</span>attn <span class="token operator">=</span> torch<span class="token punctuation">.</span>bmm<span class="token punctuation">(</span>q<span class="token punctuation">,</span> k<span class="token punctuation">)</span>attn <span class="token operator">=</span> attn <span class="token operator">*</span> <span class="token punctuation">(</span><span class="token builtin">int</span><span class="token punctuation">(</span>c<span class="token punctuation">)</span> <span class="token operator">**</span> <span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">0.5</span><span class="token punctuation">)</span><span class="token punctuation">)</span>attn <span class="token operator">=</span> F<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>attn<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>attn <span class="token operator">=</span> attn<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>A <span class="token operator">=</span> torch<span class="token punctuation">.</span>bmm<span class="token punctuation">(</span>v<span class="token punctuation">,</span> attn<span class="token punctuation">)</span>A <span class="token operator">=</span> A<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>b<span class="token punctuation">,</span> c<span class="token punctuation">,</span> h<span class="token punctuation">,</span> w<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="4-VQGAN的Codebook部分"><a href="#4-VQGAN的Codebook部分" class="headerlink" title="4. VQGAN的Codebook部分"></a>4. VQGAN的Codebook部分</h4><ul><li>为什么要使用Codebook对Encoder得到的feature做离散化编码？<ul><li>使得模型学习到更抽象和压缩的数据表示</li><li>强制信息瓶颈帮助提升生成模型的泛化能力</li><li>提高计算效率</li></ul></li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn<span class="token keyword">class</span> <span class="token class-name">Codebook</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> args<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token builtin">super</span><span class="token punctuation">(</span>Codebook<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>num_codebook_vectors <span class="token operator">=</span> args<span class="token punctuation">.</span>num_codebook_vectors  <span class="token comment"># 1024</span>        self<span class="token punctuation">.</span>latent_dim <span class="token operator">=</span> args<span class="token punctuation">.</span>latent_dim  <span class="token comment"># 256</span>        self<span class="token punctuation">.</span>beta <span class="token operator">=</span> args<span class="token punctuation">.</span>beta  <span class="token comment"># 0.25</span>        self<span class="token punctuation">.</span>embedding <span class="token operator">=</span> nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>self<span class="token punctuation">.</span>num_codebook_vectors<span class="token punctuation">,</span> self<span class="token punctuation">.</span>latent_dim<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>embedding<span class="token punctuation">.</span>weight<span class="token punctuation">.</span>data<span class="token punctuation">.</span>uniform_<span class="token punctuation">(</span>            <span class="token operator">-</span><span class="token number">1.0</span> <span class="token operator">/</span> self<span class="token punctuation">.</span>num_codebook_vectors<span class="token punctuation">,</span> <span class="token number">1.0</span> <span class="token operator">/</span> self<span class="token punctuation">.</span>num_codebook_vectors        <span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> z<span class="token punctuation">)</span><span class="token punctuation">:</span>        z <span class="token operator">=</span> z<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>contiguous<span class="token punctuation">(</span><span class="token punctuation">)</span>        z_flattened <span class="token operator">=</span> z<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>latent_dim<span class="token punctuation">)</span>  <span class="token comment"># [1*16*16, 256]</span>        d <span class="token operator">=</span> <span class="token punctuation">(</span>            torch<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>z_flattened<span class="token operator">**</span><span class="token number">2</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> keepdim<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>            <span class="token operator">+</span> torch<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>embedding<span class="token punctuation">.</span>weight<span class="token operator">**</span><span class="token number">2</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>            <span class="token operator">-</span> <span class="token number">2</span> <span class="token operator">*</span> <span class="token punctuation">(</span>torch<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>z_flattened<span class="token punctuation">,</span> self<span class="token punctuation">.</span>embedding<span class="token punctuation">.</span>weight<span class="token punctuation">.</span>t<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token punctuation">)</span>  <span class="token comment"># [256, 1024]</span>        min_encoding_indices <span class="token operator">=</span> torch<span class="token punctuation">.</span>argmin<span class="token punctuation">(</span>d<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment"># [256]</span>        z_q <span class="token operator">=</span> self<span class="token punctuation">.</span>embedding<span class="token punctuation">(</span>min_encoding_indices<span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span>z<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>        loss <span class="token operator">=</span> torch<span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token punctuation">(</span>z_q<span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">-</span> z<span class="token punctuation">)</span> <span class="token operator">**</span> <span class="token number">2</span><span class="token punctuation">)</span> <span class="token operator">+</span> self<span class="token punctuation">.</span>beta <span class="token operator">*</span> torch<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>            <span class="token punctuation">(</span>z_q <span class="token operator">-</span> z<span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">**</span> <span class="token number">2</span>        <span class="token punctuation">)</span>        <span class="token comment"># simply copy the gradient from the decoder to the encoder. </span>        <span class="token comment"># 这里实现非常巧妙，也很容易出错。</span>        z_q <span class="token operator">=</span> z <span class="token operator">+</span> <span class="token punctuation">(</span>z_q <span class="token operator">-</span> z<span class="token punctuation">)</span><span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># preserve the gradients for the backward flow.</span>        z_q <span class="token operator">=</span> z_q<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>        <span class="token keyword">return</span> z_q<span class="token punctuation">,</span> min_encoding_indices<span class="token punctuation">,</span> loss<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="5-VQGAN的Decoder部分"><a href="#5-VQGAN的Decoder部分" class="headerlink" title="5. VQGAN的Decoder部分"></a>5. VQGAN的Decoder部分</h4><ul><li>模型结构为：<br><img src="VQGAN-Decoder.png" alt="VQGAN Decoder结构"></li></ul><h4 id="6-VQGAN的Discriminator部分"><a href="#6-VQGAN的Discriminator部分" class="headerlink" title="6. VQGAN的Discriminator部分"></a>6. VQGAN的Discriminator部分</h4><ul><li><p>只有在超过一定的steps(10000)之后才开始加入Disc的训练</p></li><li><p>模型结构为：<br><img src="VQGAN-Discriminator.png" alt="VQGAN Discriminator结构"></p></li></ul><h4 id="7-VQGAN的其他loss部分"><a href="#7-VQGAN的其他loss部分" class="headerlink" title="7. VQGAN的其他loss部分"></a>7. VQGAN的其他loss部分</h4><ul><li>perceptual_loss: 加载了一个pretrained VGG16模型，计算的real_img和gen_img之间的多层feature距离。</li><li>重建loss</li><li>gan_loss:</li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python">perceptual_rec_loss <span class="token operator">=</span> <span class="token punctuation">(</span>    args<span class="token punctuation">.</span>perceptual_loss_factor <span class="token operator">*</span> perceptual_loss    <span class="token operator">+</span> args<span class="token punctuation">.</span>rec_loss_factor <span class="token operator">*</span> rec_loss<span class="token punctuation">)</span>perceptual_rec_loss <span class="token operator">=</span> perceptual_rec_loss<span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token punctuation">)</span>g_loss <span class="token operator">=</span> <span class="token operator">-</span>torch<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>disc_fake<span class="token punctuation">)</span>  <span class="token comment"># 趋于0，或者大于0</span>λ <span class="token operator">=</span> self<span class="token punctuation">.</span>vqgan<span class="token punctuation">.</span>calculate_lambda<span class="token punctuation">(</span>perceptual_rec_loss<span class="token punctuation">,</span> g_loss<span class="token punctuation">)</span>vq_loss <span class="token operator">=</span> perceptual_rec_loss <span class="token operator">+</span> q_loss <span class="token operator">+</span> disc_factor <span class="token operator">*</span> λ <span class="token operator">*</span> g_lossd_loss_real <span class="token operator">=</span> torch<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span><span class="token number">1.0</span> <span class="token operator">-</span> disc_real<span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># 大于1</span>d_loss_fake <span class="token operator">=</span> torch<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span><span class="token number">1.0</span> <span class="token operator">+</span> disc_fake<span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># 小于-1</span>gan_loss <span class="token operator">=</span> disc_factor <span class="token operator">*</span> <span class="token number">0.5</span> <span class="token operator">*</span> <span class="token punctuation">(</span>d_loss_real <span class="token operator">+</span> d_loss_fake<span class="token punctuation">)</span>self<span class="token punctuation">.</span>opt_vq<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>vq_loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span>retain_graph<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>self<span class="token punctuation">.</span>opt_disc<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>gan_loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>self<span class="token punctuation">.</span>opt_vq<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>self<span class="token punctuation">.</span>opt_disc<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="8-VQGAN的loss的calculate-lambda部分"><a href="#8-VQGAN的loss的calculate-lambda部分" class="headerlink" title="8. VQGAN的loss的calculate_lambda部分"></a>8. VQGAN的loss的calculate_lambda部分</h4><ul><li>这个函数的作用: 动态调整模型训练中不同部分的损失贡献相关。<ul><li>根据最后一层的梯度情况来平衡感知损失和GAN损失的影响，有助于控制训练过程中的损失平衡。</li></ul></li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">calculate_lambda</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> perceptual_loss<span class="token punctuation">,</span> gan_loss<span class="token punctuation">)</span><span class="token punctuation">:</span>    last_layer <span class="token operator">=</span> self<span class="token punctuation">.</span>decoder<span class="token punctuation">.</span>model<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span>    last_layer_weight <span class="token operator">=</span> last_layer<span class="token punctuation">.</span>weight    perceptual_loss_grads <span class="token operator">=</span> torch<span class="token punctuation">.</span>autograd<span class="token punctuation">.</span>grad<span class="token punctuation">(</span>        perceptual_loss<span class="token punctuation">,</span> last_layer_weight<span class="token punctuation">,</span> retain_graph<span class="token operator">=</span><span class="token boolean">True</span>    <span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>    gan_loss_grads <span class="token operator">=</span> torch<span class="token punctuation">.</span>autograd<span class="token punctuation">.</span>grad<span class="token punctuation">(</span>        gan_loss<span class="token punctuation">,</span> last_layer_weight<span class="token punctuation">,</span> retain_graph<span class="token operator">=</span><span class="token boolean">True</span>    <span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>    λ <span class="token operator">=</span> torch<span class="token punctuation">.</span>norm<span class="token punctuation">(</span>perceptual_loss_grads<span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token punctuation">(</span>torch<span class="token punctuation">.</span>norm<span class="token punctuation">(</span>gan_loss_grads<span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token number">1e-4</span><span class="token punctuation">)</span>    λ <span class="token operator">=</span> torch<span class="token punctuation">.</span>clamp<span class="token punctuation">(</span>λ<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1e4</span><span class="token punctuation">)</span><span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token keyword">return</span> <span class="token number">0.8</span> <span class="token operator">*</span> λ<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><p>到这里，我们训练VQGAN的第一步已经结束了，我们可以根据一组VQ来重建出一张图，那么接下就需要transformer来负责先预测出一组VQ然后重建出图来啦～</p><hr><h4 id="8-VQGAN的Transformer部分"><a href="#8-VQGAN的Transformer部分" class="headerlink" title="8. VQGAN的Transformer部分"></a>8. VQGAN的Transformer部分</h4><ul><li>AdamW优化器区分了bias, nn.LayerNorm, nn.Embedding以及pos_emb训练时都不加weight decay，而nn.Linear加了weight decay。</li><li>Transformer部分的网络结构主要是一个<a href="https://github.com/karpathy/minGPT/">minGPT</a>。</li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> idx<span class="token punctuation">,</span> embeddings<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token comment"># idx是输入图像通过vqgan的Encoder部分得到的indices经过随机mask掉一部分得到的indices.</span>    token_embeddings <span class="token operator">=</span> self<span class="token punctuation">.</span>tok_emb<span class="token punctuation">(</span>idx<span class="token punctuation">)</span>  <span class="token comment"># each index maps to a (learnable) vector</span>    t <span class="token operator">=</span> token_embeddings<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>    position_embeddings <span class="token operator">=</span> self<span class="token punctuation">.</span>pos_emb<span class="token punctuation">[</span>        <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span>t<span class="token punctuation">,</span> <span class="token punctuation">:</span>    <span class="token punctuation">]</span>  <span class="token comment"># each position maps to a (learnable) vector</span>    x <span class="token operator">=</span> self<span class="token punctuation">.</span>drop<span class="token punctuation">(</span>token_embeddings <span class="token operator">+</span> position_embeddings<span class="token punctuation">)</span>    x <span class="token operator">=</span> self<span class="token punctuation">.</span>blocks<span class="token punctuation">(</span>x<span class="token punctuation">)</span>  <span class="token comment"># A vanilla multi-head masked self-attention layer</span>    x <span class="token operator">=</span> self<span class="token punctuation">.</span>ln_f<span class="token punctuation">(</span>x<span class="token punctuation">)</span>    logits <span class="token operator">=</span> self<span class="token punctuation">.</span>head<span class="token punctuation">(</span>x<span class="token punctuation">)</span>  <span class="token comment"># [1, 1024]</span>    <span class="token keyword">return</span> logits<span class="token punctuation">,</span> <span class="token boolean">None</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li><p>这里训GPTTransformer的时候，</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>    _<span class="token punctuation">,</span> indices <span class="token operator">=</span> self<span class="token punctuation">.</span>encode_to_z<span class="token punctuation">(</span>x<span class="token punctuation">)</span>  <span class="token comment"># 输入图像经过vqgan的Encoder部分得到的indices</span>    sos_tokens <span class="token operator">=</span> torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span>x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">*</span> self<span class="token punctuation">.</span>sos_token    sos_tokens <span class="token operator">=</span> sos_tokens<span class="token punctuation">.</span><span class="token builtin">long</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span><span class="token string">"cuda"</span><span class="token punctuation">)</span>  <span class="token comment"># 起始token的index，默认是0</span>    mask <span class="token operator">=</span> torch<span class="token punctuation">.</span>bernoulli<span class="token punctuation">(</span>        self<span class="token punctuation">.</span>pkeep <span class="token operator">*</span> torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span>indices<span class="token punctuation">.</span>shape<span class="token punctuation">,</span> device<span class="token operator">=</span>indices<span class="token punctuation">.</span>device<span class="token punctuation">)</span>    <span class="token punctuation">)</span>    mask <span class="token operator">=</span> mask<span class="token punctuation">.</span><span class="token builtin">round</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>int64<span class="token punctuation">)</span>    random_indices <span class="token operator">=</span> torch<span class="token punctuation">.</span>randint_like<span class="token punctuation">(</span>indices<span class="token punctuation">,</span> self<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>config<span class="token punctuation">.</span>vocab_size<span class="token punctuation">)</span>    <span class="token comment"># 随机mask掉一半的indices，用随机的indices代替</span>    new_indices <span class="token operator">=</span> mask <span class="token operator">*</span> indices <span class="token operator">+</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">-</span> mask<span class="token punctuation">)</span> <span class="token operator">*</span> random_indices      new_indices <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">(</span>sos_tokens<span class="token punctuation">,</span> new_indices<span class="token punctuation">)</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>    target <span class="token operator">=</span> indices    logits<span class="token punctuation">,</span> _ <span class="token operator">=</span> self<span class="token punctuation">.</span>transformer<span class="token punctuation">(</span>new_indices<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>    <span class="token keyword">return</span> logits<span class="token punctuation">,</span> target  <span class="token comment"># 直接用cross entropy训练</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li><li><p>这样训完之后，GPT Transformer就可以根据前序的token来预测后续的token了</p></li></ul><h4 id="9-训好了怎么sample生成样本呢？"><a href="#9-训好了怎么sample生成样本呢？" class="headerlink" title="9. 训好了怎么sample生成样本呢？"></a>9. 训好了怎么sample生成样本呢？</h4><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token decorator annotation punctuation">@torch<span class="token punctuation">.</span>no_grad</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">def</span> <span class="token function">sample</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">,</span> c<span class="token punctuation">,</span> steps<span class="token punctuation">,</span> temperature<span class="token operator">=</span><span class="token number">1.0</span><span class="token punctuation">,</span> top_k<span class="token operator">=</span><span class="token number">100</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    self<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span><span class="token builtin">eval</span><span class="token punctuation">(</span><span class="token punctuation">)</span>    x <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">(</span>c<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>    <span class="token keyword">for</span> k <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>steps<span class="token punctuation">)</span><span class="token punctuation">:</span>        logits<span class="token punctuation">,</span> _ <span class="token operator">=</span> self<span class="token punctuation">.</span>transformer<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        logits <span class="token operator">=</span> logits<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token operator">/</span> temperature        <span class="token keyword">if</span> top_k <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>            logits <span class="token operator">=</span> self<span class="token punctuation">.</span>top_k_logits<span class="token punctuation">(</span>logits<span class="token punctuation">,</span> top_k<span class="token punctuation">)</span>        probs <span class="token operator">=</span> F<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>logits<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>        ix <span class="token operator">=</span> torch<span class="token punctuation">.</span>multinomial<span class="token punctuation">(</span>probs<span class="token punctuation">,</span> num_samples<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment"># 按照概率随机取一个值</span>        x <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span> ix<span class="token punctuation">)</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>    x <span class="token operator">=</span> x<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> c<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">:</span><span class="token punctuation">]</span>    self<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token keyword">return</span> x<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li>获得一个序列的indices送入到VQGAN的Decoder就可以生成图像了。</li></ul>]]></content>
      
      
      <categories>
          
          <category> Computer Vision </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Computer Vision </tag>
            
            <tag> Image Generation </tag>
            
            <tag> GAN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Python字符串神奇用法</title>
      <link href="/2024/01/23/python/zi-fu-chuan/"/>
      <url>/2024/01/23/python/zi-fu-chuan/</url>
      
        <content type="html"><![CDATA[<h4 id="1-将一个字符串中的前两个’-’替换为’-’"><a href="#1-将一个字符串中的前两个’-’替换为’-’" class="headerlink" title="1. 将一个字符串中的前两个’_’替换为’:’"></a>1. 将一个字符串中的前两个’_’替换为’:’</h4><pre class="line-numbers language-python" data-language="python"><code class="language-python">s <span class="token operator">=</span> <span class="token string">"a_b_c_d_e"</span>s_replaced <span class="token operator">=</span> s<span class="token punctuation">.</span>replace<span class="token punctuation">(</span><span class="token string">"_"</span><span class="token punctuation">,</span> <span class="token string">":"</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>  <span class="token comment"># 指定最大替换次数为2</span><span class="token keyword">print</span><span class="token punctuation">(</span>s_replaced<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><h4 id="2-argparse命令行参数中的-会自动转换为"><a href="#2-argparse命令行参数中的-会自动转换为" class="headerlink" title="2. argparse命令行参数中的-会自动转换为_"></a>2. argparse命令行参数中的-会自动转换为_</h4><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># 在使用Python的argparse库时，如果你定义了一个带有连字符（-）的命令行参数，argparse模块会自动将这些连字符转换为下划线（_）。这样做是因为在Python中，变量名不能包含连字符；它们可以包含下划线。</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h4 id="3-python的-staticmethod经常用在什么情况下？"><a href="#3-python的-staticmethod经常用在什么情况下？" class="headerlink" title="3. python的@staticmethod经常用在什么情况下？"></a>3. python的@staticmethod经常用在什么情况下？</h4><ul><li>staticmethod用于修饰类中的方法,使其可以在不创建类实例的情况下调用方法</li><li>函数逻辑与类有关联但不需要类或实例的任何信息：当你需要定义一些功能，这些功能虽然跟类相关，但执行时它们不需要类的任何信息（即不需要访问任何类变量或实例变量）。</li><li>组织工具函数：如果有一些与类操作相关的工具函数，你可能希望将它们组织在一个类里面，以保持代码的组织和清晰。</li><li>替代命名空间：当你想要使用类作为一个命名空间来避免函数名冲突时，可以定义静态方法。这样，你可以将相关的函数放在一个类下面，但这些函数并不需要访问类或实例的状态。</li><li>继承管理：在子类中，你可能想要重用某个静态方法而不是实例方法。因为静态方法不与特定的实例或类状态关联，它们更容易在继承中被复用。</li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">MathUtils</span><span class="token punctuation">:</span>    <span class="token decorator annotation punctuation">@staticmethod</span>    <span class="token keyword">def</span> <span class="token function">add</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span> y<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> x <span class="token operator">+</span> y    <span class="token comment"># 静态方法可以通过类名直接调用，无需创建类的实例</span>result <span class="token operator">=</span> MathUtils<span class="token punctuation">.</span>add<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>result<span class="token punctuation">)</span>  <span class="token comment"># 输出: 12</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="4-python的-运算符"><a href="#4-python的-运算符" class="headerlink" title="4. python的**运算符"></a>4. python的**运算符</h4><ul><li>可以使用**来将字典中的项作为关键字参数传递给函数。<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">greet</span><span class="token punctuation">(</span>first_name<span class="token punctuation">,</span> last_name<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"Hello </span><span class="token interpolation"><span class="token punctuation">{</span>first_name<span class="token punctuation">}</span></span><span class="token string"> </span><span class="token interpolation"><span class="token punctuation">{</span>last_name<span class="token punctuation">}</span></span><span class="token string">!"</span></span><span class="token punctuation">)</span>person <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token string">'first_name'</span><span class="token punctuation">:</span> <span class="token string">'John'</span><span class="token punctuation">,</span> <span class="token string">'last_name'</span><span class="token punctuation">:</span> <span class="token string">'Doe'</span><span class="token punctuation">}</span>greet<span class="token punctuation">(</span><span class="token operator">**</span>person<span class="token punctuation">)</span>  <span class="token comment"># 等同于 greet(first_name='John', last_name='Doe')</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li></ul><h4 id="5-python自带的globals函数"><a href="#5-python自带的globals函数" class="headerlink" title="5. python自带的globals函数"></a>5. python自带的globals函数</h4><ul><li>全局符号表，其中包含有关程序的所有信息，包含变量名，方法，类名等等。<pre class="line-numbers language-python" data-language="python"><code class="language-python">a <span class="token operator">=</span> <span class="token number">5</span><span class="token keyword">def</span> <span class="token function">func</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    c <span class="token operator">=</span> <span class="token number">10</span>    d <span class="token operator">=</span> c <span class="token operator">+</span> a     <span class="token builtin">globals</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token string">'a'</span><span class="token punctuation">]</span> <span class="token operator">=</span> d     <span class="token keyword">print</span><span class="token punctuation">(</span>d<span class="token punctuation">)</span>  func<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li></ul>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
            <tag> 字符串 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>欢迎</title>
      <link href="/2024/01/21/hello-world/"/>
      <url>/2024/01/21/hello-world/</url>
      
        <content type="html"><![CDATA[<p>欢迎来到一只猪的圈。</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="持续学习"><a href="#持续学习" class="headerlink" title="持续学习!"></a>持续学习!</h3><h3 id="学习是这个世界上最简单的事情"><a href="#学习是这个世界上最简单的事情" class="headerlink" title="学习是这个世界上最简单的事情!"></a>学习是这个世界上最简单的事情!</h3>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
