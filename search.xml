<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>VQGAN源码解析</title>
      <link href="/2024/01/31/computervision/vqgan/vqgan/"/>
      <url>/2024/01/31/computervision/vqgan/vqgan/</url>
      
        <content type="html"><![CDATA[<ul><li>这里看的是一个外国小哥复现的简单版本的<a href="https://github.com/dome272/VQGAN-pytorch">VQGAN</a></li></ul><h4 id="1-tqdm的一个用法"><a href="#1-tqdm的一个用法" class="headerlink" title="1. tqdm的一个用法"></a>1. tqdm的一个用法</h4><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">with</span> tqdm<span class="token punctuation">(</span><span class="token builtin">range</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>train_dataset<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token keyword">as</span> pbar<span class="token punctuation">:</span>    <span class="token keyword">for</span> i<span class="token punctuation">,</span> imgs <span class="token keyword">in</span> <span class="token builtin">zip</span><span class="token punctuation">(</span>pbar<span class="token punctuation">,</span> train_dataset<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token comment"># do something</span>        pbar<span class="token punctuation">.</span>set_postfix<span class="token punctuation">(</span>            VQ_Loss<span class="token operator">=</span>np<span class="token punctuation">.</span><span class="token builtin">round</span><span class="token punctuation">(</span>vq_loss<span class="token punctuation">.</span>cpu<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            GAN_Loss<span class="token operator">=</span>np<span class="token punctuation">.</span><span class="token builtin">round</span><span class="token punctuation">(</span>gan_loss<span class="token punctuation">.</span>cpu<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span>        <span class="token punctuation">)</span>        pbar<span class="token punctuation">.</span>update<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="2-使用torchinfo可视化网络结构"><a href="#2-使用torchinfo可视化网络结构" class="headerlink" title="2. 使用torchinfo可视化网络结构"></a>2. 使用torchinfo可视化网络结构</h4><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> torchinfo <span class="token keyword">import</span> summaryencoder <span class="token operator">=</span> Encoder<span class="token punctuation">(</span><span class="token punctuation">)</span>summary<span class="token punctuation">(</span>encoder<span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">256</span><span class="token punctuation">,</span> <span class="token number">256</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><h4 id="3-VQGAN的Encoder部分"><a href="#3-VQGAN的Encoder部分" class="headerlink" title="3. VQGAN的Encoder部分"></a>3. VQGAN的Encoder部分</h4><ul><li><p>模型结构为：<br><img src="VQGAN-Encoder.png" alt="VQGAN Encoder结构"></p></li><li><p>其中，</p><ul><li>ResidualBlock是由两个GroupNorm+Swish+Conv组成;</li><li>DownSampleBlock是一个stride=2的Conv;</li><li>NonLocalBlock是一个Attention Block;</li></ul></li><li><p>Encode之后会再接一个1x1的conv，称作是quant_conv. 然后过codebook，对应的还有个1x1的post_quant_conv。</p></li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># 简易Attention实现</span>attn <span class="token operator">=</span> torch<span class="token punctuation">.</span>bmm<span class="token punctuation">(</span>q<span class="token punctuation">,</span> k<span class="token punctuation">)</span>attn <span class="token operator">=</span> attn <span class="token operator">*</span> <span class="token punctuation">(</span><span class="token builtin">int</span><span class="token punctuation">(</span>c<span class="token punctuation">)</span> <span class="token operator">**</span> <span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">0.5</span><span class="token punctuation">)</span><span class="token punctuation">)</span>attn <span class="token operator">=</span> F<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>attn<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>attn <span class="token operator">=</span> attn<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>A <span class="token operator">=</span> torch<span class="token punctuation">.</span>bmm<span class="token punctuation">(</span>v<span class="token punctuation">,</span> attn<span class="token punctuation">)</span>A <span class="token operator">=</span> A<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>b<span class="token punctuation">,</span> c<span class="token punctuation">,</span> h<span class="token punctuation">,</span> w<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="4-VQGAN的Codebook部分"><a href="#4-VQGAN的Codebook部分" class="headerlink" title="4. VQGAN的Codebook部分"></a>4. VQGAN的Codebook部分</h4><ul><li>为什么要使用Codebook对Encoder得到的feature做离散化编码？<ul><li>使得模型学习到更抽象和压缩的数据表示</li><li>强制信息瓶颈帮助提升生成模型的泛化能力</li><li>提高计算效率</li></ul></li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn<span class="token keyword">class</span> <span class="token class-name">Codebook</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> args<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token builtin">super</span><span class="token punctuation">(</span>Codebook<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>num_codebook_vectors <span class="token operator">=</span> args<span class="token punctuation">.</span>num_codebook_vectors  <span class="token comment"># 1024</span>        self<span class="token punctuation">.</span>latent_dim <span class="token operator">=</span> args<span class="token punctuation">.</span>latent_dim  <span class="token comment"># 256</span>        self<span class="token punctuation">.</span>beta <span class="token operator">=</span> args<span class="token punctuation">.</span>beta  <span class="token comment"># 0.25</span>        self<span class="token punctuation">.</span>embedding <span class="token operator">=</span> nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>self<span class="token punctuation">.</span>num_codebook_vectors<span class="token punctuation">,</span> self<span class="token punctuation">.</span>latent_dim<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>embedding<span class="token punctuation">.</span>weight<span class="token punctuation">.</span>data<span class="token punctuation">.</span>uniform_<span class="token punctuation">(</span>            <span class="token operator">-</span><span class="token number">1.0</span> <span class="token operator">/</span> self<span class="token punctuation">.</span>num_codebook_vectors<span class="token punctuation">,</span> <span class="token number">1.0</span> <span class="token operator">/</span> self<span class="token punctuation">.</span>num_codebook_vectors        <span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> z<span class="token punctuation">)</span><span class="token punctuation">:</span>        z <span class="token operator">=</span> z<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>contiguous<span class="token punctuation">(</span><span class="token punctuation">)</span>        z_flattened <span class="token operator">=</span> z<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>latent_dim<span class="token punctuation">)</span>  <span class="token comment"># [1*16*16, 256]</span>        d <span class="token operator">=</span> <span class="token punctuation">(</span>            torch<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>z_flattened<span class="token operator">**</span><span class="token number">2</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> keepdim<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>            <span class="token operator">+</span> torch<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>embedding<span class="token punctuation">.</span>weight<span class="token operator">**</span><span class="token number">2</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>            <span class="token operator">-</span> <span class="token number">2</span> <span class="token operator">*</span> <span class="token punctuation">(</span>torch<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>z_flattened<span class="token punctuation">,</span> self<span class="token punctuation">.</span>embedding<span class="token punctuation">.</span>weight<span class="token punctuation">.</span>t<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token punctuation">)</span>  <span class="token comment"># [256, 1024]</span>        min_encoding_indices <span class="token operator">=</span> torch<span class="token punctuation">.</span>argmin<span class="token punctuation">(</span>d<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment"># [256]</span>        z_q <span class="token operator">=</span> self<span class="token punctuation">.</span>embedding<span class="token punctuation">(</span>min_encoding_indices<span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span>z<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>        loss <span class="token operator">=</span> torch<span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token punctuation">(</span>z_q<span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">-</span> z<span class="token punctuation">)</span> <span class="token operator">**</span> <span class="token number">2</span><span class="token punctuation">)</span> <span class="token operator">+</span> self<span class="token punctuation">.</span>beta <span class="token operator">*</span> torch<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>            <span class="token punctuation">(</span>z_q <span class="token operator">-</span> z<span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">**</span> <span class="token number">2</span>        <span class="token punctuation">)</span>        <span class="token comment"># simply copy the gradient from the decoder to the encoder. </span>        <span class="token comment"># 这里实现非常巧妙，也很容易出错。</span>        z_q <span class="token operator">=</span> z <span class="token operator">+</span> <span class="token punctuation">(</span>z_q <span class="token operator">-</span> z<span class="token punctuation">)</span><span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># preserve the gradients for the backward flow.</span>        z_q <span class="token operator">=</span> z_q<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>        <span class="token keyword">return</span> z_q<span class="token punctuation">,</span> min_encoding_indices<span class="token punctuation">,</span> loss<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="5-VQGAN的Decoder部分"><a href="#5-VQGAN的Decoder部分" class="headerlink" title="5. VQGAN的Decoder部分"></a>5. VQGAN的Decoder部分</h4><ul><li>模型结构为：<br><img src="VQGAN-Decoder.png" alt="VQGAN Decoder结构"></li></ul><h4 id="6-VQGAN的Discriminator部分"><a href="#6-VQGAN的Discriminator部分" class="headerlink" title="6. VQGAN的Discriminator部分"></a>6. VQGAN的Discriminator部分</h4><ul><li><p>只有在超过一定的steps(10000)之后才开始加入Disc的训练</p></li><li><p>模型结构为：<br><img src="VQGAN-Discriminator.png" alt="VQGAN Discriminator结构"></p></li></ul><h4 id="7-VQGAN的其他loss部分"><a href="#7-VQGAN的其他loss部分" class="headerlink" title="7. VQGAN的其他loss部分"></a>7. VQGAN的其他loss部分</h4><ul><li>perceptual_loss: 加载了一个pretrained VGG16模型，计算的real_img和gen_img之间的多层feature距离。</li><li>重建loss</li><li>gan_loss:</li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python">perceptual_rec_loss <span class="token operator">=</span> <span class="token punctuation">(</span>    args<span class="token punctuation">.</span>perceptual_loss_factor <span class="token operator">*</span> perceptual_loss    <span class="token operator">+</span> args<span class="token punctuation">.</span>rec_loss_factor <span class="token operator">*</span> rec_loss<span class="token punctuation">)</span>perceptual_rec_loss <span class="token operator">=</span> perceptual_rec_loss<span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token punctuation">)</span>g_loss <span class="token operator">=</span> <span class="token operator">-</span>torch<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>disc_fake<span class="token punctuation">)</span>  <span class="token comment"># 趋于0，或者大于0</span>λ <span class="token operator">=</span> self<span class="token punctuation">.</span>vqgan<span class="token punctuation">.</span>calculate_lambda<span class="token punctuation">(</span>perceptual_rec_loss<span class="token punctuation">,</span> g_loss<span class="token punctuation">)</span>vq_loss <span class="token operator">=</span> perceptual_rec_loss <span class="token operator">+</span> q_loss <span class="token operator">+</span> disc_factor <span class="token operator">*</span> λ <span class="token operator">*</span> g_lossd_loss_real <span class="token operator">=</span> torch<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span><span class="token number">1.0</span> <span class="token operator">-</span> disc_real<span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># 大于1</span>d_loss_fake <span class="token operator">=</span> torch<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span><span class="token number">1.0</span> <span class="token operator">+</span> disc_fake<span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># 小于-1</span>gan_loss <span class="token operator">=</span> disc_factor <span class="token operator">*</span> <span class="token number">0.5</span> <span class="token operator">*</span> <span class="token punctuation">(</span>d_loss_real <span class="token operator">+</span> d_loss_fake<span class="token punctuation">)</span>self<span class="token punctuation">.</span>opt_vq<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>vq_loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span>retain_graph<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>self<span class="token punctuation">.</span>opt_disc<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>gan_loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>self<span class="token punctuation">.</span>opt_vq<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>self<span class="token punctuation">.</span>opt_disc<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="8-VQGAN的loss的calculate-lambda部分"><a href="#8-VQGAN的loss的calculate-lambda部分" class="headerlink" title="8. VQGAN的loss的calculate_lambda部分"></a>8. VQGAN的loss的calculate_lambda部分</h4><ul><li>这个函数的作用: 动态调整模型训练中不同部分的损失贡献相关。<ul><li>根据最后一层的梯度情况来平衡感知损失和GAN损失的影响，有助于控制训练过程中的损失平衡。</li></ul></li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">calculate_lambda</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> perceptual_loss<span class="token punctuation">,</span> gan_loss<span class="token punctuation">)</span><span class="token punctuation">:</span>    last_layer <span class="token operator">=</span> self<span class="token punctuation">.</span>decoder<span class="token punctuation">.</span>model<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span>    last_layer_weight <span class="token operator">=</span> last_layer<span class="token punctuation">.</span>weight    perceptual_loss_grads <span class="token operator">=</span> torch<span class="token punctuation">.</span>autograd<span class="token punctuation">.</span>grad<span class="token punctuation">(</span>        perceptual_loss<span class="token punctuation">,</span> last_layer_weight<span class="token punctuation">,</span> retain_graph<span class="token operator">=</span><span class="token boolean">True</span>    <span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>    gan_loss_grads <span class="token operator">=</span> torch<span class="token punctuation">.</span>autograd<span class="token punctuation">.</span>grad<span class="token punctuation">(</span>        gan_loss<span class="token punctuation">,</span> last_layer_weight<span class="token punctuation">,</span> retain_graph<span class="token operator">=</span><span class="token boolean">True</span>    <span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>    λ <span class="token operator">=</span> torch<span class="token punctuation">.</span>norm<span class="token punctuation">(</span>perceptual_loss_grads<span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token punctuation">(</span>torch<span class="token punctuation">.</span>norm<span class="token punctuation">(</span>gan_loss_grads<span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token number">1e-4</span><span class="token punctuation">)</span>    λ <span class="token operator">=</span> torch<span class="token punctuation">.</span>clamp<span class="token punctuation">(</span>λ<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1e4</span><span class="token punctuation">)</span><span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token keyword">return</span> <span class="token number">0.8</span> <span class="token operator">*</span> λ<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><p>到这里，我们训练VQGAN的第一步已经结束了，我们可以根据一组VQ来重建出一张图，那么接下就需要transformer来负责先预测出一组VQ然后重建出图来啦～</p><hr>]]></content>
      
      
      <categories>
          
          <category> Computer Vision </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Computer Vision </tag>
            
            <tag> Image Generation </tag>
            
            <tag> GAN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Python字符串神奇用法</title>
      <link href="/2024/01/23/python/zi-fu-chuan/"/>
      <url>/2024/01/23/python/zi-fu-chuan/</url>
      
        <content type="html"><![CDATA[<h4 id="1-将一个字符串中的前两个’-’替换为’-’"><a href="#1-将一个字符串中的前两个’-’替换为’-’" class="headerlink" title="1. 将一个字符串中的前两个’_’替换为’:’"></a>1. 将一个字符串中的前两个’_’替换为’:’</h4><pre class="line-numbers language-python" data-language="python"><code class="language-python">s <span class="token operator">=</span> <span class="token string">"a_b_c_d_e"</span>s_replaced <span class="token operator">=</span> s<span class="token punctuation">.</span>replace<span class="token punctuation">(</span><span class="token string">"_"</span><span class="token punctuation">,</span> <span class="token string">":"</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>  <span class="token comment"># 指定最大替换次数为2</span><span class="token keyword">print</span><span class="token punctuation">(</span>s_replaced<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><h4 id="2-argparse命令行参数中的-会自动转换为"><a href="#2-argparse命令行参数中的-会自动转换为" class="headerlink" title="2. argparse命令行参数中的-会自动转换为_"></a>2. argparse命令行参数中的-会自动转换为_</h4><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># 在使用Python的argparse库时，如果你定义了一个带有连字符（-）的命令行参数，argparse模块会自动将这些连字符转换为下划线（_）。这样做是因为在Python中，变量名不能包含连字符；它们可以包含下划线。</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h4 id="3-python的-staticmethod经常用在什么情况下？"><a href="#3-python的-staticmethod经常用在什么情况下？" class="headerlink" title="3. python的@staticmethod经常用在什么情况下？"></a>3. python的@staticmethod经常用在什么情况下？</h4><ul><li>staticmethod用于修饰类中的方法,使其可以在不创建类实例的情况下调用方法</li><li>函数逻辑与类有关联但不需要类或实例的任何信息：当你需要定义一些功能，这些功能虽然跟类相关，但执行时它们不需要类的任何信息（即不需要访问任何类变量或实例变量）。</li><li>组织工具函数：如果有一些与类操作相关的工具函数，你可能希望将它们组织在一个类里面，以保持代码的组织和清晰。</li><li>替代命名空间：当你想要使用类作为一个命名空间来避免函数名冲突时，可以定义静态方法。这样，你可以将相关的函数放在一个类下面，但这些函数并不需要访问类或实例的状态。</li><li>继承管理：在子类中，你可能想要重用某个静态方法而不是实例方法。因为静态方法不与特定的实例或类状态关联，它们更容易在继承中被复用。</li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">MathUtils</span><span class="token punctuation">:</span>    <span class="token decorator annotation punctuation">@staticmethod</span>    <span class="token keyword">def</span> <span class="token function">add</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span> y<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> x <span class="token operator">+</span> y    <span class="token comment"># 静态方法可以通过类名直接调用，无需创建类的实例</span>result <span class="token operator">=</span> MathUtils<span class="token punctuation">.</span>add<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>result<span class="token punctuation">)</span>  <span class="token comment"># 输出: 12</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
            <tag> 字符串 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>欢迎</title>
      <link href="/2024/01/21/hello-world/"/>
      <url>/2024/01/21/hello-world/</url>
      
        <content type="html"><![CDATA[<p>欢迎来到一只猪的圈。</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="强者永不孤独，猛兽永远独行。"><a href="#强者永不孤独，猛兽永远独行。" class="headerlink" title="强者永不孤独，猛兽永远独行。"></a>强者永不孤独，猛兽永远独行。</h3><!-- ``` bash$ hexo server``` -->]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
